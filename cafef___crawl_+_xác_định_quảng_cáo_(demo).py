# -*- coding: utf-8 -*-
"""cafef _ crawl + xác định quảng cáo (demo)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i2hlMCWwQ9YTtT3k6zXkYyPu55agI0O0
"""

# ⚠️ This is a redacted demo version for portfolio purposes.
# ❌ All API keys and sensitive credentials have been removed.
# ✅ You can run this notebook by inserting your own Gemini API key and Google Sheet config.

# Cell 1: Cài đặt Thư viện và Môi trường

# --- 0. Mount Google Drive ---
try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive mounted successfully.")
except ImportError:
    print("Not running in Colab, skipping Drive mount.")
except Exception as e:
    print(f"Error mounting Google Drive: {e}")

# --- 1. Cài đặt Thư viện Python ---
print("\n--- Installing Python Libraries ---")
!pip install selenium pandas openpyxl pytz requests google-generativeai gspread google-auth google-auth-oauthlib google-auth-httplib2 --quiet
print("--- Library Installation Complete ---")

# --- 2. Cài đặt Chrome & ChromeDriver ---
print("\n--- Chrome & ChromeDriver Installation Logic Defined in Cell 5 ---")
print("--- It will be executed during the main process ---")

print("\n--- Environment Setup Cell Complete ---")

# Cell 2: Imports

print("\n--- Importing Python Libraries ---")
import sys
import json
import pandas as pd
import pytz
import os
import time
import traceback
import gspread
import requests
import random
import re
from datetime import datetime, timedelta

# --- Colab Specific ---
try:
    from google.colab import auth, files
    IS_COLAB = True
    print("   Running in Colab environment.")
except ImportError:
    IS_COLAB = False
    print("   Not running in Colab environment.")
    auth = None # Define auth as None if not in Colab
    files = None # Define files as None

# --- Google Auth ---
# Import default here, might raise error outside Colab if gcloud SDK not configured
try:
    from google.auth import default
    from google.auth import exceptions as google_auth_exceptions
except ImportError:
    print("   Warning: google.auth library not found. Google Authentication might fail.")
    default = None
    google_auth_exceptions = None


# --- AI ---
try:
    import google.generativeai as genai
except ImportError:
    print("   Warning: google-generativeai not installed. AI features disabled.")
    genai = None

# --- Selenium ---
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import (
        TimeoutException, WebDriverException, NoSuchElementException,
        StaleElementReferenceException, NoSuchWindowException
    )
except ImportError:
    print("   Error: selenium library not installed. Web crawling disabled.")
    # Define dummy classes/exceptions if needed to prevent NameErrors later, or handle missing selenium
    webdriver = None # Example

print("--- Library Imports Complete ---")

# Cell 3: Cấu hình & Biến Toàn cục

print("\n--- Loading Configuration & Globals ---")

# --- General Configuration ---
SOURCE_NAME = "CafeF"
TARGET_URL = "https://cafef.vn/thi-truong-chung-khoan.chn"
BASE_URL = "https://cafef.vn"

# --- Colab Form Parameters (with Fallbacks) ---
config_source = "Defaults" # Default source
# Initialize variables with non-sensitive defaults FIRST
SERVICE_ACCOUNT_JSON_PATH = "" # Sẽ được lấy từ form
GEMINI_API_KEY = "" # Sẽ được lấy từ form
SPREADSHEET_URL = "" # Sẽ được lấy từ form
TARGET_WORKSHEET_NAME = "your_default_sheet" # Default sheet name
HOURS_TO_CHECK = 24 # Giờ quét mặc định
MAX_SCROLLS = 5 # Scroll mặc định
SLEEP_BETWEEN_AI_CALLS = 10.0 # Trễ AI mặc định
SHEET_WRITE_DELAY = 1.0 # Trễ ghi sheet batch mặc định
SHEET_WRITE_BATCH_SIZE = 20 # Batch size mặc định

# Check if running in Colab to load form parameters
IS_COLAB = 'google.colab' in sys.modules

if IS_COLAB:
    print("   Running in Colab environment. Loading Form parameters...")
    try:
        #@markdown #### **Cấu hình Bắt buộc**
        #@markdown Đường dẫn tới file JSON Service Account (trong Google Drive):
        _SERVICE_ACCOUNT_JSON_PATH_FORM = "your_choice" #@param {type:"string"}
        #@markdown API Key Gemini (Google AI Studio):
        _GEMINI_API_KEY_FORM = "your_API_key" #@param {type:"string"}
        #@markdown URL Google Sheet (có quyền sửa):
        _SPREADSHEET_URL_FORM = "your_sheet" #@param {type:"string"}

        #@markdown #### **Cấu hình Tùy chọn**
        #@markdown Tên Tab đích trong Sheet:
        _TARGET_WORKSHEET_NAME_FORM = "your_sheet" #@param {type:"string"}
        #@markdown Số giờ quét ngược về quá khứ:
        _HOURS_TO_CHECK_FORM = 10 #@param {type:"integer"}
        #@markdown Số lần scroll tối đa:
        _MAX_SCROLLS_FORM = 3 #@param {type:"integer"}
        #@markdown Độ trễ giữa các lần gọi API Gemini (giây):
        _SLEEP_BETWEEN_AI_CALLS_FORM = 10.0 #@param {type:"number"}
        #@markdown Độ trễ giữa các lần ghi batch vào Sheet (giây):
        _SHEET_WRITE_DELAY_FORM = 1.0 #@param {type:"number"}
        #@markdown Số dòng ghi vào Sheet mỗi lần:
        _SHEET_WRITE_BATCH_SIZE_FORM = 20 #@param {type:"integer"}

        # Assign to main variables ONLY if user provided input in the form
        # Ưu tiên giá trị từ Form nếu người dùng nhập
        if _SERVICE_ACCOUNT_JSON_PATH_FORM: SERVICE_ACCOUNT_JSON_PATH = _SERVICE_ACCOUNT_JSON_PATH_FORM
        if _GEMINI_API_KEY_FORM: GEMINI_API_KEY = _GEMINI_API_KEY_FORM
        if _SPREADSHEET_URL_FORM: SPREADSHEET_URL = _SPREADSHEET_URL_FORM
        if _TARGET_WORKSHEET_NAME_FORM: TARGET_WORKSHEET_NAME = _TARGET_WORKSHEET_NAME_FORM # Gán luôn nếu có giá trị
        HOURS_TO_CHECK = _HOURS_TO_CHECK_FORM # Luôn lấy giá trị từ form (vì có default)
        MAX_SCROLLS = _MAX_SCROLLS_FORM
        SLEEP_BETWEEN_AI_CALLS = _SLEEP_BETWEEN_AI_CALLS_FORM
        SHEET_WRITE_DELAY = _SHEET_WRITE_DELAY_FORM
        SHEET_WRITE_BATCH_SIZE = _SHEET_WRITE_BATCH_SIZE_FORM

        config_source = "Colab Form"
        print("   Configuration successfully loaded/updated from Colab Form.")

    except NameError:
        print("   Colab Form parameters not defined (maybe form cell not run?). Attempting Environment Variables/Defaults.")
        # Fallback to Environment Variables if form fails or not in Colab
        SERVICE_ACCOUNT_JSON_PATH = os.environ.get("SERVICE_ACCOUNT_JSON_PATH", SERVICE_ACCOUNT_JSON_PATH)
        GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", GEMINI_API_KEY)
        SPREADSHEET_URL = os.environ.get("SPREADSHEET_URL", SPREADSHEET_URL)
        TARGET_WORKSHEET_NAME = os.environ.get("TARGET_WORKSHEET_NAME", TARGET_WORKSHEET_NAME)
        HOURS_TO_CHECK = int(os.environ.get("HOURS_TO_CHECK", HOURS_TO_CHECK))
        MAX_SCROLLS = int(os.environ.get("MAX_SCROLLS", MAX_SCROLLS))
        SLEEP_BETWEEN_AI_CALLS = float(os.environ.get("SLEEP_BETWEEN_AI_CALLS", SLEEP_BETWEEN_AI_CALLS))
        SHEET_WRITE_DELAY = float(os.environ.get("SHEET_WRITE_DELAY", SHEET_WRITE_DELAY))
        SHEET_WRITE_BATCH_SIZE = int(os.environ.get("SHEET_WRITE_BATCH_SIZE", SHEET_WRITE_BATCH_SIZE))
        config_source = "Environment/Defaults" if any(os.environ.get(k) for k in ["GEMINI_API_KEY", "SPREADSHEET_URL", "SERVICE_ACCOUNT_JSON_PATH"]) else "Defaults"

elif not IS_COLAB:
     print("   Not running in Colab. Attempting Environment Variables/Defaults.")
     # Fallback logic for non-Colab environments
     SERVICE_ACCOUNT_JSON_PATH = os.environ.get("SERVICE_ACCOUNT_JSON_PATH", SERVICE_ACCOUNT_JSON_PATH)
     GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", GEMINI_API_KEY)
     SPREADSHEET_URL = os.environ.get("SPREADSHEET_URL", SPREADSHEET_URL)
     TARGET_WORKSHEET_NAME = os.environ.get("TARGET_WORKSHEET_NAME", TARGET_WORKSHEET_NAME)
     HOURS_TO_CHECK = int(os.environ.get("HOURS_TO_CHECK", HOURS_TO_CHECK))
     MAX_SCROLLS = int(os.environ.get("MAX_SCROLLS", MAX_SCROLLS))
     SLEEP_BETWEEN_AI_CALLS = float(os.environ.get("SLEEP_BETWEEN_AI_CALLS", SLEEP_BETWEEN_AI_CALLS))
     SHEET_WRITE_DELAY = float(os.environ.get("SHEET_WRITE_DELAY", SHEET_WRITE_DELAY))
     SHEET_WRITE_BATCH_SIZE = int(os.environ.get("SHEET_WRITE_BATCH_SIZE", SHEET_WRITE_BATCH_SIZE))
     config_source = "Environment/Defaults" if any(os.environ.get(k) for k in ["GEMINI_API_KEY", "SPREADSHEET_URL", "SERVICE_ACCOUNT_JSON_PATH"]) else "Defaults"


# --- Keyword Configuration (Global) ---
KEYWORDS = {"VPS": ["VPS", "Chứng khoán VPS", "VPBankS", "VPBS", "VPBank Securities"], "SSI": ["SSI", "Công ty Cổ phần Chứng khoán SSI", "Chứng khoán SSI"], "TCBS": ["TCBS", "Công ty Cổ phần Chứng khoán Kỹ Thương", "Techcom Securities", "Chứng khoán Techcombank"], "DNSE": ["DNSE", "Công ty Cổ phần Chứng khoán DNSE", "DNSE Securities", "Chứng khoán DNSE"], "MBS": ["MBS", "Công ty Cổ phần Chứng khoán MB", "MB Securities", "Chứng khoán MB"], "VNDirect": ["VNDirect", "Công ty Cổ phần Chứng khoán VNDirect", "VNDS", "Chứng khoán VnDirect"]}
COMPETITOR_SHEET_MAP = {"VPS": "VPS_Chương trình", "SSI": "SSI_Chương trình", "TCBS": "TCBS_Chương trình", "DNSE": "DNSE_Chương trình", "MBS": "MBS_Chương trình", "VNDirect": "VNDirect_Chương trình"}

# --- Google Sheet Header V3 (Global) ---
# Giữ nguyên SHEET_HEADER_V3 và PROGRAM_SHEET_HEADER
SHEET_HEADER_V3 = [
    'Thời gian cập nhật', 'Tiêu đề bài viết', 'Công ty được phân tích', 'Là công ty đối thủ?', 'Là quảng cáo?',
    'Tên chương trình/sản phẩm được nhắc đến', 'Thông điệp chính (AI)', 'Tóm tắt phân tích LLM (AI)', 'URL', 'Trạng thái xử lý',
    'Trọng tâm bài viết (AI)', 'Phân loại chi tiết (AI/DB)', 'Giọng điệu (AI)', 'Nội dung gốc'
]

PROGRAM_SHEET_HEADER = [
    'Thời gian cập nhật', 'Tên', 'Kênh tiếp nhận', 'Thời gian triển khai', 'Loại hình sản phẩm',
    'Điều kiện tham gia', 'Đặc điểm chính', 'Ưu đãi nếu có', 'Đối tượng khách hàng mục tiêu',
    'URL Bài viết nguồn', 'Tên chương trình tương đồng ở MBS', 'Điểm giống sơ bộ', 'Điểm khác sơ bộ'
]
# --------------------------------------------------------------------

# --- Timezone & Gemini Model Name (Global) ---
TARGET_TIMEZONE = 'Asia/Ho_Chi_Minh'
GEMINI_MODEL_NAME = 'gemini-1.5-flash-latest'
model = None # Global Gemini model instance
gc = None # Global gspread client instance
mbs_program_data_for_comparison = [] # Global MBS program data

# --- Validate Configuration ---
print("\n--- Validating Configuration ---")
config_valid = True; errors = []
# Kiểm tra các biến BẮT BUỘC phải có giá trị
if not SERVICE_ACCOUNT_JSON_PATH: errors.append("Service Account JSON Path is missing.")
elif IS_COLAB and not os.path.exists(SERVICE_ACCOUNT_JSON_PATH): errors.append(f"Service Account JSON file not found at: {SERVICE_ACCOUNT_JSON_PATH}") # Chỉ check exist nếu ở Colab
if not GEMINI_API_KEY: errors.append("Gemini API Key is missing.")
if not SPREADSHEET_URL or not SPREADSHEET_URL.startswith("https://docs.google.com/spreadsheets/d/"): errors.append("Invalid or missing Google Sheet URL.")
TARGET_WORKSHEET_NAME = TARGET_WORKSHEET_NAME.strip().strip('"');
if not TARGET_WORKSHEET_NAME: errors.append("Target Worksheet Name is empty.")
if not isinstance(HOURS_TO_CHECK, int) or HOURS_TO_CHECK <= 0: errors.append("Hours to Check must be > 0.")
if not isinstance(MAX_SCROLLS, int) or MAX_SCROLLS <= 0: errors.append("Max Scrolls must be > 0.")
# Thêm kiểm tra kiểu dữ liệu cho các số khác nếu cần
if not isinstance(SLEEP_BETWEEN_AI_CALLS, (int, float)) or SLEEP_BETWEEN_AI_CALLS < 0: errors.append("AI Sleep must be a non-negative number.")
if not isinstance(SHEET_WRITE_DELAY, (int, float)) or SHEET_WRITE_DELAY < 0: errors.append("Sheet Write Delay must be a non-negative number.")
if not isinstance(SHEET_WRITE_BATCH_SIZE, int) or SHEET_WRITE_BATCH_SIZE <= 0: errors.append("Sheet Write Batch Size must be > 0.")


if errors:
    config_valid = False
    print("❌ CONFIGURATION ERRORS:")
    for err in errors: print(f"   - {err}")
else:
    print("✅ Configuration validation passed.")
    print(f"   Source: {config_source}")
    print(f"   Service Account Path: {'Set (exists check varies)' if SERVICE_ACCOUNT_JSON_PATH else 'Not Set'}")
    print(f"   Gemini Key: {'Set' if GEMINI_API_KEY else 'Not Set'}")
    print(f"   Sheet URL: {SPREADSHEET_URL}")
    print(f"   Target Tab: {TARGET_WORKSHEET_NAME}")
    print(f"   Hours Check: {HOURS_TO_CHECK}, Max Scrolls: {MAX_SCROLLS}")
    print(f"   Sheet Header V3 ({len(SHEET_HEADER_V3)} cols): {SHEET_HEADER_V3}")

print("--- Configuration & Globals Loaded ---")

# Cell 4: Google Authentication

print("\n--- Attempting Google Authentication ---")

# Đảm bảo các thư viện cần thiết đã được import từ Cell 2
# (gspread, auth, default, IS_COLAB, traceback)


def authenticate_google_sheets(service_account_key_path: str): # Thêm tham số đường dẫn
    """Authenticates using a Service Account JSON key file."""
    print(f"🔑 Attempting Google Sheets authentication using Service Account: {service_account_key_path}")
    if not service_account_key_path or not os.path.exists(service_account_key_path):
         print(f"❌ Critical Error: Service Account key file not found at path: {service_account_key_path}")
         print("   Please upload the key file or check the path in Cell 2 (SERVICE_ACCOUNT_JSON_PATH).")
         return None

    try:
        # Sử dụng service_account thay vì phương thức cũ
        gc = gspread.service_account(filename=service_account_key_path)
        # Optional: Thử mở sheet để kiểm tra quyền truy cập sớm
        # gc.open_by_key(GOOGLE_SHEET_ID) # Cần GOOGLE_SHEET_ID ở đây hoặc truyền vào
        print("✅ Google Sheets authentication successful using Service Account.")
        return gc
    except FileNotFoundError:
         print(f"❌ Critical Error: Service Account key file explicitly not found at: {service_account_key_path}")
         return None
    except Exception as e:
        print(f"❌ Critical Error: Failed Google Sheets authentication with Service Account: {e}")
        print("   Hints: ")
        print("     - Ensure the JSON key file path is correct.")
        print("     - Ensure the JSON file itself is valid.")
        print("     - **Crucially:** Ensure the Service Account email (inside the JSON key) has EDIT permissions on the target Google Sheet(s).")
        print("     - Ensure the 'Google Sheets API' is enabled in the GCP project associated with the Service Account.")
        traceback.print_exc(limit=2) # In traceback để debug
        return None

# --- Chạy thử xác thực ---
if IS_COLAB:
    gc = authenticate_google_sheets(SERVICE_ACCOUNT_JSON_PATH) # Gán kết quả vào biến gc global
    if gc:
        print("\n✅ Authentication Test Successful! (Global 'gc' object created)")
        # Optional: Thử thao tác đơn giản để kiểm tra API
        try:
            print(f"   Attempting to open Sheet URL provided in config...")
            # Sử dụng SPREADSHEET_URL và TARGET_WORKSHEET_NAME từ Cell 3
            spreadsheet_test = gc.open_by_url(SPREADSHEET_URL)
            print(f"   Successfully opened Spreadsheet: '{spreadsheet_test.title}'")
            try:
                worksheet_test = spreadsheet_test.worksheet(TARGET_WORKSHEET_NAME)
                print(f"   Successfully accessed Worksheet: '{worksheet_test.title}'")
            except gspread.exceptions.WorksheetNotFound:
                print(f"   Warning: Worksheet '{TARGET_WORKSHEET_NAME}' not found, but sheet access seems OK.")
            except Exception as e_ws:
                 print(f"   ⚠️ Error accessing worksheet '{TARGET_WORKSHEET_NAME}': {e_ws}")
        except gspread.exceptions.APIError as e_api:
             print(f"   ❌ API Error opening sheet (Check URL, Share Permissions, and Sheets API enabled): {e_api}")
        except Exception as e_open:
            print(f"   ❌ Error opening spreadsheet (Check URL & Share Permissions): {e_open}")
    else:
        print("\n❌ Authentication Test Failed! (Global 'gc' is None)")
else:
    print("ℹ️ Skipping authentication test (not in Colab).")

# Cell 5: Định nghĩa các Hàm Helper và Hàm Crawl

# Cell 5.1

# --- Imports cần thiết cho các hàm trong cell này ---
import sys
import json
import pandas as pd
import pytz
import os
import time
import traceback
import gspread
import requests
import random
import re
import subprocess
import zipfile
import io
import locale # Cần cho parse_time với %A
from datetime import datetime, timedelta
import shutil # Dùng để xóa thư mục tạm

# Thêm các thư viện Selenium nếu chưa import ở Cell 2
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import (
        TimeoutException, WebDriverException, NoSuchElementException,
        StaleElementReferenceException, NoSuchWindowException
    )
    SELENIUM_AVAILABLE = True
except ImportError:
    print("⚠️ WARNING: Selenium library not fully imported. Web crawling will fail.")
    webdriver = None
    Options = None
    By = None
    WebDriverWait = None
    EC = None
    TimeoutException = WebDriverException = NoSuchElementException = StaleElementReferenceException = NoSuchWindowException = Exception # Fallback
    SELENIUM_AVAILABLE = False

# Thêm thư viện Google Auth nếu chưa import ở Cell 2
try:
    from google.oauth2 import service_account
    from google.auth import exceptions as google_auth_exceptions
    GOOGLE_AUTH_AVAILABLE = True
except ImportError:
     print("⚠️ WARNING: google-auth library not imported. Google Sheets/Drive auth will fail.")
     service_account = None
     google_auth_exceptions = None
     GOOGLE_AUTH_AVAILABLE = False

# Thêm thư viện Generative AI nếu chưa import ở Cell 2
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    print("⚠️ WARNING: google-generativeai library not imported. AI features will be disabled.")
    genai = None
    GEMINI_AVAILABLE = False

# Thêm thư viện files của Colab (nếu dùng save_data_local_v3)
try:
    from google.colab import files
    IS_COLAB = True
    IS_COLAB_FILES = True
except ImportError:
    files = None # Define files as None if not in Colab
    IS_COLAB = False
    IS_COLAB_FILES = False


print("\n--- Defining Helper and Crawler Functions ---")

# Cell 5.2
# === Environment Setup Functions

def install_chrome_and_driver():
    """Installs Google Chrome and the appropriate ChromeDriver using CfT API and apt fallback."""
    print("\n--- Installing Chrome & ChromeDriver ---")
    chromedriver_installed_successfully = False
    chromedriver_final_path = "/usr/local/bin/chromedriver" # Standard path

    try:
        # --- 1. Install Google Chrome ---
        print("Updating apt and installing prerequisites...")
        try:
            # Run apt update quietly
            subprocess.run(['sudo', 'apt-get', 'update'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            print("   Installing required packages (wget, gnupg, unzip, ca-certs, libvulkan1)...")
            # Install prerequisite packages
            install_prereqs_cmd = ['sudo', 'apt-get', 'install', '-y', 'wget', 'gnupg', 'unzip', 'ca-certificates', 'libvulkan1']
            prereq_result = subprocess.run(install_prereqs_cmd, check=False, capture_output=True, text=True)
            if prereq_result.returncode != 0:
                print(f"   ⚠️ Warning installing prerequisites (Code: {prereq_result.returncode}): {prereq_result.stderr[:500]}...")
            else:
                print("   Prerequisites installed/updated.")
        except Exception as e_apt_update:
             print(f"   ❌ FATAL ERROR during apt update/prerequisites installation: {e_apt_update}")
             return False # Cannot proceed without prerequisites

        print("\nDownloading and installing Google Chrome browser...")
        chrome_deb_path = "google-chrome-stable_current_amd64.deb"
        chrome_url = "https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb"
        try:
            # Download Chrome .deb file
            print(f"   Downloading from {chrome_url}...")
            subprocess.run(['wget', '-q', '-O', chrome_deb_path, chrome_url], check=True, timeout=180)
            print(f"   Downloaded Chrome package: {chrome_deb_path}")

            # Install using dpkg
            print("   Installing Chrome via dpkg...")
            dpkg_result = subprocess.run(['sudo', 'dpkg', '-i', chrome_deb_path], check=False, capture_output=True, text=True)
            if dpkg_result.returncode != 0:
                print(f"   dpkg install reported issues (Code: {dpkg_result.returncode}). Attempting dependency fix...")
                # print(f"   dpkg stderr:\n{dpkg_result.stderr}") # Uncomment for full dpkg error log

            # Fix potential dependencies using apt-get install -f
            print("   Running 'apt-get install -f' to resolve dependencies (if any)...")
            subprocess.run(['sudo', 'apt-get', 'install', '-f', '-y'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            print("   Dependency check/fix complete.")
            print("✅ Google Chrome browser installed successfully.")

        except subprocess.TimeoutExpired:
            print(f"   ❌ ERROR: Timeout occurred while downloading Chrome from {chrome_url}")
            return False
        except subprocess.CalledProcessError as e_chrome_install:
            print(f"   ❌ ERROR during Chrome download/install process: {e_chrome_install}")
            if hasattr(e_chrome_install, 'stderr'): print(f"   Stderr: {e_chrome_install.stderr}")
            return False
        except Exception as e_chrome:
            print(f"   ❌ UNEXPECTED ERROR during Chrome installation: {e_chrome}")
            return False
        finally:
             # Clean up the downloaded .deb file
             if os.path.exists(chrome_deb_path):
                 try: os.remove(chrome_deb_path); print(f"   Cleaned up {chrome_deb_path}")
                 except OSError as e_clean: print(f"   Warning: Could not remove {chrome_deb_path}: {e_clean}")

        # --- 2. Install ChromeDriver ---
        print("\nAttempting to install matching ChromeDriver...")
        # Get installed Chrome version
        chrome_version = None
        try:
            result = subprocess.run(['google-chrome', '--version'], capture_output=True, text=True, check=True)
            chrome_version = result.stdout.strip().split(' ')[-1]
            print(f"Detected installed Chrome version: {chrome_version}")
        except Exception as e_get_ver:
            print(f"Warning: Could not automatically detect Chrome version ({e_get_ver}). Will target latest Stable ChromeDriver.")

        # Find suitable ChromeDriver download URL using Chrome for Testing (CfT) JSON API
        print("Searching for ChromeDriver via Chrome for Testing (CfT) API...")
        api_endpoint = "https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json"
        chromedriver_url = None
        driver_source = "Unknown"
        download_version_info = "N/A"

        try:
            print(f"   Querying CfT API: {api_endpoint}")
            response = requests.get(api_endpoint, timeout=30)
            response.raise_for_status()
            versions_data = response.json()
            print("   Successfully fetched data from CfT API.")

            target_chrome_build = None
            if chrome_version:
                target_chrome_build = ".".join(chrome_version.split('.')[:3]) # Major.Minor.Build
                print(f"   Targeting ChromeDriver for Chrome build: {target_chrome_build}")

            found_driver_info = None
            # Try to find exact build match first
            if target_chrome_build:
                for channel, data in versions_data.get('channels', {}).items():
                    channel_version = data.get('version', '')
                    if channel_version.startswith(target_chrome_build):
                        downloads = data.get('downloads', {}).get('chromedriver', [])
                        url_info = next((d for d in downloads if d.get('platform') == 'linux64'), None)
                        if url_info:
                            found_driver_info = url_info
                            driver_source = f"CfT API (Exact Build: {channel} v{channel_version})"
                            download_version_info = channel_version
                            print(f"   Found exact build match: {driver_source}")
                            break

            # If no exact match or version unknown, get latest Stable
            if not found_driver_info:
                print(f"   Exact build match not found or Chrome version unknown. Getting latest Stable...")
                stable_data = versions_data.get('channels', {}).get('Stable', {})
                if stable_data:
                    stable_version = stable_data.get('version', 'Unknown Stable')
                    downloads = stable_data.get('downloads', {}).get('chromedriver', [])
                    url_info = next((d for d in downloads if d.get('platform') == 'linux64'), None)
                    if url_info:
                        found_driver_info = url_info
                        driver_source = f"CfT API (Latest Stable v{stable_version})"
                        download_version_info = stable_version
                        print(f"   Found latest Stable driver: {driver_source}")
                    else: print("   Could not find linux64 download for Stable in CfT API.")
                else: print("   Could not find 'Stable' channel data in CfT API.")

            if found_driver_info:
                chromedriver_url = found_driver_info.get('url')
            else:
                 # Trigger fallback if CfT API fails completely
                 raise ValueError("Failed to find any suitable ChromeDriver URL using CfT API.")

        except requests.exceptions.RequestException as req_err:
           print(f"   Error during CfT API request: {req_err}. Will attempt apt fallback.")
           raise ValueError(f"CfT API request failed: {req_err}") # Trigger fallback
        except Exception as e_api:
            print(f"   Unexpected error processing CfT API data: {e_api}. Will attempt apt fallback.")
            raise ValueError(f"CfT API processing error: {e_api}") # Trigger fallback

        # --- Download and Extract ChromeDriver ---
        if not chromedriver_url:
             raise ValueError("ChromeDriver URL is missing. Cannot proceed.")

        print(f"\nDownloading ChromeDriver ({driver_source}, Version: {download_version_info}) from: {chromedriver_url}")
        try:
            # Download the zip file
            zip_response = requests.get(chromedriver_url, stream=True, timeout=180)
            zip_response.raise_for_status()
            print("   Download complete.")

            # Extract to a temporary directory
            print("   Unzipping ChromeDriver...")
            zip_file_bytes = io.BytesIO(zip_response.content)
            extract_path = "/tmp/chromedriver_extract_temp"
            if os.path.exists(extract_path): # Clean up old temp dir if exists
                 shutil.rmtree(extract_path)
            os.makedirs(extract_path, exist_ok=True)

            # Find the chromedriver executable within the zip structure
            chromedriver_zip_internal_path = None
            with zipfile.ZipFile(zip_file_bytes) as zf:
                namelist = zf.namelist()
                # Common patterns: chromedriver-linux64/chromedriver or just chromedriver
                potential_paths = [m for m in namelist if m.endswith('/chromedriver') and not m.endswith('/')]
                if potential_paths:
                     chromedriver_zip_internal_path = potential_paths[0] # Take the first match
                elif 'chromedriver' in namelist:
                     chromedriver_zip_internal_path = 'chromedriver' # Check root level
                else:
                     raise FileNotFoundError(f"Could not find 'chromedriver' executable inside zip. Contents: {namelist}")

                print(f"   Extracting '{chromedriver_zip_internal_path}' to {extract_path}")
                zf.extract(chromedriver_zip_internal_path, extract_path)
                extracted_file_temp_path = os.path.join(extract_path, chromedriver_zip_internal_path)
                print(f"   Extracted temp path: {extracted_file_temp_path}")

            # Move to final destination and set permissions
            print(f"   Moving to final path: {chromedriver_final_path}")
            # Ensure destination directory exists (it should, but safer)
            os.makedirs(os.path.dirname(chromedriver_final_path), exist_ok=True)
            # Remove existing file/link at destination first
            if os.path.lexists(chromedriver_final_path):
                 subprocess.run(['sudo', 'rm', '-f', chromedriver_final_path], check=False)
            # Move the extracted file
            subprocess.run(['sudo', 'mv', extracted_file_temp_path, chromedriver_final_path], check=True)
            # Set execute permissions
            print("   Setting execute permissions...")
            subprocess.run(['sudo', 'chmod', '+x', chromedriver_final_path], check=True)

            # Clean up temporary extraction directory
            try: shutil.rmtree(extract_path); print(f"   Cleaned up temp directory: {extract_path}")
            except OSError as e_clean: print(f"   Warning: Could not cleanup temp dir {extract_path}: {e_clean}")

            # Final verification
            if os.path.exists(chromedriver_final_path) and os.access(chromedriver_final_path, os.X_OK):
                 print(f"✅ ChromeDriver (from {driver_source}) successfully installed at {chromedriver_final_path}.")
                 chromedriver_installed_successfully = True
                 # Verify installed version
                 try:
                     driver_version_output = subprocess.check_output([chromedriver_final_path, '--version']).decode('utf-8').strip()
                     print(f"   Installed ChromeDriver version: {driver_version_output}")
                 except Exception as e_check_ver: print(f"   Could not verify installed ChromeDriver version: {e_check_ver}")
            else:
                 raise FileNotFoundError(f"ChromeDriver verification failed at {chromedriver_final_path} after install.")

        except requests.exceptions.RequestException as e_download:
            print(f"   ❌ ERROR downloading ChromeDriver: {e_download}. Will attempt apt fallback.")
            raise # Trigger fallback
        except (zipfile.BadZipFile, FileNotFoundError) as e_zip:
            print(f"   ❌ ERROR processing ChromeDriver zip: {e_zip}. Will attempt apt fallback.")
            raise # Trigger fallback
        except subprocess.CalledProcessError as e_mv_chmod:
            print(f"   ❌ ERROR moving/setting permissions: {e_mv_chmod}. Stderr: {e_mv_chmod.stderr}")
            print("   Will attempt apt fallback.")
            raise # Trigger fallback
        except Exception as e_unzip:
             print(f"   ❌ UNEXPECTED ERROR during download/unzip: {e_unzip}. Will attempt apt fallback.")
             raise # Trigger fallback

    # --- Fallback: Install via apt ---
    except Exception as e_install_driver: # Catch errors raised from download/install steps
        print(f"\n--- ChromeDriver download/install failed ({e_install_driver}). Attempting apt fallback. ---")
        try:
            print("   Running 'apt-get update' before apt install...")
            subprocess.run(['sudo', 'apt-get', 'update'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)

            print("   Installing 'chromium-chromedriver' package via apt...")
            apt_install_cmd = ['sudo', 'apt-get', 'install', '-y', 'chromium-chromedriver']
            apt_result = subprocess.run(apt_install_cmd, check=False, capture_output=True, text=True)

            if apt_result.returncode != 0:
                 print(f"   ❌ ERROR installing chromium-chromedriver via apt (Code: {apt_result.returncode}).")
                 print(f"   Stderr: {apt_result.stderr[:500]}...")
                 raise Exception("apt install chromium-chromedriver failed") # Signal failure
            else:
                 print("   'chromium-chromedriver' package installed via apt.")

            # Find the path where apt installed chromedriver
            fallback_paths = ["/usr/lib/chromium-browser/chromedriver", "/usr/bin/chromedriver"]
            installed_fallback_path = next((p for p in fallback_paths if os.path.exists(p) and os.access(p, os.X_OK)), None)

            if installed_fallback_path:
                 print(f"   Found fallback chromedriver from apt at: {installed_fallback_path}")
                 # Ensure the standard path links to the apt version
                 link_needed = True
                 if os.path.lexists(chromedriver_final_path):
                     if os.path.islink(chromedriver_final_path) and os.readlink(chromedriver_final_path) == installed_fallback_path:
                          print(f"   Symlink already correct: {chromedriver_final_path} -> {installed_fallback_path}")
                          link_needed = False
                          chromedriver_installed_successfully = True
                     else:
                          print(f"   Removing incorrect file/link at {chromedriver_final_path}...")
                          subprocess.run(['sudo', 'rm', '-f', chromedriver_final_path], check=False)

                 if link_needed:
                     print(f"   Creating symlink: {chromedriver_final_path} -> {installed_fallback_path}")
                     subprocess.run(['sudo', 'ln', '-sf', installed_fallback_path, chromedriver_final_path], check=True)
                     # Verify symlink creation
                     if os.path.exists(chromedriver_final_path) and os.access(chromedriver_final_path, os.X_OK) and os.path.islink(chromedriver_final_path):
                         print("   Symlink created successfully.")
                         chromedriver_installed_successfully = True
                     else: print(f"   ❌ Failed to create/verify symlink at {chromedriver_final_path}.")

                 # Verify version of the fallback driver via the symlink
                 if chromedriver_installed_successfully:
                     try:
                         driver_version_output = subprocess.check_output([chromedriver_final_path, '--version']).decode('utf-8').strip()
                         print(f"   Fallback ChromeDriver (apt) version: {driver_version_output}")
                     except Exception as e_check_ver_fb: print(f"   Could not verify fallback driver version: {e_check_ver_fb}")

            else: # apt install ran, but executable not found
                 print(f"   ❌ ERROR: apt install finished, but chromedriver executable not found in {fallback_paths}.")

        except Exception as fallback_err:
            print(f"   ❌❌ FATAL ERROR during apt fallback installation process: {fallback_err}")
            chromedriver_installed_successfully = False # Ensure failure state

    # --- Final Conclusion ---
    print("-" * 60)
    if chromedriver_installed_successfully:
        print("✅✅ Chrome & ChromeDriver Installation Attempt Completed Successfully. ✅✅")
        return True
    else:
        print("❌❌ Chrome & ChromeDriver Installation Attempt Failed after all methods. ❌❌")
        print("    Web crawling might not work. Check logs above for details.")
        return False
# --- End install_chrome_and_driver ---

# Cell 5.3
# === AI Model Functions               ===

import traceback
import json
import time # Thêm import time để dùng sleep

# Assume 'genai' and 'google_auth_exceptions' are imported elsewhere or mocked
# Example imports if needed:
try:
    import google.generativeai as genai
    from google.auth import exceptions as google_auth_exceptions
    # Define GEMINI_AVAILABLE based on successful import
    GEMINI_AVAILABLE = True
except ImportError:
    print("Warning: google.generativeai or google.auth failed to import. Gemini features may be unavailable.")
    genai = None
    google_auth_exceptions = None
    GEMINI_AVAILABLE = False # Set flag to False

# --- Globals (Assuming defined elsewhere, provide defaults for robustness) ---
# Cần đảm bảo các biến này được gán giá trị ở cell khác
GEMINI_API_KEY = globals().get("GEMINI_API_KEY", None) # Example fetch from globals
GEMINI_MODEL_NAME = globals().get("GEMINI_MODEL_NAME", "gemini-1.5-flash") # Example, use your actual model
model = globals().get("model", None) # Global model variable

# Function Definitions start at Level 0 indentation
def setup_gemini():
    """Configures and initializes the global Gemini AI model instance."""
    # Inside function: Level 1 indentation (4 spaces)
    global model, GEMINI_API_KEY, GEMINI_MODEL_NAME, GEMINI_AVAILABLE # Access global vars

    print(f"\n--- Configuring Gemini AI (Model: {GEMINI_MODEL_NAME}) ---")

    # Check if library is available (using the flag set during import)
    if not GEMINI_AVAILABLE or not genai or not google_auth_exceptions:
        # Inside 'if': Level 2 indentation (8 spaces)
        print("❌ ERROR: google-generativeai library not available or failed to import. Cannot set up Gemini.")
        model = None
        return # Exit the function

    # Check for API Key
    if not GEMINI_API_KEY:
        # Inside 'if': Level 2 indentation (8 spaces)
        print("❌ ERROR: Gemini API Key is missing (Check Cell 3 configuration).")
        model = None
        return # Exit the function

    model = None # Reset model before setup (Level 1)
    try: # Level 1
        # Inside 'try': Level 2 indentation (8 spaces)
        print(f"   Using API Key ending in '...{GEMINI_API_KEY[-4:]}'")
        genai.configure(api_key=GEMINI_API_KEY)

        # Initialize the Generative Model
        print(f"   Initializing model: {GEMINI_MODEL_NAME}")
        # --- Sửa đổi nhỏ: Đảm bảo safety_settings mặc định được áp dụng khi khởi tạo model ---
        # Các cài đặt an toàn tiêu chuẩn
        default_safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]
        model_instance = genai.GenerativeModel(
            GEMINI_MODEL_NAME,
            safety_settings=default_safety_settings # Áp dụng cài đặt an toàn mặc định
        )

        # Verify model access with a simple, short generation
        print(f"   Verifying model access with a test prompt...")
        # Check if genai.types exists before using it
        gen_config_class = getattr(genai, 'types', None)
        if gen_config_class and hasattr(gen_config_class, 'GenerationConfig'):
            # Inside 'if': Level 3 indentation (12 spaces)
            test_config = gen_config_class.GenerationConfig(max_output_tokens=10, temperature=0.1)
            # Sử dụng safety settings đã định nghĩa ở trên cho test call luôn
            _ = model_instance.generate_content("Say 'test ok'", generation_config=test_config) # Safety settings đã có trong model
        else: # Level 2
            # Inside 'else': Level 3 indentation (12 spaces)
            print("   [Warning] genai.types.GenerationConfig not found, skipping detailed verification.")
            _ = model_instance.generate_content("Say 'test ok'") # Simpler verification

        # Assign to global variable only after successful verification (Level 2)
        model = model_instance
        print(f"✅ Global Gemini Model '{GEMINI_MODEL_NAME}' Initialized and Verified.")

    # CORRECTED EXCEPTION HANDLING STARTS HERE:
    # Block 1: Catch the specific error IF the module was imported
    except google_auth_exceptions.DefaultCredentialsError as cred_err: # Level 1
        print(f"❌ ERROR: Google Cloud Credentials Error: {cred_err}")
        print("   Ensure your environment is authenticated if not using API key directly.")
        model = None
    # Block 2: Catch ANY other exception
    except Exception as config_err: # Level 1
        error_str = str(config_err).lower()
        if "api key not valid" in error_str or "permission denied" in error_str or "authenticate" in error_str:
             print(f"❌ ERROR: Invalid Gemini API Key or Permission Denied. Check the key in Cell 3.")
        elif "could not find model" in error_str or "model_name is invalid" in error_str:
             print(f"❌ ERROR: Gemini Model '{GEMINI_MODEL_NAME}' not found or unavailable. Check the model name.")
        elif "quota" in error_str or "rate limit" in error_str or "resource exhausted" in error_str:
             print(f"❌ ERROR: Quota or Rate Limit likely exceeded during setup/verification: {config_err}")
        elif "deadline exceeded" in error_str:
             print(f"❌ ERROR: Timeout during setup/verification call to Gemini API: {config_err}")
        else:
             print(f"❌ UNEXPECTED ERROR Configuring/Verifying Gemini: {config_err}")
             traceback.print_exc(limit=1)
        model = None
# --- End setup_gemini ---


# Function Definition start at Level 0 indentation
def get_default_analysis_result(error_message="Analysis failed"):
    """Returns a default, structured analysis result dictionary on error."""
    # Ensure keys match the successful output structure of analyze_with_gemini
    return {
        "primary_competitor_focus": "Lỗi AI",
        "is_program_pr": False,
        "promoted_program_name": "",
        "is_brand_pr": False, # <<< THÊM MỚI
        "program_details": { # Default empty details structure
            "ThoiGianTrienKhai": "Không rõ", "LoaiHinhSanPham": "Không rõ",
            "DieuKienThamGia": "Không rõ", "DacDiemChinh": "Không rõ",
            "UuDaiNeuCo": "Không rõ", "DoiTuongKhachHang": "Không rõ"
        },
        "analysis_summary": f"Lỗi: {error_message}"
    }
# --- End get_default_analysis_result ---


# Function Definition start at Level 0 indentation
def analyze_with_gemini(model, title, content, detected_competitors_list):
    """
    Analyzes article using LLM for focus, PR type, program details.
    Expects JSON output. Includes retry logic.
    """
    # --- Input Validation and Preparation ---
    if not model:
        print("   [AnalyzerV2] Skipping analysis: AI Model not available.")
        return get_default_analysis_result("AI Model not available")
    if not title and not content:
        print("   [AnalyzerV2] Skipping analysis: Empty title and content.")
        return get_default_analysis_result("Empty title and content")

    competitor_context = "Không có đối thủ nào được xác định trước"
    if detected_competitors_list:
        unique_competitors = sorted(list(set(filter(None, map(str, detected_competitors_list)))))
        if unique_competitors:
             competitor_context = ", ".join(unique_competitors)
        else:
            print("   [AnalyzerV2] Warning: Competitor list provided but contained no valid names.")

    max_content_length = 8000 # Giữ nguyên hoặc điều chỉnh nếu cần
    truncated_content = content[:max_content_length] if content else ""
    if content and len(content) > max_content_length:
        print(f"   [AnalyzerV2] Input content truncated to {max_content_length} chars.")

    # --- Define Prompt (V2 JSON Structure) ---
    detail_fields_prompt = """
        - ThoiGianTrienKhai: (String) Thời gian bắt đầu/kết thúc chương trình nếu có (ví dụ: "01/08/2024 - 30/09/2024", "Tháng 8/2024"). Mặc định "Không rõ".
        - LoaiHinhSanPham: (String) Loại sản phẩm/dịch vụ chính (ví dụ: "Vay Margin", "Mở tài khoản"). Mặc định "Không rõ".
        - DieuKienThamGia: (String) Điều kiện chính để tham gia (ví dụ: "Khách hàng mới", "Giao dịch tối thiểu 1 tỷ"). Mặc định "Không rõ".
        - DacDiemChinh: (String) Mô tả ngắn gọn đặc điểm nổi bật nhất. Mặc định "Không rõ".
        - UuDaiNeuCo: (String) Ưu đãi cụ thể nếu có (ví dụ: "Lãi suất 9%", "Miễn phí giao dịch"). Mặc định "Không rõ".
        - DoiTuongKhachHang: (String) Đối tượng khách hàng mục tiêu (ví dụ: "Nhà đầu tư cá nhân"). Mặc định "Không rõ".
    """
    prompt = f"""
Phân tích bài báo tài chính tiếng Việt sau. Đối thủ cạnh tranh có thể liên quan: [{competitor_context}].

**Tiêu đề:** "{title}"

**Nội dung (trích đoạn tối đa {max_content_length} ký tự):**
"{truncated_content}"

**Yêu cầu Phân tích:**
1.  **Trọng tâm chính (primary_competitor_focus):** Công ty nào trong [{competitor_context}] (nếu có) là trọng tâm chính của bài? (Trả về tên công ty hoặc "N/A").
2.  **Là PR Chương trình? (is_program_pr):** Bài viết có phải PR/quảng cáo cho một chương trình/sản phẩm/dịch vụ CỤ THỂ của công ty trọng tâm không? (Trả về boolean `true` hoặc `false`).
3.  **Tên Chương trình (promoted_program_name):** Nếu (2) là `true`, trích xuất tên chương trình/sản phẩm đó. (Trả về string, hoặc `""` nếu không áp dụng).
4.  **Chi tiết Chương trình (program_details):** Nếu (2) là `true`, trích xuất chi tiết theo cấu trúc sau (điền "Không rõ" nếu thiếu). Nếu (2) là `false`, trả về object rỗng {{}}.
    {detail_fields_prompt}
5.  **Là PR Thương hiệu? (is_brand_pr):** Ngay cả khi không phải PR cho một chương trình cụ thể (tức là mục 2 là `false`), bài viết này có mục đích chính là quảng bá hình ảnh, uy tín, thành tựu chung, hoặc là một bài phỏng vấn lãnh đạo mang tính tích cực cho công ty trọng tâm (ở mục 1) không? (Trả về boolean `true` hoặc `false`).
6.  **Tóm tắt Phân tích (analysis_summary):** Tóm tắt ngắn gọn (1-2 câu) mục đích/nội dung chính của bài viết liên quan đến công ty trọng tâm (hoặc tóm tắt chung nếu không có trọng tâm).

**Định dạng Output:**
Chỉ trả về một JSON object DUY NHẤT, hợp lệ, không có giải thích bên ngoài. Các key phải đúng như trong ví dụ.

**Ví dụ JSON (Trường hợp PR Chương trình):**
```json
{{
  "primary_competitor_focus": "VPS",
  "is_program_pr": true,
  "promoted_program_name": "Ưu đãi phí giao dịch 0%",
  "is_brand_pr": false,
  "program_details": {{
    "ThoiGianTrienKhai": "Từ 01/07/2024", "LoaiHinhSanPham": "Giao dịch chứng khoán cơ sở",
    "DieuKienThamGia": "Khách hàng mở tài khoản mới", "DacDiemChinh": "Miễn phí giao dịch chứng khoán cơ sở.",
    "UuDaiNeuCo": "Phí giao dịch 0%", "DoiTuongKhachHang": "Nhà đầu tư cá nhân"
  }},
  "analysis_summary": "Bài viết quảng bá chương trình miễn phí giao dịch 0% của VPS dành cho khách hàng mới."
}}

{{
  "primary_competitor_focus": "SSI",
  "is_program_pr": false,
  "promoted_program_name": "",
  "is_brand_pr": true,
  "program_details": {{}},
  "analysis_summary": "Bài phỏng vấn Chủ tịch SSI về triển vọng và niềm tin vào thị trường, nhấn mạnh tiềm năng của công ty."
}}
```"""

    # === PHẦN BỔ SUNG: GỌI API VÀ XỬ LÝ KẾT QUẢ ===
    MAX_RETRIES = 2
    RETRY_DELAY = 5 # Giây

    for attempt in range(MAX_RETRIES):
        try:
            print(f"   [AnalyzerV2 Attempt {attempt+1}/{MAX_RETRIES}] Calling AI for '{title[:50]}...'")

            # --- Cấu hình cho lời gọi API ---
            # Yêu cầu trả về JSON và đặt nhiệt độ thấp để bám sát sự thật
            gen_config = genai.types.GenerationConfig(
                temperature=0.2, # Giữ nguyên hoặc tăng nhẹ nếu muốn AI linh hoạt hơn trong việc nhận diện PR thương hiệu
                response_mime_type="application/json"
            )

            request_options = {'timeout': 120} # Giữ nguyên hoặc tăng nếu prompt dài hơn

            # --- Gọi API ---
            response = model.generate_content(
                prompt,
                generation_config=gen_config,
                request_options=request_options
            )
            print(f"   [AnalyzerV2 Attempt {attempt+1}] Response received.")

            # --- Kiểm tra nội dung bị chặn ---
            if not response.parts:
                 block_reason = "Unknown"
                 if hasattr(response, 'prompt_feedback') and hasattr(response.prompt_feedback,'block_reason'):
                      block_reason = response.prompt_feedback.block_reason
                 if block_reason == 'SAFETY':
                      print(f"   [AnalyzerV2 Attempt {attempt+1}] ⚠️ Content blocked by SAFETY filter. Retrying if possible...")
                      if attempt < MAX_RETRIES - 1:
                           time.sleep(RETRY_DELAY)
                           continue
                      else:
                           raise ValueError(f"Content blocked by SAFETY filter after {MAX_RETRIES} attempts.")
                 else:
                      raise ValueError(f"Content blocked by API. Reason: {block_reason}")

            # --- Parse JSON (với fallback) ---
            response_text = response.text
            result_dict = None
            try:
                result_dict = json.loads(response_text)
                print(f"   [AnalyzerV2 Attempt {attempt+1}] ✅ Parsed Direct JSON.")
            except json.JSONDecodeError:
                print(f"   [AnalyzerV2 Attempt {attempt+1}] ⚠️ Direct JSON parse failed. Cleaning and retrying...")
                cleaned_text = response_text.strip().lstrip('```json').rstrip('```').strip()
                if not cleaned_text:
                    if attempt < MAX_RETRIES - 1:
                        print(f"   [AnalyzerV2 Attempt {attempt+1}] Cleaned JSON empty. Retrying...")
                        time.sleep(RETRY_DELAY)
                        continue
                    else:
                        raise ValueError("Cleaned JSON response is empty after retries.")
                try:
                    result_dict = json.loads(cleaned_text)
                    print(f"   [AnalyzerV2 Attempt {attempt+1}] ✅ Parsed Cleaned JSON.")
                except json.JSONDecodeError as e_json_clean:
                    if attempt < MAX_RETRIES - 1:
                        print(f"   [AnalyzerV2 Attempt {attempt+1}] Cleaned JSON parse failed: {e_json_clean}. Retrying...")
                        time.sleep(RETRY_DELAY)
                        continue
                    else:
                         raise ValueError(f"Failed to parse JSON even after cleaning and retries: {e_json_clean}. Original text: {response_text[:200]}...")

            # --- Xác thực và Chuẩn hóa kết quả ---
            final_result = {
                "primary_competitor_focus": str(result_dict.get("primary_competitor_focus", "N/A")).strip(),
                "is_program_pr": bool(result_dict.get("is_program_pr", False)),
                "promoted_program_name": str(result_dict.get("promoted_program_name", "")).strip(),
                "is_brand_pr": bool(result_dict.get("is_brand_pr", False)), # <<< THÊM MỚI
                "program_details": {},
                "analysis_summary": str(result_dict.get("analysis_summary", "N/A")).strip()
            }

            if final_result["is_program_pr"]:
                raw_details = result_dict.get("program_details")
                if isinstance(raw_details, dict) and raw_details:
                    final_result["program_details"] = {
                        "ThoiGianTrienKhai": str(raw_details.get("ThoiGianTrienKhai", "Không rõ")).strip(),
                        "LoaiHinhSanPham": str(raw_details.get("LoaiHinhSanPham", "Không rõ")).strip(),
                        "DieuKienThamGia": str(raw_details.get("DieuKienThamGia", "Không rõ")).strip(),
                        "DacDiemChinh": str(raw_details.get("DacDiemChinh", "Không rõ")).strip(),
                        "UuDaiNeuCo": str(raw_details.get("UuDaiNeuCo", "Không rõ")).strip(),
                        "DoiTuongKhachHang": str(raw_details.get("DoiTuongKhachHang", "Không rõ")).strip()
                    }
                elif raw_details is None or not isinstance(raw_details, dict):
                    print(f"   [AnalyzerV2] Warning: program_details was not a valid dict from LLM (is_program_pr=True). Received: {type(raw_details)}. Setting to default.")
                    default_details = get_default_analysis_result()["program_details"]
                    final_result["program_details"] = default_details

            print(f"   [AnalyzerV2] ✅ Analysis successful for '{title[:50]}...'")
            return final_result

        except Exception as e:
            print(f"❌ ERROR during AI Analysis attempt {attempt+1} for '{title[:50]}...': {e}")
            if attempt < MAX_RETRIES - 1:
                print(f"   Retrying after {RETRY_DELAY} seconds...")
                time.sleep(RETRY_DELAY)
            else:
                print(f"   [AnalyzerV2] Max retries reached. Returning error result.")
                # traceback.print_exc(limit=2)
                return get_default_analysis_result(f"Lỗi phân tích AI sau {MAX_RETRIES} lần thử: {str(e)[:150]}")

    print(f"   [AnalyzerV2] Reached end of analysis function unexpectedly after loop for '{title[:50]}...'")
    return get_default_analysis_result("Lỗi logic không mong muốn trong hàm phân tích")

# --- End analyze_with_gemini ---

## Cell 5.4
import json
# Assume 'genai' is imported correctly elsewhere if needed for the API call
# import google.generativeai as genai # Example import

# Mock model object for testing if you don't have the real one
class MockModel:
    def generate_content(self, *args, **kwargs):
        # Simulate a successful response structure
        class MockResponse:
            def __init__(self):
                # Example valid JSON response
                self.text = json.dumps({
                    "best_mbs_match_name": "MBS Example Program",
                    "similarity_high": True,
                    "similarities": "Both target individual investors.",
                    "differences": "New program has higher minimum investment."
                })
                self.parts = [self.text] # Simulate having parts
        return MockResponse()

# Replace with your actual model or keep MockModel for testing
# model = genai.GenerativeModel(...)
model = MockModel() # Using the mock model for this example

def compare_with_mbs_programs_llm(model, new_program_name, new_program_details_dict, mbs_programs_list):
    """
    Compares a new program with MBS list using LLM. Expects JSON output.
    Includes error handling and clearer default results.
    """
    # --- Default Result Structure ---
    comparison_results = {
        'best_mbs_match_name': "N/A",          # Default: Not Applicable / No Comparison
        'similarity_high': False,
        'similarities': "N/A",
        'differences': "N/A"
    }

    # --- Input Validation ---
    if not model:
        print("   [Comparer] Skipping comparison: AI Model unavailable.")
        # Indicate error clearly in the result
        return {**comparison_results, 'best_mbs_match_name': "Lỗi (No AI Model)"}
    if not new_program_name:
         print("   [Comparer] Skipping comparison: New program name is empty.")
         return {**comparison_results, 'best_mbs_match_name': "Lỗi (Tên CT mới rỗng)"}
    if not mbs_programs_list:
        print("   [Comparer] Skipping comparison: MBS program list is empty or unavailable.")
        # Distinguish between empty list and other issues if possible
        return {**comparison_results, 'best_mbs_match_name': "Không có CT MBS để SS"}

    # --- Prepare Prompt Inputs ---
    # Create a concise description for the new program
    new_details_parts = []
    # Focus on key differentiating aspects for comparison
    key_aspects = ['LoaiHinhSanPham', 'DacDiemChinh', 'UuDaiNeuCo', 'DoiTuongKhachHang', 'ThoiGianTrienKhai']
    for key in key_aspects:
        value = new_program_details_dict.get(key)
        # Only include meaningful values
        if value and str(value).strip() and str(value).strip().lower() != "không rõ":
            # Use shorter keys in the prompt for brevity
            short_key_map = {
                'LoaiHinhSanPham': 'Loại SP', 'DacDiemChinh': 'Đặc điểm',
                'UuDaiNeuCo': 'Ưu đãi', 'DoiTuongKhachHang': 'Đối tượng',
                'ThoiGianTrienKhai': 'TG', 'DieuKienThamGia': 'ĐK'
            }
            short_key = short_key_map.get(key, key) # Fallback to original key if not in map
            # Limit value length to avoid overly long prompts
            new_details_parts.append(f"{short_key}: {str(value)[:100]}")
    new_details_str = "; ".join(new_details_parts) if new_details_parts else "Không có mô tả chi tiết."

    # Create list of MBS programs (Name and 'details' which should be 'Đặc điểm chính')
    mbs_program_descs = []
    for prog in mbs_programs_list:
        name = prog.get('name','').strip()
        # 'details' comes from 'Đặc điểm chính' during read - Make sure this key exists in your data
        details = prog.get('details','').strip()
        if name: # Only include programs with a name
             # Limit length of description per program
             mbs_program_descs.append(f"- Tên: {name}\n  Mô tả: {details[:150]}")

    if not mbs_program_descs:
        print("   [Comparer] Skipping comparison: No valid MBS programs with names found in the provided list.")
        return {**comparison_results, 'best_mbs_match_name': "Không có CT MBS hợp lệ"}

    mbs_list_str = "\n".join(mbs_program_descs)

    # --- Define Comparison Prompt ---
    # Prompt asking for JSON output with specific keys and structure
    prompt = f"""
So sánh chương trình/sản phẩm tài chính MỚI sau đây:
### Chương trình Mới:
Tên: "{new_program_name}"
Mô tả: "{new_details_str}"

### Danh sách Chương trình Hiện có của MBS:
{mbs_list_str}

### Yêu cầu So sánh:
1.  **Tìm Chương trình Tương đồng Nhất:** Từ danh sách MBS, hãy tìm ra chương trình DUY NHẤT có vẻ tương đồng nhất về mặt mục tiêu, đối tượng hoặc loại hình sản phẩm so với chương trình mới. Nếu không có chương trình nào thực sự tương đồng đáng kể, trả về chuỗi "Không tìm thấy".
2.  **Đánh giá Độ Tương đồng:** Mức độ tương đồng giữa chương trình mới và chương trình MBS tìm được (ở bước 1) là CAO (`true`) hay THẤP (`false`)? Chỉ đánh giá là CAO nếu chúng có cùng ý tưởng cốt lõi hoặc giải quyết cùng một nhu cầu chính của khách hàng. Nếu không tìm thấy chương trình MBS tương đồng, kết quả phải là `false`.
3.  **Điểm Giống nhau:** Nếu độ tương đồng là CAO (`true`), hãy liệt kê 1-2 điểm giống nhau chính yếu nhất giữa hai chương trình. Nếu độ tương đồng THẤP, trả về "N/A".
4.  **Điểm Khác biệt:** Nếu độ tương đồng là CAO (`true`), hãy liệt kê 1-2 điểm khác biệt chính yếu nhất (nếu có) giữa hai chương trình. Nếu độ tương đồng THẤP, trả về "N/A".

### Định dạng Output:
Chỉ trả về một JSON object DUY NHẤT, hợp lệ, không có giải thích bên ngoài. Các khóa phải là: `best_mbs_match_name` (String), `similarity_high` (Boolean), `similarities` (String), `differences` (String).

Ví dụ JSON (Tương đồng cao):
```json
{{
  "best_mbs_match_name": "MBS Margin 9.9",
  "similarity_high": true,
  "similarities": "Cùng là chương trình cho vay margin với lãi suất ưu đãi; Nhắm vào NĐT cá nhân.",
  "differences": "Chương trình mới có điều kiện tham gia khác; Thời gian ưu đãi của MBS dài hơn."
}}
```""" # End of the f-string prompt

    # --- API Call ---
    try:
        print(f"   [Comparer] Comparing '{new_program_name}' with {len(mbs_program_descs)} MBS programs...")
        # Configure generation settings for comparison
        # Ensure genai is imported and configured if using the real API
        # gen_config = genai.types.GenerationConfig(
        #     temperature=0.1, # Low temperature for factual comparison
        #     response_mime_type="application/json" # Expect JSON
        # )
        # Dummy config for mock model or if genai not used directly here
        gen_config = {"temperature": 0.1, "response_mime_type": "application/json"}

        # Use less strict safety settings for comparison task if needed, or standard ones
        safety_settings_comp = [
             {"category": c, "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
             for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH",
                       "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]
        ]

        response = model.generate_content(
            prompt,
            # generation_config=gen_config, # Use this if using real genai
            # safety_settings=safety_settings_comp, # Use this if using real genai
            request_options={'timeout': 120} # Reasonable timeout for comparison task
            # Pass dummy kwargs if needed by mock model, or remove if using real API
            , generation_config_dummy=gen_config, safety_settings_dummy=safety_settings_comp
        )

        # Check for blocked content before parsing
        # Accessing prompt_feedback might differ depending on the actual library/response object
        # This is a common pattern for Google's API
        if not response.parts:
             block_reason = "Unknown"
             # Check if feedback exists (adjust based on actual library)
             if hasattr(response, 'prompt_feedback') and hasattr(response.prompt_feedback,'block_reason'):
                block_reason = response.prompt_feedback.block_reason
             raise ValueError(f"Comparer content blocked by API. Reason: {block_reason}")
        print(f"   [Comparer] Response received.")

        # --- Process and Parse Response ---
        result_dict = None
        response_text = response.text
        try:
            # Try direct JSON parsing first
            result_dict = json.loads(response_text)
            print(f"   [Comparer] ✅ Comparison successful (Parsed Direct JSON).")
        except json.JSONDecodeError:
             # Fallback: Clean potential markdown ```json ... ``` wrappers
             print(f"   [Comparer] ⚠️ Direct JSON parse failed. Cleaning and retrying...")
             # Improved cleaning: handle potential leading/trailing whitespace and markdown fences
             cleaned_text = response_text.strip()
             if cleaned_text.startswith("```json"):
                 cleaned_text = cleaned_text[len("```json"):].strip()
             if cleaned_text.startswith("```"):
                  cleaned_text = cleaned_text[len("```"):].strip()
             if cleaned_text.endswith("```"):
                 cleaned_text = cleaned_text[:-len("```")].strip()

             if not cleaned_text: raise ValueError("Comparer cleaned JSON response is empty.")
             result_dict = json.loads(cleaned_text) # Try parsing cleaned text
             print(f"   [Comparer] ✅ Comparison successful (Parsed Cleaned JSON).")

        # --- Normalize and Validate Result Dictionary ---
        # Ensure keys exist before processing, provide defaults
        final_match = str(result_dict.get('best_mbs_match_name', "Không tìm thấy")).strip()
        final_sim_high = bool(result_dict.get('similarity_high', False))
        final_sim = "N/A" # Default for similarities/differences
        final_diff = "N/A"

        # Only populate similarities/differences if similarity is high
        if final_sim_high:
            sim_text = str(result_dict.get('similarities', "N/A")).strip()
            diff_text = str(result_dict.get('differences', "N/A")).strip()
            # Use more descriptive defaults if AI returns "N/A" despite high similarity
            final_sim = sim_text if sim_text and sim_text.lower() != "n/a" else "Không có điểm giống nổi bật được nêu."
            final_diff = diff_text if diff_text and diff_text.lower() != "n/a" else "Không có điểm khác biệt nổi bật được nêu."

            # Ensure a match name is present if similarity is high, unless it was explicitly "Không tìm thấy"
            if not final_match or final_match.lower() == "n/a" or final_match == "Không tìm thấy":
                 print(f"   [Comparer] Warning: Similarity rated high, but no valid match name provided ('{final_match}'). Setting default.")
                 # Decide on a default or leave as is depending on desired behavior
                 final_match = "Tương đồng cao nhưng không rõ tên CT khớp" # Example default
                 # Or maybe force similarity low if name is missing? Depends on strictness needed.
                 # final_sim_high = False # Option: Force low similarity if name is missing

        else: # similarity_high is False
            # Ensure match is "Không tìm thấy" if similarity is low
            if final_match != "Không tìm thấy":
                 print(f"   [Comparer] Warning: Similarity rated low, but match found ('{final_match}'). Overriding match to 'Không tìm thấy'.")
                 final_match = "Không tìm thấy"
            final_sim = "N/A (Độ tương đồng thấp)"
            final_diff = "N/A (Độ tương đồng thấp)"


        # Return the final, validated comparison dictionary
        return {
            'best_mbs_match_name': final_match,
            'similarity_high': final_sim_high,
            'similarities': final_sim,
            'differences': final_diff
        }

    # --- Error Handling for API Call / Processing ---
    except Exception as e:
        print(f"   [Comparer] ❌ ERROR during comparison for '{new_program_name}': {e}")
        # Log the full traceback for detailed debugging if needed
        # import traceback
        # print(traceback.format_exc())
        # Return a clear error state
        return {
            'best_mbs_match_name': "Lỗi So Sánh",
            'similarity_high': False,
            'similarities': f"Lỗi: {str(e)[:100]}", # Include brief error message
            'differences': "N/A"
        }
# --- End compare_with_mbs_programs_llm ---

# --- Example Usage ---
# Mock Input Data
new_prog_name_example = "Super Saver Account"
new_prog_details_example = {
    'LoaiHinhSanPham': 'Tài khoản tiết kiệm',
    'DacDiemChinh': 'Lãi suất bậc thang theo số dư, miễn phí quản lý',
    'UuDaiNeuCo': 'Tặng voucher du lịch cho số dư > 1 tỷ',
    'DoiTuongKhachHang': 'Khách hàng cá nhân có thu nhập ổn định',
    'ThoiGianTrienKhai': '01/01/2024 - 31/12/2024',
    'DieuKienThamGia': 'Số dư tối thiểu 10 triệu VND'
}
mbs_programs_example = [
    {'name': 'MBS Saving Plus', 'details': 'Tài khoản tiết kiệm lãi suất cố định 6%/năm.'},
    {'name': 'MBS Invest Pro', 'details': 'Sản phẩm đầu tư trái phiếu doanh nghiệp.'},
    {'name': 'MBS Margin 9.9', 'details': 'Cho vay ký quỹ lãi suất ưu đãi 9.9%.'},
    {'name': '  ', 'details': 'Invalid program with no name'}, # Test invalid entry
    {'name': 'MBS StepUp Savings', 'details': 'Tiết kiệm lãi suất tăng dần theo thời gian gửi.'}, # Potentially similar
]

# Run the comparison
comparison_result = compare_with_mbs_programs_llm(
    model, # Using the MockModel defined earlier
    new_prog_name_example,
    new_prog_details_example,
    mbs_programs_example
)

print("\n--- Comparison Result ---")
print(json.dumps(comparison_result, indent=2, ensure_ascii=False))

## Cell 5.5
import pytz
import re
import locale # Cần import locale nếu bạn dùng %A
from datetime import datetime, timedelta
import traceback

# Giả sử TARGET_TIMEZONE đã được định nghĩa ở đâu đó
# TARGET_TIMEZONE = 'Asia/Ho_Chi_Minh' # Ví dụ

def parse_time(time_text: str, now: datetime, time_limit: datetime) -> datetime | None:
    """
    Phân tích các định dạng thời gian khác nhau, trả về datetime có múi giờ
    hoặc None nếu không hợp lệ/quá cũ. Đã tái cấu trúc xử lý múi giờ.
    """
    if not time_text: return None
    time_text = time_text.strip().replace('\xa0', ' ') # Clean input text

    try:
        # Đảm bảo TARGET_TIMEZONE có sẵn, fallback về UTC
        target_tz_name = TARGET_TIMEZONE if 'TARGET_TIMEZONE' in globals() else 'Asia/Ho_Chi_Minh'
        try:
            tz_vn = pytz.timezone(target_tz_name)
        except pytz.exceptions.UnknownTimeZoneError:
            print(f"⚠️ Không rõ múi giờ '{target_tz_name}', dùng UTC.")
            tz_vn = pytz.utc
    except Exception as e_tz:
        print(f"⚠️ Lỗi khi cài đặt múi giờ, dùng UTC: {e_tz}")
        tz_vn = pytz.utc

    # --- Đảm bảo 'now' và 'time_limit' là timezone-aware ---
    # Rất quan trọng: đảm bảo các tham số đầu vào đã có múi giờ
    if now.tzinfo is None or now.tzinfo.utcoffset(now) is None:
        print(f"⚠️ Tham số 'now' ({now}) không có múi giờ. Gán múi giờ {tz_vn.zone}.")
        now = tz_vn.localize(now)
    else:
        now = now.astimezone(tz_vn) # Chuyển về múi giờ target nếu khác

    if time_limit.tzinfo is None or time_limit.tzinfo.utcoffset(time_limit) is None:
        print(f"⚠️ Tham số 'time_limit' ({time_limit}) không có múi giờ. Gán múi giờ {tz_vn.zone}.")
        time_limit = tz_vn.localize(time_limit)
    else:
        time_limit = time_limit.astimezone(tz_vn) # Chuyển về múi giờ target nếu khác
    # --- Xong phần kiểm tra tham số đầu vào ---

    parsed_dt = None          # Kết quả cuối cùng sau khi chuẩn hóa múi giờ
    dt_intermediate = None    # Kết quả tạm thời từ các hàm parse
    parsed_success = False    # Cờ đánh dấu parse thành công

    try:
        time_text_lower = time_text.lower() # Để khớp không phân biệt hoa thường

        # 1. Xử lý Thời gian Tương đối (dùng regex)
        is_relative = False
        if "vài giây trước" in time_text_lower or "vừa xong" in time_text_lower:
            dt_intermediate = now; is_relative = True; parsed_success = True
        elif (match := re.search(r'(\d+)\s+ngày\s+trước', time_text_lower)):
            dt_intermediate = now - timedelta(days=int(match.group(1))); is_relative = True; parsed_success = True
        elif (match := re.search(r'(\d+)\s+giờ\s+trước', time_text_lower)):
            dt_intermediate = now - timedelta(hours=int(match.group(1))); is_relative = True; parsed_success = True
        elif (match := re.search(r'(\d+)\s+phút\s+trước', time_text_lower)):
            dt_intermediate = now - timedelta(minutes=int(match.group(1))); is_relative = True; parsed_success = True
        elif "hôm nay" in time_text_lower:
            if (time_match := re.search(r'(\d{1,2})[h:](\d{1,2})', time_text_lower)):
                hour, minute = map(int, time_match.groups())
                if 0 <= hour < 24 and 0 <= minute < 60:
                    # Tạo datetime aware ngay từ đầu
                    dt_intermediate = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
                    is_relative = True
                    parsed_success = True
            # Nếu không parse được giờ phút từ "hôm nay", sẽ thử định dạng tuyệt đối sau

        # 2. Xử lý Thời gian Tuyệt đối (nếu chưa phải tương đối)
        if not is_relative:
            # Làm sạch và chuẩn hóa chuỗi
            time_text_proc = re.sub(r'\s+', ' ', time_text.replace(",", "").replace(" SA", " AM").replace(" CH", " PM").strip())
            formats = [
                "%d/%m/%Y - %H:%M", "%d-%m-%Y - %H:%M",
                "%d/%m/%Y %H:%M", "%d-%m-%Y %H:%M",
                "%H:%M %d/%m/%Y",
                "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S.%f%z", # ISO có timezone
                "%Y-%m-%dT%H:%M:%SZ",                        # ISO UTC Zulu
                "%Y-%m-%d %H:%M:%S",                        # Format phổ biến
                "%Y-%m-%dT%H:%M:%S",                        # ISO không có timezone (sẽ được xử lý sau)
                "%d/%m/%Y", "%d-%m-%Y",                      # Chỉ có ngày
                "%d/%m/%Y %I:%M %p", "%d-%m-%Y %I:%M %p",    # AM/PM
                "%I:%M %p %d/%m/%Y",
            ]

            # Thử các định dạng strptime
            for fmt in formats:
                try:
                    # Xử lý các định dạng có timezone trước
                    if "%z" in fmt or fmt.endswith("Z"):
                        time_str_tz = time_text_proc.replace("Z", "+0000")
                        time_str_tz = re.sub(r'([+-])(\d{2}):(\d{2})(?:\.\d+)?$', r'\1\2\3', time_str_tz)
                        fmt_to_use = fmt.replace('.%f', '') if '.' not in time_str_tz and '.%f' in fmt else fmt
                        # Kết quả từ strptime với %z đã là aware
                        dt_intermediate = datetime.strptime(time_str_tz, fmt_to_use.replace("Z", "%z"))
                    # Xử lý các định dạng naive (bao gồm cả AM/PM vì strptime xử lý nó thành naive)
                    else:
                        dt_naive = datetime.strptime(time_text_proc, fmt)
                        if "%H" not in fmt and "%M" not in fmt and "%S" not in fmt and "%I" not in fmt:
                            dt_naive = dt_naive.replace(hour=0, minute=0, second=0)
                        # Kết quả vẫn là naive ở đây
                        dt_intermediate = dt_naive

                    # Đánh dấu thành công và thoát vòng lặp
                    parsed_success = True
                    # print(f"   [Time] Parsed '{time_text}' with format '{fmt}' -> {dt_intermediate} (Intermediate)") # Debug
                    break
                except (ValueError, TypeError):
                    continue # Thử format tiếp theo

            # Nếu các định dạng strptime không thành công, thử fromisoformat
            if not parsed_success:
                try:
                    # Thử parse trực tiếp (có thể trả về naive hoặc aware tùy input)
                    dt_intermediate = datetime.fromisoformat(time_text_proc.replace("Z", "+00:00"))
                    parsed_success = True
                    # print(f"   [Time] Parsed '{time_text}' using direct ISO -> {dt_intermediate} (Intermediate)") # Debug
                except ValueError:
                    # Thử thay ' ' bằng 'T'
                    try:
                        dt_intermediate = datetime.fromisoformat(time_text_proc.replace(" ", "T").replace("Z", "+00:00"))
                        parsed_success = True
                        # print(f"   [Time] Parsed '{time_text}' using ISO (space->T) -> {dt_intermediate} (Intermediate)") # Debug
                    except ValueError:
                        pass # Thất bại hoàn toàn

        # 3. === Chuẩn hóa Múi giờ TẬP TRUNG ===
        if parsed_success and dt_intermediate is not None:
            try:
                if dt_intermediate.tzinfo is None or dt_intermediate.tzinfo.utcoffset(dt_intermediate) is None:
                    # Nếu kết quả là naive, gán múi giờ target
                    parsed_dt = tz_vn.localize(dt_intermediate, is_dst=None)
                    # print(f"   [Time] Localized naive result: {parsed_dt}") # Debug
                else:
                    # Nếu kết quả đã aware, chuyển đổi về múi giờ target
                    parsed_dt = dt_intermediate.astimezone(tz_vn)
                    # print(f"   [Time] Converted aware result: {parsed_dt}") # Debug
            except (pytz.exceptions.NonExistentTimeError, pytz.exceptions.AmbiguousTimeError) as e_tz_final:
                 print(f"   [Time] Warning: Lỗi khi chuẩn hóa múi giờ cuối cùng cho '{time_text}': {e_tz_final}. Bỏ qua.")
                 parsed_dt = None # Không thể chuẩn hóa múi giờ
                 parsed_success = False # Đánh dấu lại là thất bại
            except Exception as e_conv: # Bắt lỗi không ngờ khác khi chuẩn hóa
                 print(f"   [Time] Warning: Lỗi không mong muốn khi chuẩn hóa múi giờ cuối cùng cho '{time_text}': {e_conv}. Bỏ qua.")
                 parsed_dt = None
                 parsed_success = False

        # 4. Xác thực cuối cùng: Đã parse thành công và trong giới hạn thời gian?
        if parsed_success and parsed_dt:
            # So sánh với time_limit (cả hai đều đã aware và cùng múi giờ tz_vn)
            if parsed_dt >= time_limit:
                return parsed_dt # Trả về kết quả hợp lệ
            else:
                # print(f"   [Time] Parsed time {parsed_dt} is older than limit {time_limit}.") # Debug
                return None # Quá cũ
        else:
            # print(f"   [Time] Failed to parse or normalize time: '{time_text}'") # Debug
            return None # Không parse/chuẩn hóa được

    except ValueError as ve:
        # Xử lý lỗi ValueError chung (ví dụ từ int() trong phần relative)
        if "invalid literal for int()" in str(ve):
             print(f"⚠️ Lỗi chuyển đổi số trong thời gian tương đối cho '{time_text}': {ve}")
        else:
             print(f"⚠️ Lỗi ValueError trong quá trình parse_time cho '{time_text}': {ve}")
        return None
    except Exception as e: # Bắt các lỗi không mong muốn khác
        print(f"⚠️ Lỗi KHÔNG MONG MUỐN trong parse_time cho '{time_text}': {e}")
        traceback.print_exc(limit=1)
        return None
# --- End parse_time ---

# Cell 5.6
def contains_keyword(text, keywords_list):
    """Checks if any keyword from the list exists in the text (case-insensitive, whole word)."""
    if not text or not keywords_list: return False
    text_lower = text.lower() # Convert text to lower once
    for keyword in keywords_list:
        if not keyword: continue # Skip empty keywords
        # Use word boundaries (\b) to match whole words only
        # Escape special regex characters in the keyword itself
        pattern = r'\b' + re.escape(keyword.lower()) + r'\b'
        if re.search(pattern, text_lower):
            return True # Found a match
    return False # No match found in the entire list
# --- End contains_keyword ---


def get_detailed_article_time(driver):
    """Gets publication time from detail page, prioritizing structured data."""
    # Selectors ordered by reliability (structured data first, then common patterns)
    selectors = [
        "time[datetime]",                       # Standard HTML5 time element
        "meta[itemprop='datePublished'][content]", # Schema.org metadata
        "meta[property='article:published_time'][content]", # Open Graph metadata
        "span[pubdate='pubdate']",              # Older pubdate markers
        "abbr.timeago[title]",                  # Relative time with full date in title
        "span.time[title]",                     # Spans with class 'time' and title attribute
        "span.date[title]",                     # Spans with class 'date' and title attribute
        # Common class names from news sites / CMS
        ".article-publish-date time", ".article-timestamp", ".entry-date",
        ".published-date", ".posted-on time", ".timestamp", ".byline__datetime",
        # CafeF specific / Vietnamese common
        "span.detail-time", "span.pdate", ".pdate", "span.date", ".date",
        # Generic text content search (less reliable, last resort)
        # Look for spans/divs containing typical date/time characters or relative terms
        "//span[contains(text(),'/') or contains(text(),'-') or contains(text(),':') or contains(text(),'ngày') or contains(text(),'giờ') or contains(text(),'phút')][string-length(normalize-space(text())) > 5 and string-length(normalize-space(text())) < 50]",
        "//div[contains(@class,'time') or contains(@class,'date') or contains(@class,'meta')][normalize-space() and string-length(normalize-space(text())) < 50]",
    ]
    # print("   [Time Detail] Searching for time element...")
    for i, selector in enumerate(selectors):
         try:
            # Use a short wait for each selector attempt
            element = WebDriverWait(driver, 0.15).until(
                EC.presence_of_element_located((By.XPATH, selector) if selector.startswith("//") else (By.CSS_SELECTOR, selector))
            )
            # Prioritize extracting structured data from attributes
            datetime_attr = element.get_attribute("datetime")
            if datetime_attr: return datetime_attr # ISO format preferred
            content_attr = element.get_attribute("content") # For meta tags
            if content_attr: return content_attr
            title_attr = element.get_attribute("title")
            if title_attr and len(title_attr) > 5: return title_attr # title often has full datetime

            # Fallback to the element's visible text content
            text = element.text.strip()
            if text: return text
            # If element found but no useful data, continue to next selector
            # print(f"   [Time Detail] Found element with selector {i+1}, but no usable attribute/text.")

         except (TimeoutException, NoSuchElementException, StaleElementReferenceException):
             continue # Element not found or became stale, try next selector
         except Exception as e:
             # Log minor errors if needed for debugging selectors
             # print(f"   [Time Detail] Minor error on selector {i+1} ('{selector}'): {e}")
             continue # Ignore minor errors and proceed

    # print("   [Time Detail] ❌ Could not find time element using any selector.")
    return None # Return None if no time information was found
# --- End get_detailed_article_time ---

# Cell 5.7
def get_article_content(driver):
    """Extracts the main article content, cleaning unwanted elements."""
    content = ""
    # Selectors for main content containers, ordered by likely relevance
    content_selectors = [
        "article[itemprop='articleBody']", "div[itemprop='articleBody']", # Schema.org standard
        "div#abody.content-detail", "div.content-detail.text-content", # CafeF likely
        "div.article__content", "div.article-content", # Common article classes
        "div.entry-content", "div.post-content", "div.post-body", # Common CMS/Blog classes
        "div.main-content", "div.story-content", "div.article-body", "div.news-body",
        "div.noidung", # Vietnamese specific
        "div.contentdetail", "div.content_detail",
        "td.detailsView", # Less common table-based layout
        "div[class*='content']", # Generic wildcard (lower priority)
    ]
    main_content_element = None
    # print("   [Content Detail] Searching for main content container...")
    # 1. Find the best matching main content container
    for selector in content_selectors:
        try:
            # Use a short wait, assuming the element is already mostly loaded
            main_content_element = WebDriverWait(driver, 0.2).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            # print(f"   [Content Detail] Found main container with selector: {selector}")
            break # Use the first container found
        except (TimeoutException, NoSuchElementException):
            continue # Try the next selector

    # 2. Extract and clean text if a container was found
    if main_content_element:
        paragraphs_text = []
        try:
            # --- Remove unwanted elements using JavaScript ---
            selectors_to_remove = [
                'script', 'style', 'iframe', 'figure > figcaption', 'noscript', 'aside', 'nav', 'form', 'button', 'input',
                '.ad', '.ads', '.advert', '.advertisement', '.banner', '.quangcao', '.qcunbuyable', '.sponsor', # Ads & Sponsors
                '.related', '.related-posts', '.yarpp-related', '.tags', '.categories', # Related, Tags, Cats
                '.social', '.share', '.sharing', '.socail-wrapper-content', # Social sharing
                '.author', '.byline', '.meta', '.credit', '.timestamp', '.source', # Author/Meta/Source info
                '.comment', '.feedback', '#comments', # Comments sections
                '.video', '.player', '.viewbox-video', 'video', 'audio', # Media players
                '.boxcheckstock', '.VCSortableInPreviewMode', # Specific site elements
                '.print-button', '.email-button', '.subscribe-box', # Action buttons/boxes
                'footer', '.footer', '#footer' # Footer elements within the main content (less likely but possible)
            ]
            try:
                 js_remove_script = f"""
                     var container = arguments[0];
                     var selectors = {json.dumps(selectors_to_remove)};
                     selectors.forEach(function(sel) {{
                         try {{ container.querySelectorAll(sel).forEach(el => el.remove()); }}
                         catch (e) {{ console.warn('Minor error removing selector:', sel, e); }}
                     }});
                 """
                 driver.execute_script(js_remove_script, main_content_element)
                 # print("   [Content Detail] Attempted JS removal of unwanted elements.")
            except Exception as e_remove:
                 # Don't stop if removal fails, just log a warning
                 print(f"   [Content Detail] Warning: JavaScript removal script failed: {e_remove}")

            # --- Extract text primarily from <p> tags ---
            paragraphs = main_content_element.find_elements(By.TAG_NAME, "p")
            if paragraphs:
                # print(f"   [Content Detail] Found {len(paragraphs)} <p> elements after cleaning.")
                seen_texts = set()
                for p in paragraphs:
                    try:
                        p_text = p.text
                        if p_text:
                            p_text_stripped = p_text.strip()
                            # Filter out very short paragraphs and common noisy lines
                            p_lower = p_text_stripped.lower()
                            min_para_length = 40 # Minimum characters for a meaningful paragraph
                            is_meaningful = (
                                len(p_text_stripped) >= min_para_length and
                                p_text_stripped not in seen_texts and # Avoid duplicates
                                not p_lower.startswith(">>") and
                                not p_lower.startswith("ảnh:") and
                                not p_lower.startswith("nguồn:") and
                                not p_lower.startswith("bài viết liên quan") and
                                "tag :" not in p_lower and
                                "copyright" not in p_lower and
                                "liên hệ quảng cáo" not in p_lower and
                                ("theo " not in p_lower or len(p_lower) > 60) # Allow "theo xyz" if longer sentence
                            )
                            if is_meaningful:
                                paragraphs_text.append(p_text_stripped)
                                seen_texts.add(p_text_stripped)
                    except StaleElementReferenceException: continue # Skip if element goes stale

                # Join the extracted paragraphs
                content = "\n".join(paragraphs_text)
            else:
                # Fallback: If no <p> tags, get the full inner text of the container
                print("   [Content Detail] No <p> tags found inside container, getting container's full text (fallback).")
                try: content = main_content_element.text.strip()
                except StaleElementReferenceException: content = "Lỗi Stale Element (Container Fallback)"

        except Exception as e_process:
            print(f"   [Content Detail] Error processing container: {e_process}. Trying final text fallback.")
            try: content = main_content_element.text.strip() # Final fallback
            except Exception: content = "Lỗi lấy text container (Final Fallback)"
    else:
        # 3. Last Resort: No specific container found, get text from <body>
        print("   [Content Detail] ❌ No specific content container found. Getting text from <body>.")
        try:
            body_element = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            # Ideally, remove header/footer from body text too, but it's complex. Get raw text.
            content = body_element.text.strip()
        except Exception as e_body:
            print(f"   [Content Detail] Error getting body text: {e_body}")
            content = "Không lấy được nội dung chi tiết (Body Error)"

    # --- Final Content Check ---
    min_len_threshold = 100 # Minimum acceptable content length
    if content and len(content) >= min_len_threshold:
        # print(f"   [Content Detail] ✅ Extracted content length: {len(content)}")
        # Limit overall length just in case (e.g., for database storage)
        max_overall_len = 25000
        return content[:max_overall_len] if len(content) > max_overall_len else content
    elif content: # Extracted but too short
         print(f"   [Content Detail] ⚠️ Extracted content is too short ({len(content)} chars).")
         return content # Return it anyway, might still be useful
    else: # Nothing extracted
         print(f"   [Content Detail] ❌ Failed to extract sufficient content.")
         return "Không lấy được nội dung chi tiết"
# --- End get_article_content ---

## Cell 5.8
def open_spreadsheet(gc, url):
    """Opens Google Sheet using the authenticated gspread client."""
    if not gc:
        print("❌ Cannot open sheet: Google client (gc) not authenticated.")
        return None
    if not url or not url.startswith("https://docs.google.com/spreadsheets/d/"):
        print(f"❌ Invalid Google Sheet URL provided: {url}")
        return None
    try:
        # Extract Sheet ID for logging, making it slightly less revealing
        sheet_id = url.split('/d/')[1].split('/')[0]
        print(f"📊 Opening Google Sheet (ID: ...{sheet_id[-6:]})...")
        spreadsheet = gc.open_by_url(url)
        print(f"✅ Spreadsheet '{spreadsheet.title}' opened successfully.")
        return spreadsheet
    except gspread.exceptions.APIError as api_error:
         error_details = str(api_error)
         # Check for common, informative errors
         if 'PERMISSION_DENIED' in error_details or '403' in error_details:
             # Try to get the service account email for a more helpful message
             sa_email = "Unknown"
             if hasattr(gc, 'auth') and hasattr(gc.auth, 'service_account_email'):
                 sa_email = gc.auth.service_account_email
             print(f"❌ PERMISSION DENIED: Cannot open Google Sheet. Ensure Service Account ({sa_email}) has 'Editor' access to this specific Sheet.")
         elif 'NOT_FOUND' in error_details or '404' in error_details:
              print(f"❌ NOT FOUND: Google Sheet URL seems invalid or the Sheet was deleted.")
         else: # General API error
             print(f"❌ Google Sheets API Error opening sheet: {api_error}")
         return None
    except Exception as e:
        # Catch any other unexpected exceptions during the open process
        print(f"❌ UNEXPECTED Error opening sheet: {e}")
        traceback.print_exc(limit=1) # Print traceback for unexpected errors
        return None
# --- End open_spreadsheet ---

# Cell 5.9
def read_competitor_programs(gc, sheet_url, competitor_sheets_map):
    """Reads program names and details from specified tabs in a Google Sheet."""
    # Use global constants for column names if available, otherwise use defaults
    # Ensure these match the actual headers in your _Chương trình sheets
    name_col_header = "tên"
    details_col_header = "đặc điểm chính"
    name_col_fallback_idx = 1 # Column B (0-based index)
    details_col_fallback_idx = 6 # Column G (0-based index)

    print("\n--- Reading Competitor Program Data from Tabs ---")
    programs_data = {} # { "CompetitorKey": [{"name": "ProgA", "details": "..."}, ...], ... }

    # Input validation
    if not gc: print("   ⚠️ Warning: Google Sheets client unavailable. Cannot read programs."); return programs_data
    if not sheet_url: print("   ⚠️ Warning: Google Sheet URL missing. Cannot read programs."); return programs_data
    if not competitor_sheets_map: print("   ℹ️ Info: No competitor sheet map provided. Skipping program read."); return programs_data

    spreadsheet = None
    try: # Open the spreadsheet once
        spreadsheet = gc.open_by_url(sheet_url)
        print(f"   Accessing Spreadsheet: '{spreadsheet.title}' for reading programs.")
    except Exception as e:
        print(f"   ❌ ERROR: Cannot access Google Sheet at {sheet_url}: {e}. Aborting program read."); return programs_data

    # Iterate through the map to read each specified tab
    for competitor_key, sheet_name in competitor_sheets_map.items():
        programs_data[competitor_key] = [] # Initialize list for this competitor
        try:
            print(f"   Reading tab for '{competitor_key}': '{sheet_name}'...")
            worksheet = spreadsheet.worksheet(sheet_name)
            # Optimize: Get only necessary columns if possible? No, get_all_values is often simpler.
            all_values = worksheet.get_all_values() # Get all data at once

            if len(all_values) < 2: # Need header + at least one data row
                print(f"      Tab '{sheet_name}' is empty or has only headers. Skipping."); continue

            # Find column indices (case-insensitive)
            header = [h.strip().lower() for h in all_values[0]]
            data_rows = all_values[1:]
            try: name_col_idx = header.index(name_col_header)
            except ValueError: name_col_idx = name_col_fallback_idx; print(f"      ⚠️ Warning: '{name_col_header}' header not found in '{sheet_name}', using fallback Col index {name_col_idx+1}.")
            try: details_col_idx = header.index(details_col_header)
            except ValueError: details_col_idx = details_col_fallback_idx; print(f"      ⚠️ Warning: '{details_col_header}' header not found in '{sheet_name}', using fallback Col index {details_col_idx+1}.")

            # Validate indices
            max_cols_header = len(header)
            if not (0 <= name_col_idx < max_cols_header and 0 <= details_col_idx < max_cols_header):
                 print(f"      ❌ Error: Invalid column indices ({name_col_idx}, {details_col_idx}) for '{sheet_name}'. Skipping tab."); continue

            count = 0
            for r_idx, row in enumerate(data_rows):
                 row_len = len(row)
                 # Safely get name and details
                 name = str(row[name_col_idx]).strip() if row_len > name_col_idx and row[name_col_idx] else ""
                 details = str(row[details_col_idx]).strip() if row_len > details_col_idx and row[details_col_idx] else ""

                 if name: # Only add if program name exists
                     programs_data[competitor_key].append({"name": name, "details": details})
                     count += 1

            print(f"      Read {count} programs for '{competitor_key}'.")

        except gspread.exceptions.WorksheetNotFound:
            print(f"   ⚠️ Warning: Worksheet '{sheet_name}' for '{competitor_key}' not found.")
        except gspread.exceptions.APIError as api_error:
             print(f"   ❌ API Error reading sheet '{sheet_name}': {api_error}")
        except Exception as e:
            print(f"   ❌ Unexpected Error reading sheet '{sheet_name}': {e}")

    print(f"--- Finished reading program data. ---")
    return programs_data
# --- End read_competitor_programs ---


def add_or_update_program_sheet(gc, spreadsheet, competitor_key, program_name, program_details_dict,
                                article_url, crawl_time_str, model, mbs_programs_list):
    """Appends new program info to the competitor's program sheet, including LLM comparison results."""
    # Use global constants for sheet map and header
    # Add safety checks in case globals weren't defined/loaded properly
    competitor_map = COMPETITOR_SHEET_MAP if 'COMPETITOR_SHEET_MAP' in globals() else {}
    output_header = PROGRAM_SHEET_HEADER if 'PROGRAM_SHEET_HEADER' in globals() else []
    source_name_global = SOURCE_NAME if 'SOURCE_NAME' in globals() else 'UnknownSource'

    # --- Input Validation ---
    if not gc: print("❌ Cannot add program: Google client unavailable."); return
    if not spreadsheet: print("❌ Cannot add program: Spreadsheet object invalid."); return
    if not competitor_key or not program_name: print("❌ Cannot add program: Missing Competitor Key or Program Name."); return
    if not competitor_map: print("❌ Cannot add program: COMPETITOR_SHEET_MAP not defined."); return
    if not output_header: print("❌ Cannot add program: PROGRAM_SHEET_HEADER not defined."); return
    if competitor_key not in competitor_map: print(f"❌ Cannot add program: Key '{competitor_key}' not in COMPETITOR_SHEET_MAP."); return

    target_sheet_name = competitor_map[competitor_key]
    worksheet = None

    try:
        print(f"   [Program Sheet] Processing '{program_name}' for tab '{target_sheet_name}'...")
        # 1. Get or Create Worksheet & Verify Header
        try:
            worksheet = spreadsheet.worksheet(target_sheet_name)
            print(f"      Found program tab '{target_sheet_name}'.")
            # Check/Update Header (similar to write_to_google_sheet_v3)
            try:
                current_header = worksheet.row_values(1) if worksheet.row_count >= 1 else []
                # Simple check: ensure first column matches and length is sufficient
                if not current_header or len(current_header) < len(output_header) or current_header[0] != output_header[0]:
                    print(f"      Warning: Header mismatch/incomplete in '{target_sheet_name}'. Updating...")
                    worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
                    worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(output_header))}', {'textFormat': {'bold': True}})
            except Exception as e_hdr: print(f"      Warning: Could not check/update header: {e_hdr}")
        except gspread.exceptions.WorksheetNotFound:
            print(f"      Tab not found, creating '{target_sheet_name}'...")
            worksheet = spreadsheet.add_worksheet(title=target_sheet_name, rows=100, cols=len(output_header) + 2)
            worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
            worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(output_header))}', {'textFormat': {'bold': True}})
            print(f"      Created tab and wrote headers.")

        if not worksheet: print(f"   ❌ Failed to access/create worksheet '{target_sheet_name}'."); return

        # 2. Compare with MBS Programs using LLM
        comparison_result = {'best_mbs_match_name': "N/A (Không SS)", 'similarity_high': False, 'similarities': "N/A", 'differences': "N/A"}
        if model and mbs_programs_list:
             comparison_result = compare_with_mbs_programs_llm(model, program_name, program_details_dict, mbs_programs_list)
             # Optional short delay after comparison API call can be added here if needed
             # time.sleep(1.0)
        else: print("      Skipping MBS comparison (AI model or MBS list unavailable).")

        # 3. Prepare Data Row
        new_row_dict = {
            'Thời gian cập nhật': crawl_time_str, 'Tên': program_name, 'Kênh tiếp nhận': source_name_global,
            'Thời gian triển khai': program_details_dict.get('ThoiGianTrienKhai', "Không rõ"),
            'Loại hình sản phẩm': program_details_dict.get('LoaiHinhSanPham', "Không rõ"),
            'Điều kiện tham gia': program_details_dict.get('DieuKienThamGia', "Không rõ"),
            'Đặc điểm chính': program_details_dict.get('DacDiemChinh', "Không rõ"),
            'Ưu đãi nếu có': program_details_dict.get('UuDaiNeuCo', "Không rõ"),
            'Đối tượng khách hàng mục tiêu': program_details_dict.get('DoiTuongKhachHang', "Không rõ"),
            'URL Bài viết nguồn': article_url,
            'Tên chương trình tương đồng ở MBS': comparison_result.get('best_mbs_match_name', "Lỗi/Không SS"),
            'Điểm giống sơ bộ': comparison_result.get('similarities', "N/A"),
            'Điểm khác sơ bộ': comparison_result.get('differences', "N/A") }
        row_to_append = [str(new_row_dict.get(col, "")).strip() for col in output_header]

        # 4. Append Row
        print(f"      Appending program data to '{target_sheet_name}'...")
        # Use insert_rows to avoid potential overwrites if sheet grows unexpectedly
        worksheet.append_row(row_to_append, value_input_option='USER_ENTERED', insert_data_option='INSERT_ROWS', table_range='A1')
        print(f"      ✅ Appended '{program_name}' successfully.")

    except gspread.exceptions.APIError as api_error:
         # Handle specific API errors gracefully
         err_str = str(api_error).lower()
         if 'permission denied' in err_str or '403' in err_str: print(f"   ❌ PERMISSION DENIED writing to '{target_sheet_name}'.")
         elif 'rate limit' in err_str or '429' in err_str: print(f"   ❌ RATE LIMIT Hit writing to '{target_sheet_name}'.")
         else: print(f"   ❌ Google Sheets API Error writing to '{target_sheet_name}': {api_error}")
    except Exception as e:
        print(f"   ❌ UNEXPECTED Error adding program to sheet '{target_sheet_name}': {e}")
# --- End add_or_update_program_sheet ---


def write_to_google_sheet_v3(gc, spreadsheet, worksheet_name, header, data_to_write):
    """Writes data to a specified worksheet using V3 header structure, with batching and retries."""
    # --- Input Validation ---
    if not gc: print("❌ Cannot write V3: Google client unavailable."); return False
    if not spreadsheet: print("❌ Cannot write V3: Spreadsheet object invalid."); return False
    if not worksheet_name: print("❌ Cannot write V3: Target worksheet name empty."); return False
    if not header: print("❌ Cannot write V3: Header list empty."); return False
    if not data_to_write: print("   [Sheet Write V3] 🤷 No new data to write."); return True

    # --- Setup ---
    worksheet = None
    output_header = header
    num_cols = len(output_header)
    # Get batch size/delay from globals or use defaults
    # Ensure globals are accessed safely with defaults
    batch_size = SHEET_WRITE_BATCH_SIZE if 'SHEET_WRITE_BATCH_SIZE' in globals() and isinstance(SHEET_WRITE_BATCH_SIZE, int) and SHEET_WRITE_BATCH_SIZE > 0 else 50
    write_delay = SHEET_WRITE_DELAY if 'SHEET_WRITE_DELAY' in globals() and isinstance(SHEET_WRITE_DELAY, (int, float)) and SHEET_WRITE_DELAY >= 0 else 1.0

    try:
        print(f"   [Sheet Write V3] Processing write to tab: '{worksheet_name}'...")
        # 1. Get or Create Worksheet & Verify Header
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
            print(f"      Found target tab '{worksheet_name}'.")
            # Check/Update Header (similar logic as before)
            try:
                current_header = worksheet.row_values(1) if worksheet.row_count >= 1 else []
                if not current_header or len(current_header) < num_cols or current_header[:num_cols] != output_header:
                    print(f"      Warning: Header mismatch/incomplete. Updating...")
                    worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
                    worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, num_cols)}', {'textFormat': {'bold': True}})
            except Exception as e_hdr: print(f"      ⚠️ Warning: Error checking/updating header: {e_hdr}")
        except gspread.exceptions.WorksheetNotFound:
            print(f"      Tab not found, creating '{worksheet_name}'...")
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=len(data_to_write) + 100, cols=num_cols + 5)
            worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
            worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, num_cols)}', {'textFormat': {'bold': True}})
            print(f"      Created tab and wrote headers.")

        if not worksheet: print(f"   ❌ Failed to access/create target worksheet '{worksheet_name}'."); return False

        # 2. Prepare Data Rows
        print(f"      Preparing {len(data_to_write)} rows for V3 append...")
        rows_to_append = [[str(item_dict.get(col, "")).strip() for col in output_header] for item_dict in data_to_write]

        # 3. Append Data in Batches
        if rows_to_append:
             num_rows = len(rows_to_append)
             total_batches = (num_rows + batch_size - 1) // batch_size
             print(f"      Appending {num_rows} rows in {total_batches} batches (size: {batch_size})...")
             for i in range(0, num_rows, batch_size):
                 batch = rows_to_append[i:min(i + batch_size, num_rows)]
                 batch_num = (i // batch_size) + 1
                 print(f"         Writing batch {batch_num}/{total_batches} ({len(batch)} rows)...")
                 write_attempt = 0
                 max_write_retries = 2 # Try original + 2 retries on rate limit
                 while write_attempt <= max_write_retries:
                     try:
                          worksheet.append_rows(batch, value_input_option='USER_ENTERED', insert_data_option='INSERT_ROWS', table_range='A1')
                          print(f"         ✅ Batch {batch_num} written.")
                          # Add delay between successful batches if needed
                          if write_delay > 0 and i + batch_size < num_rows: time.sleep(write_delay)
                          break # Success, exit retry loop for this batch
                     except gspread.exceptions.APIError as write_err:
                          err_str = str(write_err).lower()
                          is_rate_limit = ('rate limit' in err_str or '429' in err_str or 'quota' in err_str)
                          if is_rate_limit and write_attempt < max_write_retries:
                               wait_time = (write_delay + 1) * (2 ** write_attempt) # Exponential backoff
                               print(f"         ⚠️ Rate limit/Quota hit (Attempt {write_attempt+1}). Waiting {wait_time:.1f}s and retrying...")
                               time.sleep(wait_time)
                               write_attempt += 1
                          else: # Fatal API error or max retries reached
                               print(f"         ❌ API Error on batch {batch_num} (Attempt {write_attempt+1}): {write_err}")
                               print(f"            Skipping remaining batches.")
                               return False # Abort
                     except Exception as general_write_err: # Catch other unexpected errors
                          print(f"         ❌ UNEXPECTED Error writing batch {batch_num} (Attempt {write_attempt+1}): {general_write_err}")
                          print(f"            Skipping remaining batches.")
                          return False # Abort

             print(f"   ✅ Appended {num_rows} rows successfully to '{worksheet_name}'.")
             return True
        else:
             print("      No valid rows prepared.")
             return True

    except gspread.exceptions.APIError as api_error:
         # Handle errors during worksheet access/creation
         err_details = str(api_error).lower()
         if 'permission denied' in err_details or '403' in err_details: print(f"   ❌ PERMISSION DENIED preparing V3 write to '{worksheet_name}'.")
         else: print(f"   ❌ Google Sheets API Error preparing V3 write: {api_error}")
         return False
    except Exception as e:
        print(f"   ❌ UNEXPECTED Error during V3 Sheet Write process: {e}")
        return False
# --- End write_to_google_sheet_v3 ---

## Cell 5.10
# === Data Saving & Export Functions   ===
# ========================================

def save_data_local_v3(non_keyword_data, analyzed_data_v3, sheets_success_flag):
    """Combines non-keyword and analyzed keyword data into a single V3-formatted Excel file."""
    print("\n--- Saving All Articles Locally (V3 Format Excel) ---")

    # Prepare timestamp and target timezone safely
    try:
        # Access global TARGET_TIMEZONE if defined, else default
        target_tz_name = TARGET_TIMEZONE if 'TARGET_TIMEZONE' in globals() else 'Asia/Ho_Chi_Minh'
        tz = pytz.timezone(target_tz_name)
    except Exception: tz = pytz.utc
    timestamp = datetime.now(tz).strftime("%Y%m%d_%H%M%S")

    # Define Excel header safely
    base_header_fallback = [ # Fallback header
        'Thời gian cập nhật', 'Tiêu đề bài viết', 'Công ty được phân tích', 'Là công ty đối thủ?', 'Là quảng cáo?',
        'Tên chương trình/sản phẩm được nhắc đến', 'Thông điệp chính (AI)', 'Tóm tắt phân tích LLM (AI)', 'URL', 'Trạng thái xử lý',
        'Trọng tâm bài viết (AI)', 'Phân loại chi tiết (AI/DB)', 'Giọng điệu (AI)', 'Nội dung gốc' ]
    base_header = SHEET_HEADER_V3 if 'SHEET_HEADER_V3' in globals() else base_header_fallback
    if not base_header: print("   ❌ ERROR: Could not determine V3 header. Cannot create Excel."); return
    excel_header = ['STT'] + base_header

    all_data_for_excel = []
    stt_counter = 1

    # 1. Process Non-Keyword Articles
    print(f"   Processing {len(non_keyword_data)} non-keyword articles...")
    for item in non_keyword_data:
        row_dict = {col: '' for col in excel_header} # Initialize row
        row_dict['STT'] = stt_counter
        # Map basic info
        row_dict['Thời gian cập nhật'] = item.get('Thời gian', '')
        row_dict['Tiêu đề bài viết'] = item.get('Tiêu đề', 'N/A')
        row_dict['URL'] = item.get('URL', 'N/A')
        # Set defaults for non-keyword
        row_dict['Công ty được phân tích'] = 'N/A (Non-KW)'
        row_dict['Là công ty đối thủ?'] = 'Không'
        row_dict['Là quảng cáo?'] = 'Không'
        row_dict['Phân loại chi tiết (AI/DB)'] = 'Non-KW'
        content_or_error = item.get('Nội dung', '')
        if isinstance(content_or_error, str) and content_or_error.startswith("LỖI"):
            row_dict['Trạng thái xử lý'] = content_or_error # Put error in status
            row_dict['Nội dung gốc'] = '' # Clear content
        else:
            row_dict['Trạng thái xử lý'] = 'Bỏ qua (Non-KW)'
            # Limit content length for Excel
            row_dict['Nội dung gốc'] = str(content_or_error)[:32764] + '...' if len(str(content_or_error)) > 32767 else str(content_or_error)
        # Set remaining AI/Analysis columns to N/A
        for col in ['Tên chương trình/sản phẩm được nhắc đến', 'Thông điệp chính (AI)',
                    'Tóm tắt phân tích LLM (AI)', 'Trọng tâm bài viết (AI)', 'Giọng điệu (AI)']:
            if col in row_dict: row_dict[col] = 'N/A' # Check if col exists in header
        all_data_for_excel.append(row_dict)
        stt_counter += 1

    # 2. Process Analyzed Keyword Articles
    print(f"   Processing {len(analyzed_data_v3)} analyzed keyword articles...")
    for item_v3 in analyzed_data_v3:
        row_dict = {col: item_v3.get(col, '') for col in base_header} # Get V3 data
        row_dict['STT'] = stt_counter # Add STT
        # Limit content length again just in case
        content_kw = str(row_dict.get('Nội dung gốc', ''))
        if len(content_kw) >= 32767: row_dict['Nội dung gốc'] = content_kw[:32764] + '...'
        all_data_for_excel.append(row_dict)
        stt_counter += 1

    # 3. Create DataFrame and Save to Excel
    if all_data_for_excel:
        print(f"   Creating DataFrame with {len(all_data_for_excel)} total rows...")
        try:
            df_all = pd.DataFrame(all_data_for_excel)
            # Ensure columns are in the correct order specified by excel_header
            # Filter excel_header to only include columns present in the DataFrame to avoid KeyError
            valid_excel_header = [col for col in excel_header if col in df_all.columns]
            df_all = df_all[valid_excel_header]
        except Exception as e_df:
            print(f"   ❌ ERROR creating Pandas DataFrame: {e_df}. Cannot save Excel."); return

        # Define filename and path
        sheet_status = "SheetOK" if sheets_success_flag else "SheetFailOrSkipped"
        # Safely access global SOURCE_NAME
        source_prefix = SOURCE_NAME if 'SOURCE_NAME' in globals() else "Crawl"
        filename = f"{source_prefix}_all_articles_v3_{sheet_status}_{timestamp}.xlsx"
        # Save to /content/ in Colab, current dir otherwise (check IS_COLAB flag)
        output_path = os.path.join('/content/', filename) if IS_COLAB else filename

        try:
            print(f"   Saving combined data to Excel file: {output_path} ...")
            df_all.to_excel(output_path, index=False, engine="openpyxl")
            print(f"✅ Successfully saved {len(all_data_for_excel)} rows to: {output_path}")

            # 4. Trigger Colab download if applicable
            # Access IS_COLAB_FILES and files globals safely
            if 'IS_COLAB_FILES' in globals() and IS_COLAB_FILES and 'files' in globals() and files:
                print(f"\n   📥 Triggering file download for '{filename}' (Colab)...")
                try: files.download(output_path); print(f"      Download initiated.")
                except Exception as e_dl: print(f"      ❌ Error triggering download: {e_dl}")
            elif 'IS_COLAB' in globals() and not IS_COLAB:
                 print(f"\n   ℹ️ File saved locally: {os.path.abspath(output_path)}")

        except Exception as e_save: print(f"   ❌ ERROR saving data to Excel: {e_save}")
    else: print("   ℹ️ No articles processed. Excel file not created.")
# --- End save_data_local_v3 ---

# Cell 5.10
# === Main Crawler Function            ===
# ========================================

def crawl_cafef(hours_to_check, existing_urls_from_sheet):
    """
    Crawls CafeF news, extracts articles, parses details, identifies keywords.
    Returns lists of keyword/non-keyword articles and the set of all processed URLs.
    """
    # --- Access Global Config Safely ---
    # Use globals() to check existence and provide defaults if missing
    source_name = globals().get('SOURCE_NAME', 'CafeF')
    target_url = globals().get('TARGET_URL')
    base_url = globals().get('BASE_URL')
    max_scrolls_limit = globals().get('MAX_SCROLLS', 5)
    target_tz_name = globals().get('TARGET_TIMEZONE', 'Asia/Ho_Chi_Minh')
    keywords_global = globals().get('KEYWORDS', {})

    print(f"\n--- Starting {source_name} Crawl ---")
    if not target_url: print("❌ ERROR: TARGET_URL not defined. Cannot start crawl."); return [], [], set()
    if not base_url: base_url = target_url # Fallback base URL

    # --- Setup Timezone and Limit ---
    try: tz_vn = pytz.timezone(target_tz_name)
    except Exception: tz_vn = pytz.utc
    now = datetime.now(tz=tz_vn)
    time_limit = now - timedelta(hours=hours_to_check)
    print(f"   Crawling articles published after: {time_limit.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    print(f"   Target URL: {target_url}")
    print(f"   Max Scrolls: {max_scrolls_limit}")

    # --- Initialize Results ---
    keyword_articles = []
    non_keyword_articles = []
    processed_urls_this_run = set(existing_urls_from_sheet) # Start with known URLs
    print(f"   Ignoring {len(processed_urls_this_run)} URLs from sheet initially.")

    # --- WebDriver Setup ---
    if not SELENIUM_AVAILABLE or not webdriver: # Check if Selenium is ready
        print("❌ ERROR: Selenium WebDriver is not available. Crawl aborted.")
        return [], [], processed_urls_this_run

    chrome_options = Options()
    # Apply standard options
    arguments = ["--headless=new", "--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu",
                 "--window-size=1920,1080", "--disable-blink-features=AutomationControlled",
                 "--blink-settings=imagesEnabled=false", "--log-level=3",
                 "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
                ]
    for arg in arguments: chrome_options.add_argument(arg)
    chrome_options.page_load_strategy = 'eager' # Faster load
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

    driver = None
    try:
        print(f"   Initializing WebDriver...")
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(90) # Increased timeout
        driver.implicitly_wait(5)
        print("   WebDriver initialized.")
    except Exception as e_init:
        print(f"❌ CRITICAL Error initializing WebDriver: {e_init}. Crawl aborted.")
        traceback.print_exc(limit=1)
        return [], [], processed_urls_this_run

    # --- Main Crawl Logic ---
    articles_processed_count = 0
    try:
        print(f"   Opening main page: {target_url}")
        driver.get(target_url)

        # Wait for initial content load
        article_link_selector = ".tlitem.box-category-item h3 > a" # Adjust if needed
        print(f"   Waiting for initial articles ('{article_link_selector}', 60s)...")
        WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, article_link_selector)))
        print("   Initial articles loaded.")

        # --- Scroll Loop ---
        print("   Starting scroll loop...")
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        consecutive_no_change = 0
        stop_scrolling_flag = False # Stop if old article found

        while scroll_count < max_scrolls_limit and not stop_scrolling_flag:
            scroll_count += 1
            print(f"\n   --- Scroll attempt {scroll_count}/{max_scrolls_limit} ---")
            # Scroll down and wait for JS loading
            try:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                print(f"      Scrolled down. Waiting...")
                time.sleep(random.uniform(3.5, 5.5)) # Wait longer
            except WebDriverException as scroll_err: print(f"      Scroll error: {scroll_err}. Stopping."); break

            # Check height change
            try: current_height = driver.execute_script("return document.body.scrollHeight")
            except WebDriverException as height_err: print(f"      Height check error: {height_err}. Stopping."); break
            if current_height == last_height:
                consecutive_no_change += 1
                print(f"      Scroll height unchanged ({consecutive_no_change}/3).")
                if consecutive_no_change >= 3: print("      -> End of page/no new content. Stopping."); break
            else: print(f"      Page height: {last_height} -> {current_height}"); consecutive_no_change = 0
            last_height = current_height

            # --- Process Articles Visible ---
            article_item_selector = ".tlitem.box-category-item" # Adjust if needed
            articles_on_page = []
            try: articles_on_page = driver.find_elements(By.CSS_SELECTOR, article_item_selector)
            except Exception as e_find: print(f"      Error finding items: {e_find}"); continue
            print(f"      Found {len(articles_on_page)} article items in view.")
            if not articles_on_page and scroll_count == 1: print("      Warning: No articles on first load?"); break

            newly_processed_on_scroll = 0
            for index, article_element in enumerate(articles_on_page):
                article_data = {"Nguồn": source_name, "URL": "", "Thời gian": "N/A", "Tiêu đề": "N/A", "Nội dung": "N/A", "Keywords": ""}
                is_new_url_in_run = False

                try:
                    # 1. Extract Link & Title
                    link_elem = article_element.find_element(By.CSS_SELECTOR, article_link_selector)
                    raw_url = link_elem.get_attribute("href")
                    if raw_url and not raw_url.startswith('http'): article_data["URL"] = requests.compat.urljoin(base_url, raw_url).strip()
                    elif raw_url: article_data["URL"] = raw_url.strip()
                    else: continue
                    article_data["Tiêu đề"] = (link_elem.get_attribute("title") or link_elem.text or "").strip()
                    if not article_data["Tiêu đề"]: continue

                    # 2. Check if URL Processed
                    article_url = article_data["URL"]
                    if not article_url or article_url in processed_urls_this_run: continue

                    # 3. Process New URL
                    is_new_url_in_run = True; newly_processed_on_scroll += 1; articles_processed_count += 1
                    processed_urls_this_run.add(article_url)
                    log_prefix = f"      [Article #{articles_processed_count} Sc{scroll_count}-Idx{index}]"
                    print(f"\n{log_prefix} Processing NEW: {article_data['Tiêu đề'][:65]}...")
                    print(f"{log_prefix} URL: {article_url}")

                    # 4. Open Detail Tab
                    content = "Lỗi mở tab chi tiết"
                    time_obj = None; detail_tab_opened = False
                    original_window = driver.current_window_handle; new_window = None
                    try:
                        driver.execute_script("window.open(arguments[0], '_blank');", article_url)
                        detail_tab_opened = True
                        WebDriverWait(driver, 15).until(EC.number_of_windows_to_be(len(driver.window_handles)))
                        new_window = [w for w in driver.window_handles if w != original_window][-1]
                        driver.switch_to.window(new_window)

                        detail_page_wait_selector = "body h1, body article, body div.content-detail, div#mainContent"
                        WebDriverWait(driver, 75).until(EC.presence_of_element_located((By.CSS_SELECTOR, detail_page_wait_selector)))

                        # 5. Get Time
                        time_text = get_detailed_article_time(driver)
                        if time_text:
                            time_obj = parse_time(time_text, now, time_limit)
                            if time_obj:
                                article_data["Thời gian"] = time_obj.strftime("%d/%m/%Y %H:%M")
                                print(f"{log_prefix} ✅ Time OK: {article_data['Thời gian']}")
                            else:
                                time_check_obj = parse_time(time_text, now, now - timedelta(days=730))
                                if time_check_obj:
                                    print(f"{log_prefix} 🛑 STOP: Article too old (Parsed: {time_check_obj.strftime('%d/%m/%Y')}).")
                                    stop_scrolling_flag = True; raise StopIteration
                                else: raise ValueError(f"Time parsing failed for '{time_text}'")
                        else: raise ValueError("Time not found on detail page")

                        # 6. Get Content
                        content = get_article_content(driver)
                        article_data["Nội dung"] = content
                        print(f"{log_prefix} Content Len: {len(content)}. Checking Keywords...")
                        if content == "Không lấy được nội dung chi tiết" or len(content) < 100:
                            print(f"{log_prefix} ⚠️ Warning: Insufficient content.")

                        # 7. Check Keywords
                        keywords_found = set(); has_kw = False
                        title_low = article_data["Tiêu đề"].lower()
                        content_low = article_data["Nội dung"].lower() if article_data["Nội dung"] != "Không lấy được nội dung chi tiết" else ""
                        for comp_key, kw_list in keywords_global.items():
                            if contains_keyword(title_low, kw_list):
                                has_kw = True; keywords_found.update(kw for kw in kw_list if re.search(r'\b'+re.escape(kw.lower())+r'\b', title_low))
                            if content_low and contains_keyword(content_low, kw_list):
                                has_kw = True; keywords_found.update(kw for kw in kw_list if re.search(r'\b'+re.escape(kw.lower())+r'\b', content_low))

                        # 8. Classify and Store
                        if has_kw:
                            article_data["Keywords"] = ", ".join(sorted(list(keywords_found)))
                            keyword_articles.append(article_data)
                            print(f"{log_prefix} 🔥 KEYWORD Article Added (KW: {article_data['Keywords']})")
                        else:
                            if not str(article_data["Nội dung"]).startswith("LỖI"):
                                non_keyword_articles.append(article_data)

                    # Detail Tab Error Handling
                    except StopIteration: print(f"{log_prefix} StopIteration: Halting page."); break
                    except ValueError as e_skip:
                         print(f"{log_prefix} ⏭️ Skipping article: {e_skip}")
                         article_data["Nội dung"] = f"LỖI: {e_skip}"
                         non_keyword_articles.append(article_data)
                    except (TimeoutException, WebDriverException, NoSuchWindowException) as e_wd_detail:
                         print(f"{log_prefix} ❌ WebDriver Error (Detail): {e_wd_detail}")
                         article_data["Nội dung"] = f"LỖI WebDriver Detail: {e_wd_detail}"
                         non_keyword_articles.append(article_data)
                    except Exception as e_detail_other:
                         print(f"{log_prefix} ❌ Unexpected Error (Detail): {e_detail_other}")
                         article_data["Nội dung"] = f"LỖI KHÁC Detail: {e_detail_other}"
                         non_keyword_articles.append(article_data)
                    finally: # Close Detail Tab
                        if detail_tab_opened:
                            try:
                                if new_window in driver.window_handles and driver.current_window_handle == new_window: driver.close()
                                if original_window in driver.window_handles: driver.switch_to.window(original_window)
                                elif driver.window_handles: driver.switch_to.window(driver.window_handles[0])
                            except Exception: pass
                        time.sleep(0.1)

                # List Item Error Handling
                except Exception as e_item_level:
                    print(f"      [Article Sc{scroll_count}-Idx{index}] ❌ Error processing item: {e_item_level}")
                    if is_new_url_in_run:
                         article_data["Nội dung"] = f"LỖI Item Level: {e_item_level}"
                         non_keyword_articles.append(article_data)

            # End article loop
            print(f"   --- Processed {newly_processed_on_scroll} new articles on scroll {scroll_count} ---")
            if stop_scrolling_flag: break # Exit scroll loop

        # End scroll loop
        if not stop_scrolling_flag and scroll_count >= max_scrolls_limit:
            print(f"   Reached max scrolls ({max_scrolls_limit}).")

    # Main Crawl Error Handling
    except TimeoutException as e_timeout_main: print(f"❌ FATAL TIMEOUT waiting for initial page load. Crawl aborted.")
    except WebDriverException as e_main_wd: print(f"❌ FATAL WebDriver Error during crawl: {e_main_wd}")
    except Exception as e_main_other: print(f"❌ FATAL UNEXPECTED Error during crawl: {e_main_other}"); traceback.print_exc()
    finally: # Ensure WebDriver Quits
        if driver:
            print("\n   Closing WebDriver session...")
            try: driver.quit()
            except Exception as e_quit: print(f"      Error quitting WebDriver: {e_quit}")
            print("   WebDriver closed.")

    # Final Summary
    print(f"\n✅ Crawl Finished for {source_name}.")
    print(f"   - Keyword Articles Found: {len(keyword_articles)}")
    print(f"   - Non-Keyword/Error Articles Logged: {len(non_keyword_articles)}")
    # Calculate new URLs processed correctly
    new_urls_count = len(processed_urls_this_run) - len(existing_urls_from_sheet)
    print(f"   - New URLs Processed in this run: {new_urls_count}")
    return keyword_articles, non_keyword_articles, processed_urls_this_run
# --- End crawl_cafef ---

## Cell 5.11:
print("\n--- All Helper and Crawler Functions Defined in Cell 5 ---")

# Cell 6: Chạy code

# --- Imports cần thiết cho cell này (nếu chưa import ở Cell 2) ---
import time
from datetime import datetime
import pytz
import os
import sys
import traceback # Thêm import traceback để log lỗi chi tiết hơn nếu cần

# --- Hàm thực thi chính ---
def main():
    # Sử dụng các biến global đã được khai báo/cấu hình ở Cell 3 và các client/model ở Cell 5
    global gc, model, mbs_program_data_for_comparison, config_valid
    global SERVICE_ACCOUNT_JSON_PATH, SPREADSHEET_URL, TARGET_WORKSHEET_NAME, COMPETITOR_SHEET_MAP
    global HOURS_TO_CHECK, MAX_SCROLLS, SLEEP_BETWEEN_AI_CALLS, SHEET_HEADER_V3, TARGET_TIMEZONE, SOURCE_NAME
    global KEYWORDS, IS_COLAB, files # Thêm IS_COLAB, files nếu dùng trong save_data_local_v3

    start_time_total = time.time()
    try: target_tz = pytz.timezone(TARGET_TIMEZONE)
    except Exception: target_tz = pytz.utc
    start_dt = datetime.now(target_tz)
    print(f"--- Starting Main Process at {start_dt.strftime('%Y-%m-%d %H:%M:%S %Z')} ---")

    if not config_valid:
        print("❌ Aborting execution due to configuration errors found in Cell 3.")
        return
    print("✅ Configuration appears valid. Proceeding...")

    # Các biến để lưu thời gian từng bước
    install_duration, conn_duration, sheet_open_duration, read_mbs_duration, read_urls_duration = 0,0,0,0,0
    crawl_duration, analysis_duration, sheet_write_duration, save_backup_duration = 0,0,0,0


    try:
        # --- 1. Cài đặt Chrome & Driver ---
        print("\n--- Step 1: Installing Chrome & ChromeDriver ---")
        install_start_step = time.time()
        install_success = install_chrome_and_driver()
        install_duration = time.time() - install_start_step
        print(f"--- Step 1 Finished (Duration: {install_duration:.2f}s) ---")
        if not install_success:
            print("❌ Aborting: Failed to install Chrome/ChromeDriver. Cannot continue.")
            return

        # --- 2. Khởi tạo các kết nối ---
        print("\n--- Step 2: Initializing Connections (Google Sheets, Gemini AI) ---")
        conn_start_step = time.time()
        if not gc:
            print("   Authenticating Google Sheets...")
            gc = authenticate_google_sheets(SERVICE_ACCOUNT_JSON_PATH)
        if not gc:
            print("❌ Aborting: Google Sheet client initialization failed.")
            return
        if not model:
             print("   Setting up Gemini AI Model...")
             setup_gemini()
        if not model:
            print("⚠️ Warning: Gemini model initialization failed. Proceeding without AI analysis features.")
        conn_duration = time.time() - conn_start_step
        print(f"--- Step 2 Finished (Duration: {conn_duration:.2f}s) ---")

        # --- 3. Mở Spreadsheet chính ---
        print("\n--- Step 3: Opening Main Google Spreadsheet ---")
        sheet_open_start_step = time.time()
        spreadsheet = open_spreadsheet(gc, SPREADSHEET_URL)
        if not spreadsheet:
            print("❌ Aborting: Failed to open the main Google Spreadsheet.")
            return
        sheet_open_duration = time.time() - sheet_open_start_step
        print(f"--- Step 3 Finished (Duration: {sheet_open_duration:.2f}s) ---")

        # --- 4. Đọc dữ liệu chương trình MBS để so sánh ---
        print("\n--- Step 4: Reading MBS Program Data for Comparison ---")
        read_mbs_start_step = time.time()
        mbs_program_data_for_comparison = []
        temp_mbs_map = {k: v for k, v in COMPETITOR_SHEET_MAP.items() if k == "MBS"}
        if temp_mbs_map:
            all_program_data = read_competitor_programs(gc, SPREADSHEET_URL, temp_mbs_map)
            mbs_program_data_for_comparison = all_program_data.get("MBS", [])
            if not mbs_program_data_for_comparison: print("   ⚠️ Warning: Could not read MBS program list.")
            else: print(f"   ✅ Read {len(mbs_program_data_for_comparison)} MBS programs.")
        else: print("   ⚠️ Warning: 'MBS' key not found in COMPETITOR_SHEET_MAP.")
        read_mbs_duration = time.time() - read_mbs_start_step
        print(f"--- Step 4 Finished (Duration: {read_mbs_duration:.2f}s) ---")

        # --- 5. Đọc URL đã có trên Sheet chính ---
        print(f"\n--- Step 5: Reading Existing URLs from Main Sheet ('{TARGET_WORKSHEET_NAME}') ---")
        read_urls_start_step = time.time()
        existing_urls_on_sheet = set()
        try:
            worksheet_read = spreadsheet.worksheet(TARGET_WORKSHEET_NAME)
            headers_list = worksheet_read.row_values(1)
            url_col_name_lower = 'url'
            url_col_index = -1
            try: url_col_index = [h.strip().lower() for h in headers_list].index(url_col_name_lower) + 1
            except ValueError: url_col_index = 9; print(f"   ⚠️ Warning: Header '{url_col_name_lower}' not found. Assuming URL in Col I.")
            if url_col_index > 0:
                print(f"   Reading URLs from column {url_col_index}...")
                urls_raw = worksheet_read.col_values(url_col_index)[1:]
                existing_urls_on_sheet = set(str(url).strip() for url in urls_raw if isinstance(url, str) and url.strip().startswith('http'))
                print(f"   Found {len(existing_urls_on_sheet)} unique existing URLs.")
            else: print(f"   Could not determine URL column. Crawling all.")
        except gspread.exceptions.WorksheetNotFound: print(f"   Target tab '{TARGET_WORKSHEET_NAME}' not found.")
        except Exception as e_read: print(f"   ⚠️ Unexpected Error reading existing URLs: {e_read}")
        read_urls_duration = time.time() - read_urls_start_step
        print(f"--- Step 5 Finished (Duration: {read_urls_duration:.2f}s) ---")

        # --- 6. Thực hiện Crawl ---
        print("\n" + "="*20 + f" Step 6: Starting {SOURCE_NAME} Crawl " + "="*20)
        crawl_start_step = time.time()
        keyword_articles, non_keyword_articles, processed_urls_total = crawl_cafef(HOURS_TO_CHECK, existing_urls_on_sheet)
        crawl_duration = time.time() - crawl_start_step
        print(f"--- Step 6: Web Crawl Finished (Duration: {crawl_duration:.2f}s) ---")
        print(f"   - Found {len(keyword_articles)} new articles with keywords.")
        print(f"   - Found {len(non_keyword_articles)} new articles without keywords (or with errors).")
        print("="* (44 + len(SOURCE_NAME)) + "\n")

        # --- 7. Phân tích các bài có Keyword và chuẩn bị non-keyword ---
        print(f"--- Step 7: Processing {len(keyword_articles)} Keyword Articles & {len(non_keyword_articles)} Non-Keyword Articles ---")
        all_results_for_sheet = [] # List chứa TOÀN BỘ kết quả cho sheet
        analysis_start_step = time.time()
        current_run_update_time_str = start_dt.strftime('%Y-%m-%d %H:%M:%S')

        # --- Xử lý Keyword Articles ---
        if not keyword_articles:
            print("   No new keyword articles found to analyze.")
        else:
            for i, article in enumerate(keyword_articles):
                article_title = article.get('Tiêu đề', 'N/A')
                print(f"\n--- Processing KW Article {i+1}/{len(keyword_articles)}: {article_title[:70]}...")
                url = article.get('URL', 'N/A')
                content = article.get('Nội dung', '')
                time_str = article.get('Thời gian', current_run_update_time_str)
                keywords_found_str = article.get("Keywords", "")
                detected_competitors_in_article = sorted(list(set(
                    comp_key for comp_key, kw_list in KEYWORDS.items()
                    if any(kw.strip() in keywords_found_str for kw in kw_list)
                )))

                if not detected_competitors_in_article:
                    print(f"   ⚠️ Skipping KW article: No valid competitors identified from keywords '{keywords_found_str}'.")
                    # Chuyển bài này sang non_keyword nếu muốn
                    article['Nội dung'] = f"Lỗi KW: Không tìm thấy đối thủ từ '{keywords_found_str}'"
                    non_keyword_articles.append(article)
                    continue

                print(f"   Competitors mentioned (context for AI): {', '.join(detected_competitors_in_article)}")

                analysis_result = get_default_analysis_result("AI model disabled")
                if model:
                    print(f"   Calling AI analysis...")
                    analysis_result = analyze_with_gemini(model, article_title, content, detected_competitors_in_article)
                    if i < len(keyword_articles) - 1 and SLEEP_BETWEEN_AI_CALLS > 0:
                        print(f"   Sleeping for {SLEEP_BETWEEN_AI_CALLS:.1f}s...")
                        time.sleep(SLEEP_BETWEEN_AI_CALLS)
                else:
                    print("   Skipping AI analysis (AI model unavailable).")

                primary_focus = analysis_result.get('primary_competitor_focus', 'Lỗi AI')
                is_program_pr = analysis_result.get('is_program_pr', False)
                promoted_program = analysis_result.get('promoted_program_name', '')
                is_brand_pr_from_ai = analysis_result.get('is_brand_pr', False) # Lấy từ AI
                program_details = analysis_result.get('program_details', {})
                ai_summary = analysis_result.get('analysis_summary', 'Lỗi AI / AI bị tắt')

                is_competitor_flag = "Không"
                is_ad_final = "Không"
                processing_status_final = "Lỗi AI / AI Tắt"
                ad_classification_detail_final = "Lỗi AI / AI Tắt"
                main_message_ai_final = "N/A"

                if "Lỗi AI" not in primary_focus and "AI Tắt" not in primary_focus:
                    if primary_focus in KEYWORDS: is_competitor_flag = "Có"

                    if is_program_pr and promoted_program:
                        is_ad_final = "Có"
                        processing_status_final = f"Đã xử lý (CT: {primary_focus} - {promoted_program[:30]}...)"
                        ad_classification_detail_final = f"PR Chương trình ({primary_focus} - {promoted_program[:50]})"
                        main_message_ai_final = program_details.get('DacDiemChinh', ai_summary)
                        if model and gc and spreadsheet: # Cập nhật sheet chương trình
                             add_or_update_program_sheet(
                                gc=gc, spreadsheet=spreadsheet, competitor_key=primary_focus,
                                program_name=promoted_program, program_details_dict=program_details,
                                article_url=url, crawl_time_str=current_run_update_time_str,
                                model=model, mbs_programs_list=mbs_program_data_for_comparison
                            )
                    elif is_brand_pr_from_ai and primary_focus in KEYWORDS:
                        is_ad_final = "Có"
                        processing_status_final = f"Đã xử lý (PR TH: {primary_focus})"
                        ad_classification_detail_final = f"PR Thương hiệu ({primary_focus})"
                        main_message_ai_final = ai_summary
                    elif primary_focus in KEYWORDS: # Có focus đối thủ nhưng không rõ là PR CT hay TH
                        is_ad_final = "Có (Xem xét)" # Hoặc "Không" tùy theo quyết định của bạn
                        processing_status_final = f"Đã xử lý (Focus: {primary_focus}, cần xem xét PR)"
                        ad_classification_detail_final = f"Bài viết về {primary_focus} (AI)"
                        main_message_ai_final = ai_summary
                    else: # Không focus đối thủ cụ thể được xác định là KW
                        is_competitor_flag = "Không"
                        is_ad_final = "Không"
                        processing_status_final = "Đã xử lý (Không focus ĐT)"
                        ad_classification_detail_final = "Không quảng cáo (AI)"
                        main_message_ai_final = ai_summary

                # Xử lý trường hợp primary_focus không nằm trong KEYWORDS nhưng AI vẫn có thể tìm thấy
                # Ví dụ: AI trả về "Công ty ABC" nhưng "Công ty ABC" không có trong dict KEYWORDS
                if primary_focus != "N/A" and primary_focus not in KEYWORDS:
                    is_competitor_flag = "Không (Ngoài DS)" # Hoặc một nhãn khác
                    # Giữ nguyên is_ad_final, ad_classification_detail_final dựa trên is_program_pr, is_brand_pr
                    # Nếu bạn muốn bài này không được tính là PR của đối thủ (vì không trong list), thì set is_ad_final = "Không"
                    if not (is_program_pr or is_brand_pr_from_ai): # Nếu AI cũng ko nói là PR
                        is_ad_final = "Không"
                        ad_classification_detail_final = f"Bài viết về {primary_focus} (Ngoài DS)"


                sheet_v3_row = {
                    'Thời gian cập nhật': current_run_update_time_str,
                    'Tiêu đề bài viết': article_title,
                    'Công ty được phân tích': primary_focus if primary_focus != "Lỗi AI" else "N/A",
                    'Là công ty đối thủ?': is_competitor_flag,
                    'Là quảng cáo?': is_ad_final,
                    'Tên chương trình/sản phẩm được nhắc đến': promoted_program if is_program_pr else "",
                    'Thông điệp chính (AI)': main_message_ai_final,
                    'Tóm tắt phân tích LLM (AI)': ai_summary,
                    'URL': url,
                    'Trạng thái xử lý': processing_status_final,
                    'Trọng tâm bài viết (AI)': primary_focus if primary_focus != "Lỗi AI" else "N/A",
                    'Phân loại chi tiết (AI/DB)': ad_classification_detail_final,
                    'Giọng điệu (AI)': 'N/A', # Bạn có thể yêu cầu AI phân tích thêm nếu muốn
                    'Nội dung gốc': content if len(content) < 32767 else content[:32764]+'...',
                }
                all_results_for_sheet.append(sheet_v3_row)

        # --- Chuẩn bị Non-Keyword Articles ---
        if non_keyword_articles:
            print(f"\n--- Preparing {len(non_keyword_articles)} Non-Keyword articles for sheet ---")
        for item_non_kw in non_keyword_articles:
            row_dict_non_kw = {col: '' for col in SHEET_HEADER_V3}
            row_dict_non_kw['Thời gian cập nhật'] = item_non_kw.get('Thời gian', current_run_update_time_str)
            row_dict_non_kw['Tiêu đề bài viết'] = item_non_kw.get('Tiêu đề', 'N/A')
            row_dict_non_kw['URL'] = item_non_kw.get('URL', 'N/A')
            original_content = item_non_kw.get('Nội dung', 'N/A')
            row_dict_non_kw['Nội dung gốc'] = original_content if len(original_content) < 32767 else original_content[:32764]+'...'

            row_dict_non_kw['Công ty được phân tích'] = 'N/A'
            row_dict_non_kw['Là công ty đối thủ?'] = 'Không'
            row_dict_non_kw['Là quảng cáo?'] = 'Không'
            row_dict_non_kw['Tên chương trình/sản phẩm được nhắc đến'] = ''
            row_dict_non_kw['Thông điệp chính (AI)'] = 'N/A'
            row_dict_non_kw['Tóm tắt phân tích LLM (AI)'] = 'N/A (Non-KW)'
            row_dict_non_kw['Trạng thái xử lý'] = 'Non-KW (Không có từ khóa đối thủ)' if not str(original_content).startswith("LỖI") else f"Lỗi Crawl: {original_content[:100]}"
            row_dict_non_kw['Trọng tâm bài viết (AI)'] = 'N/A'
            row_dict_non_kw['Phân loại chi tiết (AI/DB)'] = 'Non-KW'
            row_dict_non_kw['Giọng điệu (AI)'] = 'N/A'
            all_results_for_sheet.append(row_dict_non_kw)

        analysis_duration = time.time() - analysis_start_step
        print(f"--- Step 7: Article Processing Finished (Duration: {analysis_duration:.2f}s) ---")

        # --- 8. Ghi TẤT CẢ kết quả vào Google Sheet (V3 Header) ---
        print("\n" + "="*20 + " Step 8: Writing ALL Results to Google Sheet " + "="*20)
        sheet_write_start_step = time.time()
        sheets_write_successful = False
        if gc and spreadsheet:
            if all_results_for_sheet: # Ghi nếu có dữ liệu
                sheets_write_successful = write_to_google_sheet_v3(
                    gc=gc, spreadsheet=spreadsheet, worksheet_name=TARGET_WORKSHEET_NAME,
                    header=SHEET_HEADER_V3, data_to_write=all_results_for_sheet
                )
                if sheets_write_successful:
                    print(f"   ✅ Successfully wrote {len(all_results_for_sheet)} rows to '{TARGET_WORKSHEET_NAME}'.")
                else:
                    print(f"   ❌ Failed to write all_results_for_sheet to '{TARGET_WORKSHEET_NAME}'.")

            else:
                print("   No new data (keyword or non-keyword) to write to the sheet.")
                sheets_write_successful = True # Coi như thành công vì không có gì để ghi
        else:
            print("   Skipping Google Sheet write (Google Client or Spreadsheet object not available).")
        sheet_write_duration = time.time() - sheet_write_start_step
        print(f"--- Step 8: Sheet Writing Attempt {'Completed' if sheets_write_successful else 'Failed/Skipped'} (Duration: {sheet_write_duration:.2f}s) ---")
        print("="*60 + "\n")

        # --- 9. Lưu file Excel Backup (TÙY CHỌN) ---
        # Bạn có thể giữ lại bước này để backup, hoặc bỏ qua nếu không cần
        print("="*20 + " Step 9: Saving Local Backup File (Optional) " + "="*20)
        save_start_step = time.time()
        # Hàm save_data_local_v3 giờ sẽ nhận toàn bộ all_results_for_sheet
        # để tránh trùng lặp. Bạn cần điều chỉnh save_data_local_v3 một chút
        # hoặc tạo một hàm save mới đơn giản hơn chỉ để dump `all_results_for_sheet` ra Excel.
        # Giả sử save_data_local_v3 được điều chỉnh để chỉ nhận 1 list data:
        # save_data_all_to_excel(all_results_for_sheet, sheets_write_successful)
        # Hiện tại, để đơn giản, ta vẫn có thể gọi save_data_local_v3 như cũ,
        # nhưng nó sẽ chỉ lưu các bài non_keyword chưa được gộp vào all_results_for_sheet
        # nếu bạn không gộp `non_keyword_articles` vào `all_results_for_sheet` ở trên
        # TỐT NHẤT LÀ: Sửa save_data_local_v3 để nó chỉ nhận 1 list là all_results_for_sheet
        # HOẶC, nếu bạn muốn giữ logic cũ của save_data_local_v3:
        # Tách final_keyword_results từ all_results_for_sheet (nếu đã gộp)
        # final_keyword_results_for_excel = [row for row in all_results_for_sheet if row.get('Trạng thái xử lý','').startswith('Đã xử lý') or row.get('Phân loại chi tiết (AI/DB)','').startswith('PR')]
        # non_keyword_for_excel = [row for row in all_results_for_sheet if 'Non-KW' in row.get('Phân loại chi tiết (AI/DB)','')]
        # save_data_local_v3(non_keyword_for_excel, final_keyword_results_for_excel, sheets_write_successful)
        print("   (Logic for saving to Excel needs review based on new flow - for now, this step might be redundant if Sheet is primary)")
        # Ví dụ đơn giản là không làm gì ở đây nếu không muốn Excel nữa:
        if IS_COLAB and files: # Vẫn có thể muốn download file từ Colab nếu có
            # Tìm file Excel đã tạo (nếu có từ lần chạy trước hoặc logic cũ) để download ví dụ
            # Hoặc bạn tạo một file excel mới từ `all_results_for_sheet` ở đây
            pass

        save_backup_duration = time.time() - save_start_step
        print(f"--- Step 9: Backup Saving Finished (Duration: {save_backup_duration:.2f}s) ---")
        print("="*60 + "\n")

        # --- Tổng kết ---
        end_time_total = time.time()
        end_dt = datetime.now(target_tz)
        total_duration_script = end_time_total - start_time_total
        print(f"--- Main Process Finished at {end_dt.strftime('%Y-%m-%d %H:%M:%S %Z')} ---")
        print(f"Total Execution Duration: {total_duration_script:.2f} seconds ({total_duration_script/60:.2f} minutes)")
        print(f"   - Step 1 (Install Env): {install_duration:.2f}s")
        print(f"   - Step 2 (Connections): {conn_duration:.2f}s")
        print(f"   - Step 3 (Open Sheet):  {sheet_open_duration:.2f}s")
        print(f"   - Step 4 (Read MBS):    {read_mbs_duration:.2f}s")
        print(f"   - Step 5 (Read URLs):   {read_urls_duration:.2f}s")
        print(f"   - Step 6 (Crawl):       {crawl_duration:.2f}s")
        print(f"   - Step 7 (Processing):  {analysis_duration:.2f}s")
        print(f"   - Step 8 (Sheet Write): {sheet_write_duration:.2f}s")
        print(f"   - Step 9 (Save Backup): {save_backup_duration:.2f}s")
        print("="*70)

    except Exception as e_main:
        print(f"\n❌❌❌ CRITICAL ERROR in Main Execution ❌❌❌")
        print(f"Error Type: {type(e_main).__name__}")
        print(f"Error Message: {e_main}")
        print("--- Traceback ---")
        traceback.print_exc()
        print("--- End Traceback ---")
    finally:
        print("\nScript execution finished.")

# --- Chạy chương trình ---
if __name__ == "__main__" or 'google.colab' in sys.modules:
    if 'config_valid' in globals() and config_valid:
        main()
    elif 'config_valid' not in globals():
         print("\n❌ Configuration validation status unknown. Please run Cell 3 first.")
    else:
        print("\n❌ Execution aborted due to configuration errors detected before running main(). Check Cell 3.")