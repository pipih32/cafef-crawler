# -*- coding: utf-8 -*-
"""cafef _ crawl + x√°c ƒë·ªãnh qu·∫£ng c√°o (demo)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i2hlMCWwQ9YTtT3k6zXkYyPu55agI0O0
"""

# ‚ö†Ô∏è This is a redacted demo version for portfolio purposes.
# ‚ùå All API keys and sensitive credentials have been removed.
# ‚úÖ You can run this notebook by inserting your own Gemini API key and Google Sheet config.

# Cell 1: C√†i ƒë·∫∑t Th∆∞ vi·ªán v√† M√¥i tr∆∞·ªùng

# --- 0. Mount Google Drive ---
try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive mounted successfully.")
except ImportError:
    print("Not running in Colab, skipping Drive mount.")
except Exception as e:
    print(f"Error mounting Google Drive: {e}")

# --- 1. C√†i ƒë·∫∑t Th∆∞ vi·ªán Python ---
print("\n--- Installing Python Libraries ---")
!pip install selenium pandas openpyxl pytz requests google-generativeai gspread google-auth google-auth-oauthlib google-auth-httplib2 --quiet
print("--- Library Installation Complete ---")

# --- 2. C√†i ƒë·∫∑t Chrome & ChromeDriver ---
print("\n--- Chrome & ChromeDriver Installation Logic Defined in Cell 5 ---")
print("--- It will be executed during the main process ---")

print("\n--- Environment Setup Cell Complete ---")

# Cell 2: Imports

print("\n--- Importing Python Libraries ---")
import sys
import json
import pandas as pd
import pytz
import os
import time
import traceback
import gspread
import requests
import random
import re
from datetime import datetime, timedelta

# --- Colab Specific ---
try:
    from google.colab import auth, files
    IS_COLAB = True
    print("   Running in Colab environment.")
except ImportError:
    IS_COLAB = False
    print("   Not running in Colab environment.")
    auth = None # Define auth as None if not in Colab
    files = None # Define files as None

# --- Google Auth ---
# Import default here, might raise error outside Colab if gcloud SDK not configured
try:
    from google.auth import default
    from google.auth import exceptions as google_auth_exceptions
except ImportError:
    print("   Warning: google.auth library not found. Google Authentication might fail.")
    default = None
    google_auth_exceptions = None


# --- AI ---
try:
    import google.generativeai as genai
except ImportError:
    print("   Warning: google-generativeai not installed. AI features disabled.")
    genai = None

# --- Selenium ---
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import (
        TimeoutException, WebDriverException, NoSuchElementException,
        StaleElementReferenceException, NoSuchWindowException
    )
except ImportError:
    print("   Error: selenium library not installed. Web crawling disabled.")
    # Define dummy classes/exceptions if needed to prevent NameErrors later, or handle missing selenium
    webdriver = None # Example

print("--- Library Imports Complete ---")

# Cell 3: C·∫•u h√¨nh & Bi·∫øn To√†n c·ª•c

print("\n--- Loading Configuration & Globals ---")

# --- General Configuration ---
SOURCE_NAME = "CafeF"
TARGET_URL = "https://cafef.vn/thi-truong-chung-khoan.chn"
BASE_URL = "https://cafef.vn"

# --- Colab Form Parameters (with Fallbacks) ---
config_source = "Defaults" # Default source
# Initialize variables with non-sensitive defaults FIRST
SERVICE_ACCOUNT_JSON_PATH = "" # S·∫Ω ƒë∆∞·ª£c l·∫•y t·ª´ form
GEMINI_API_KEY = "" # S·∫Ω ƒë∆∞·ª£c l·∫•y t·ª´ form
SPREADSHEET_URL = "" # S·∫Ω ƒë∆∞·ª£c l·∫•y t·ª´ form
TARGET_WORKSHEET_NAME = "your_default_sheet" # Default sheet name
HOURS_TO_CHECK = 24 # Gi·ªù qu√©t m·∫∑c ƒë·ªãnh
MAX_SCROLLS = 5 # Scroll m·∫∑c ƒë·ªãnh
SLEEP_BETWEEN_AI_CALLS = 10.0 # Tr·ªÖ AI m·∫∑c ƒë·ªãnh
SHEET_WRITE_DELAY = 1.0 # Tr·ªÖ ghi sheet batch m·∫∑c ƒë·ªãnh
SHEET_WRITE_BATCH_SIZE = 20 # Batch size m·∫∑c ƒë·ªãnh

# Check if running in Colab to load form parameters
IS_COLAB = 'google.colab' in sys.modules

if IS_COLAB:
    print("   Running in Colab environment. Loading Form parameters...")
    try:
        #@markdown #### **C·∫•u h√¨nh B·∫Øt bu·ªôc**
        #@markdown ƒê∆∞·ªùng d·∫´n t·ªõi file JSON Service Account (trong Google Drive):
        _SERVICE_ACCOUNT_JSON_PATH_FORM = "your_choice" #@param {type:"string"}
        #@markdown API Key Gemini (Google AI Studio):
        _GEMINI_API_KEY_FORM = "your_API_key" #@param {type:"string"}
        #@markdown URL Google Sheet (c√≥ quy·ªÅn s·ª≠a):
        _SPREADSHEET_URL_FORM = "your_sheet" #@param {type:"string"}

        #@markdown #### **C·∫•u h√¨nh T√πy ch·ªçn**
        #@markdown T√™n Tab ƒë√≠ch trong Sheet:
        _TARGET_WORKSHEET_NAME_FORM = "your_sheet" #@param {type:"string"}
        #@markdown S·ªë gi·ªù qu√©t ng∆∞·ª£c v·ªÅ qu√° kh·ª©:
        _HOURS_TO_CHECK_FORM = 10 #@param {type:"integer"}
        #@markdown S·ªë l·∫ßn scroll t·ªëi ƒëa:
        _MAX_SCROLLS_FORM = 3 #@param {type:"integer"}
        #@markdown ƒê·ªô tr·ªÖ gi·ªØa c√°c l·∫ßn g·ªçi API Gemini (gi√¢y):
        _SLEEP_BETWEEN_AI_CALLS_FORM = 10.0 #@param {type:"number"}
        #@markdown ƒê·ªô tr·ªÖ gi·ªØa c√°c l·∫ßn ghi batch v√†o Sheet (gi√¢y):
        _SHEET_WRITE_DELAY_FORM = 1.0 #@param {type:"number"}
        #@markdown S·ªë d√≤ng ghi v√†o Sheet m·ªói l·∫ßn:
        _SHEET_WRITE_BATCH_SIZE_FORM = 20 #@param {type:"integer"}

        # Assign to main variables ONLY if user provided input in the form
        # ∆Øu ti√™n gi√° tr·ªã t·ª´ Form n·∫øu ng∆∞·ªùi d√πng nh·∫≠p
        if _SERVICE_ACCOUNT_JSON_PATH_FORM: SERVICE_ACCOUNT_JSON_PATH = _SERVICE_ACCOUNT_JSON_PATH_FORM
        if _GEMINI_API_KEY_FORM: GEMINI_API_KEY = _GEMINI_API_KEY_FORM
        if _SPREADSHEET_URL_FORM: SPREADSHEET_URL = _SPREADSHEET_URL_FORM
        if _TARGET_WORKSHEET_NAME_FORM: TARGET_WORKSHEET_NAME = _TARGET_WORKSHEET_NAME_FORM # G√°n lu√¥n n·∫øu c√≥ gi√° tr·ªã
        HOURS_TO_CHECK = _HOURS_TO_CHECK_FORM # Lu√¥n l·∫•y gi√° tr·ªã t·ª´ form (v√¨ c√≥ default)
        MAX_SCROLLS = _MAX_SCROLLS_FORM
        SLEEP_BETWEEN_AI_CALLS = _SLEEP_BETWEEN_AI_CALLS_FORM
        SHEET_WRITE_DELAY = _SHEET_WRITE_DELAY_FORM
        SHEET_WRITE_BATCH_SIZE = _SHEET_WRITE_BATCH_SIZE_FORM

        config_source = "Colab Form"
        print("   Configuration successfully loaded/updated from Colab Form.")

    except NameError:
        print("   Colab Form parameters not defined (maybe form cell not run?). Attempting Environment Variables/Defaults.")
        # Fallback to Environment Variables if form fails or not in Colab
        SERVICE_ACCOUNT_JSON_PATH = os.environ.get("SERVICE_ACCOUNT_JSON_PATH", SERVICE_ACCOUNT_JSON_PATH)
        GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", GEMINI_API_KEY)
        SPREADSHEET_URL = os.environ.get("SPREADSHEET_URL", SPREADSHEET_URL)
        TARGET_WORKSHEET_NAME = os.environ.get("TARGET_WORKSHEET_NAME", TARGET_WORKSHEET_NAME)
        HOURS_TO_CHECK = int(os.environ.get("HOURS_TO_CHECK", HOURS_TO_CHECK))
        MAX_SCROLLS = int(os.environ.get("MAX_SCROLLS", MAX_SCROLLS))
        SLEEP_BETWEEN_AI_CALLS = float(os.environ.get("SLEEP_BETWEEN_AI_CALLS", SLEEP_BETWEEN_AI_CALLS))
        SHEET_WRITE_DELAY = float(os.environ.get("SHEET_WRITE_DELAY", SHEET_WRITE_DELAY))
        SHEET_WRITE_BATCH_SIZE = int(os.environ.get("SHEET_WRITE_BATCH_SIZE", SHEET_WRITE_BATCH_SIZE))
        config_source = "Environment/Defaults" if any(os.environ.get(k) for k in ["GEMINI_API_KEY", "SPREADSHEET_URL", "SERVICE_ACCOUNT_JSON_PATH"]) else "Defaults"

elif not IS_COLAB:
     print("   Not running in Colab. Attempting Environment Variables/Defaults.")
     # Fallback logic for non-Colab environments
     SERVICE_ACCOUNT_JSON_PATH = os.environ.get("SERVICE_ACCOUNT_JSON_PATH", SERVICE_ACCOUNT_JSON_PATH)
     GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", GEMINI_API_KEY)
     SPREADSHEET_URL = os.environ.get("SPREADSHEET_URL", SPREADSHEET_URL)
     TARGET_WORKSHEET_NAME = os.environ.get("TARGET_WORKSHEET_NAME", TARGET_WORKSHEET_NAME)
     HOURS_TO_CHECK = int(os.environ.get("HOURS_TO_CHECK", HOURS_TO_CHECK))
     MAX_SCROLLS = int(os.environ.get("MAX_SCROLLS", MAX_SCROLLS))
     SLEEP_BETWEEN_AI_CALLS = float(os.environ.get("SLEEP_BETWEEN_AI_CALLS", SLEEP_BETWEEN_AI_CALLS))
     SHEET_WRITE_DELAY = float(os.environ.get("SHEET_WRITE_DELAY", SHEET_WRITE_DELAY))
     SHEET_WRITE_BATCH_SIZE = int(os.environ.get("SHEET_WRITE_BATCH_SIZE", SHEET_WRITE_BATCH_SIZE))
     config_source = "Environment/Defaults" if any(os.environ.get(k) for k in ["GEMINI_API_KEY", "SPREADSHEET_URL", "SERVICE_ACCOUNT_JSON_PATH"]) else "Defaults"


# --- Keyword Configuration (Global) ---
KEYWORDS = {"VPS": ["VPS", "Ch·ª©ng kho√°n VPS", "VPBankS", "VPBS", "VPBank Securities"], "SSI": ["SSI", "C√¥ng ty C·ªï ph·∫ßn Ch·ª©ng kho√°n SSI", "Ch·ª©ng kho√°n SSI"], "TCBS": ["TCBS", "C√¥ng ty C·ªï ph·∫ßn Ch·ª©ng kho√°n K·ªπ Th∆∞∆°ng", "Techcom Securities", "Ch·ª©ng kho√°n Techcombank"], "DNSE": ["DNSE", "C√¥ng ty C·ªï ph·∫ßn Ch·ª©ng kho√°n DNSE", "DNSE Securities", "Ch·ª©ng kho√°n DNSE"], "MBS": ["MBS", "C√¥ng ty C·ªï ph·∫ßn Ch·ª©ng kho√°n MB", "MB Securities", "Ch·ª©ng kho√°n MB"], "VNDirect": ["VNDirect", "C√¥ng ty C·ªï ph·∫ßn Ch·ª©ng kho√°n VNDirect", "VNDS", "Ch·ª©ng kho√°n VnDirect"]}
COMPETITOR_SHEET_MAP = {"VPS": "VPS_Ch∆∞∆°ng tr√¨nh", "SSI": "SSI_Ch∆∞∆°ng tr√¨nh", "TCBS": "TCBS_Ch∆∞∆°ng tr√¨nh", "DNSE": "DNSE_Ch∆∞∆°ng tr√¨nh", "MBS": "MBS_Ch∆∞∆°ng tr√¨nh", "VNDirect": "VNDirect_Ch∆∞∆°ng tr√¨nh"}

# --- Google Sheet Header V3 (Global) ---
# Gi·ªØ nguy√™n SHEET_HEADER_V3 v√† PROGRAM_SHEET_HEADER
SHEET_HEADER_V3 = [
    'Th·ªùi gian c·∫≠p nh·∫≠t', 'Ti√™u ƒë·ªÅ b√†i vi·∫øt', 'C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch', 'L√† c√¥ng ty ƒë·ªëi th·ªß?', 'L√† qu·∫£ng c√°o?',
    'T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn', 'Th√¥ng ƒëi·ªáp ch√≠nh (AI)', 'T√≥m t·∫Øt ph√¢n t√≠ch LLM (AI)', 'URL', 'Tr·∫°ng th√°i x·ª≠ l√Ω',
    'Tr·ªçng t√¢m b√†i vi·∫øt (AI)', 'Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)', 'Gi·ªçng ƒëi·ªáu (AI)', 'N·ªôi dung g·ªëc'
]

PROGRAM_SHEET_HEADER = [
    'Th·ªùi gian c·∫≠p nh·∫≠t', 'T√™n', 'K√™nh ti·∫øp nh·∫≠n', 'Th·ªùi gian tri·ªÉn khai', 'Lo·∫°i h√¨nh s·∫£n ph·∫©m',
    'ƒêi·ªÅu ki·ªán tham gia', 'ƒê·∫∑c ƒëi·ªÉm ch√≠nh', '∆Øu ƒë√£i n·∫øu c√≥', 'ƒê·ªëi t∆∞·ª£ng kh√°ch h√†ng m·ª•c ti√™u',
    'URL B√†i vi·∫øt ngu·ªìn', 'T√™n ch∆∞∆°ng tr√¨nh t∆∞∆°ng ƒë·ªìng ·ªü MBS', 'ƒêi·ªÉm gi·ªëng s∆° b·ªô', 'ƒêi·ªÉm kh√°c s∆° b·ªô'
]
# --------------------------------------------------------------------

# --- Timezone & Gemini Model Name (Global) ---
TARGET_TIMEZONE = 'Asia/Ho_Chi_Minh'
GEMINI_MODEL_NAME = 'gemini-1.5-flash-latest'
model = None # Global Gemini model instance
gc = None # Global gspread client instance
mbs_program_data_for_comparison = [] # Global MBS program data

# --- Validate Configuration ---
print("\n--- Validating Configuration ---")
config_valid = True; errors = []
# Ki·ªÉm tra c√°c bi·∫øn B·∫ÆT BU·ªòC ph·∫£i c√≥ gi√° tr·ªã
if not SERVICE_ACCOUNT_JSON_PATH: errors.append("Service Account JSON Path is missing.")
elif IS_COLAB and not os.path.exists(SERVICE_ACCOUNT_JSON_PATH): errors.append(f"Service Account JSON file not found at: {SERVICE_ACCOUNT_JSON_PATH}") # Ch·ªâ check exist n·∫øu ·ªü Colab
if not GEMINI_API_KEY: errors.append("Gemini API Key is missing.")
if not SPREADSHEET_URL or not SPREADSHEET_URL.startswith("https://docs.google.com/spreadsheets/d/"): errors.append("Invalid or missing Google Sheet URL.")
TARGET_WORKSHEET_NAME = TARGET_WORKSHEET_NAME.strip().strip('"');
if not TARGET_WORKSHEET_NAME: errors.append("Target Worksheet Name is empty.")
if not isinstance(HOURS_TO_CHECK, int) or HOURS_TO_CHECK <= 0: errors.append("Hours to Check must be > 0.")
if not isinstance(MAX_SCROLLS, int) or MAX_SCROLLS <= 0: errors.append("Max Scrolls must be > 0.")
# Th√™m ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu cho c√°c s·ªë kh√°c n·∫øu c·∫ßn
if not isinstance(SLEEP_BETWEEN_AI_CALLS, (int, float)) or SLEEP_BETWEEN_AI_CALLS < 0: errors.append("AI Sleep must be a non-negative number.")
if not isinstance(SHEET_WRITE_DELAY, (int, float)) or SHEET_WRITE_DELAY < 0: errors.append("Sheet Write Delay must be a non-negative number.")
if not isinstance(SHEET_WRITE_BATCH_SIZE, int) or SHEET_WRITE_BATCH_SIZE <= 0: errors.append("Sheet Write Batch Size must be > 0.")


if errors:
    config_valid = False
    print("‚ùå CONFIGURATION ERRORS:")
    for err in errors: print(f"   - {err}")
else:
    print("‚úÖ Configuration validation passed.")
    print(f"   Source: {config_source}")
    print(f"   Service Account Path: {'Set (exists check varies)' if SERVICE_ACCOUNT_JSON_PATH else 'Not Set'}")
    print(f"   Gemini Key: {'Set' if GEMINI_API_KEY else 'Not Set'}")
    print(f"   Sheet URL: {SPREADSHEET_URL}")
    print(f"   Target Tab: {TARGET_WORKSHEET_NAME}")
    print(f"   Hours Check: {HOURS_TO_CHECK}, Max Scrolls: {MAX_SCROLLS}")
    print(f"   Sheet Header V3 ({len(SHEET_HEADER_V3)} cols): {SHEET_HEADER_V3}")

print("--- Configuration & Globals Loaded ---")

# Cell 4: Google Authentication

print("\n--- Attempting Google Authentication ---")

# ƒê·∫£m b·∫£o c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë√£ ƒë∆∞·ª£c import t·ª´ Cell 2
# (gspread, auth, default, IS_COLAB, traceback)


def authenticate_google_sheets(service_account_key_path: str): # Th√™m tham s·ªë ƒë∆∞·ªùng d·∫´n
    """Authenticates using a Service Account JSON key file."""
    print(f"üîë Attempting Google Sheets authentication using Service Account: {service_account_key_path}")
    if not service_account_key_path or not os.path.exists(service_account_key_path):
         print(f"‚ùå Critical Error: Service Account key file not found at path: {service_account_key_path}")
         print("   Please upload the key file or check the path in Cell 2 (SERVICE_ACCOUNT_JSON_PATH).")
         return None

    try:
        # S·ª≠ d·ª•ng service_account thay v√¨ ph∆∞∆°ng th·ª©c c≈©
        gc = gspread.service_account(filename=service_account_key_path)
        # Optional: Th·ª≠ m·ªü sheet ƒë·ªÉ ki·ªÉm tra quy·ªÅn truy c·∫≠p s·ªõm
        # gc.open_by_key(GOOGLE_SHEET_ID) # C·∫ßn GOOGLE_SHEET_ID ·ªü ƒë√¢y ho·∫∑c truy·ªÅn v√†o
        print("‚úÖ Google Sheets authentication successful using Service Account.")
        return gc
    except FileNotFoundError:
         print(f"‚ùå Critical Error: Service Account key file explicitly not found at: {service_account_key_path}")
         return None
    except Exception as e:
        print(f"‚ùå Critical Error: Failed Google Sheets authentication with Service Account: {e}")
        print("   Hints: ")
        print("     - Ensure the JSON key file path is correct.")
        print("     - Ensure the JSON file itself is valid.")
        print("     - **Crucially:** Ensure the Service Account email (inside the JSON key) has EDIT permissions on the target Google Sheet(s).")
        print("     - Ensure the 'Google Sheets API' is enabled in the GCP project associated with the Service Account.")
        traceback.print_exc(limit=2) # In traceback ƒë·ªÉ debug
        return None

# --- Ch·∫°y th·ª≠ x√°c th·ª±c ---
if IS_COLAB:
    gc = authenticate_google_sheets(SERVICE_ACCOUNT_JSON_PATH) # G√°n k·∫øt qu·∫£ v√†o bi·∫øn gc global
    if gc:
        print("\n‚úÖ Authentication Test Successful! (Global 'gc' object created)")
        # Optional: Th·ª≠ thao t√°c ƒë∆°n gi·∫£n ƒë·ªÉ ki·ªÉm tra API
        try:
            print(f"   Attempting to open Sheet URL provided in config...")
            # S·ª≠ d·ª•ng SPREADSHEET_URL v√† TARGET_WORKSHEET_NAME t·ª´ Cell 3
            spreadsheet_test = gc.open_by_url(SPREADSHEET_URL)
            print(f"   Successfully opened Spreadsheet: '{spreadsheet_test.title}'")
            try:
                worksheet_test = spreadsheet_test.worksheet(TARGET_WORKSHEET_NAME)
                print(f"   Successfully accessed Worksheet: '{worksheet_test.title}'")
            except gspread.exceptions.WorksheetNotFound:
                print(f"   Warning: Worksheet '{TARGET_WORKSHEET_NAME}' not found, but sheet access seems OK.")
            except Exception as e_ws:
                 print(f"   ‚ö†Ô∏è Error accessing worksheet '{TARGET_WORKSHEET_NAME}': {e_ws}")
        except gspread.exceptions.APIError as e_api:
             print(f"   ‚ùå API Error opening sheet (Check URL, Share Permissions, and Sheets API enabled): {e_api}")
        except Exception as e_open:
            print(f"   ‚ùå Error opening spreadsheet (Check URL & Share Permissions): {e_open}")
    else:
        print("\n‚ùå Authentication Test Failed! (Global 'gc' is None)")
else:
    print("‚ÑπÔ∏è Skipping authentication test (not in Colab).")

# Cell 5: ƒê·ªãnh nghƒ©a c√°c H√†m Helper v√† H√†m Crawl

# Cell 5.1

# --- Imports c·∫ßn thi·∫øt cho c√°c h√†m trong cell n√†y ---
import sys
import json
import pandas as pd
import pytz
import os
import time
import traceback
import gspread
import requests
import random
import re
import subprocess
import zipfile
import io
import locale # C·∫ßn cho parse_time v·ªõi %A
from datetime import datetime, timedelta
import shutil # D√πng ƒë·ªÉ x√≥a th∆∞ m·ª•c t·∫°m

# Th√™m c√°c th∆∞ vi·ªán Selenium n·∫øu ch∆∞a import ·ªü Cell 2
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import (
        TimeoutException, WebDriverException, NoSuchElementException,
        StaleElementReferenceException, NoSuchWindowException
    )
    SELENIUM_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è WARNING: Selenium library not fully imported. Web crawling will fail.")
    webdriver = None
    Options = None
    By = None
    WebDriverWait = None
    EC = None
    TimeoutException = WebDriverException = NoSuchElementException = StaleElementReferenceException = NoSuchWindowException = Exception # Fallback
    SELENIUM_AVAILABLE = False

# Th√™m th∆∞ vi·ªán Google Auth n·∫øu ch∆∞a import ·ªü Cell 2
try:
    from google.oauth2 import service_account
    from google.auth import exceptions as google_auth_exceptions
    GOOGLE_AUTH_AVAILABLE = True
except ImportError:
     print("‚ö†Ô∏è WARNING: google-auth library not imported. Google Sheets/Drive auth will fail.")
     service_account = None
     google_auth_exceptions = None
     GOOGLE_AUTH_AVAILABLE = False

# Th√™m th∆∞ vi·ªán Generative AI n·∫øu ch∆∞a import ·ªü Cell 2
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è WARNING: google-generativeai library not imported. AI features will be disabled.")
    genai = None
    GEMINI_AVAILABLE = False

# Th√™m th∆∞ vi·ªán files c·ªßa Colab (n·∫øu d√πng save_data_local_v3)
try:
    from google.colab import files
    IS_COLAB = True
    IS_COLAB_FILES = True
except ImportError:
    files = None # Define files as None if not in Colab
    IS_COLAB = False
    IS_COLAB_FILES = False


print("\n--- Defining Helper and Crawler Functions ---")

# Cell 5.2
# === Environment Setup Functions

def install_chrome_and_driver():
    """Installs Google Chrome and the appropriate ChromeDriver using CfT API and apt fallback."""
    print("\n--- Installing Chrome & ChromeDriver ---")
    chromedriver_installed_successfully = False
    chromedriver_final_path = "/usr/local/bin/chromedriver" # Standard path

    try:
        # --- 1. Install Google Chrome ---
        print("Updating apt and installing prerequisites...")
        try:
            # Run apt update quietly
            subprocess.run(['sudo', 'apt-get', 'update'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            print("   Installing required packages (wget, gnupg, unzip, ca-certs, libvulkan1)...")
            # Install prerequisite packages
            install_prereqs_cmd = ['sudo', 'apt-get', 'install', '-y', 'wget', 'gnupg', 'unzip', 'ca-certificates', 'libvulkan1']
            prereq_result = subprocess.run(install_prereqs_cmd, check=False, capture_output=True, text=True)
            if prereq_result.returncode != 0:
                print(f"   ‚ö†Ô∏è Warning installing prerequisites (Code: {prereq_result.returncode}): {prereq_result.stderr[:500]}...")
            else:
                print("   Prerequisites installed/updated.")
        except Exception as e_apt_update:
             print(f"   ‚ùå FATAL ERROR during apt update/prerequisites installation: {e_apt_update}")
             return False # Cannot proceed without prerequisites

        print("\nDownloading and installing Google Chrome browser...")
        chrome_deb_path = "google-chrome-stable_current_amd64.deb"
        chrome_url = "https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb"
        try:
            # Download Chrome .deb file
            print(f"   Downloading from {chrome_url}...")
            subprocess.run(['wget', '-q', '-O', chrome_deb_path, chrome_url], check=True, timeout=180)
            print(f"   Downloaded Chrome package: {chrome_deb_path}")

            # Install using dpkg
            print("   Installing Chrome via dpkg...")
            dpkg_result = subprocess.run(['sudo', 'dpkg', '-i', chrome_deb_path], check=False, capture_output=True, text=True)
            if dpkg_result.returncode != 0:
                print(f"   dpkg install reported issues (Code: {dpkg_result.returncode}). Attempting dependency fix...")
                # print(f"   dpkg stderr:\n{dpkg_result.stderr}") # Uncomment for full dpkg error log

            # Fix potential dependencies using apt-get install -f
            print("   Running 'apt-get install -f' to resolve dependencies (if any)...")
            subprocess.run(['sudo', 'apt-get', 'install', '-f', '-y'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            print("   Dependency check/fix complete.")
            print("‚úÖ Google Chrome browser installed successfully.")

        except subprocess.TimeoutExpired:
            print(f"   ‚ùå ERROR: Timeout occurred while downloading Chrome from {chrome_url}")
            return False
        except subprocess.CalledProcessError as e_chrome_install:
            print(f"   ‚ùå ERROR during Chrome download/install process: {e_chrome_install}")
            if hasattr(e_chrome_install, 'stderr'): print(f"   Stderr: {e_chrome_install.stderr}")
            return False
        except Exception as e_chrome:
            print(f"   ‚ùå UNEXPECTED ERROR during Chrome installation: {e_chrome}")
            return False
        finally:
             # Clean up the downloaded .deb file
             if os.path.exists(chrome_deb_path):
                 try: os.remove(chrome_deb_path); print(f"   Cleaned up {chrome_deb_path}")
                 except OSError as e_clean: print(f"   Warning: Could not remove {chrome_deb_path}: {e_clean}")

        # --- 2. Install ChromeDriver ---
        print("\nAttempting to install matching ChromeDriver...")
        # Get installed Chrome version
        chrome_version = None
        try:
            result = subprocess.run(['google-chrome', '--version'], capture_output=True, text=True, check=True)
            chrome_version = result.stdout.strip().split(' ')[-1]
            print(f"Detected installed Chrome version: {chrome_version}")
        except Exception as e_get_ver:
            print(f"Warning: Could not automatically detect Chrome version ({e_get_ver}). Will target latest Stable ChromeDriver.")

        # Find suitable ChromeDriver download URL using Chrome for Testing (CfT) JSON API
        print("Searching for ChromeDriver via Chrome for Testing (CfT) API...")
        api_endpoint = "https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json"
        chromedriver_url = None
        driver_source = "Unknown"
        download_version_info = "N/A"

        try:
            print(f"   Querying CfT API: {api_endpoint}")
            response = requests.get(api_endpoint, timeout=30)
            response.raise_for_status()
            versions_data = response.json()
            print("   Successfully fetched data from CfT API.")

            target_chrome_build = None
            if chrome_version:
                target_chrome_build = ".".join(chrome_version.split('.')[:3]) # Major.Minor.Build
                print(f"   Targeting ChromeDriver for Chrome build: {target_chrome_build}")

            found_driver_info = None
            # Try to find exact build match first
            if target_chrome_build:
                for channel, data in versions_data.get('channels', {}).items():
                    channel_version = data.get('version', '')
                    if channel_version.startswith(target_chrome_build):
                        downloads = data.get('downloads', {}).get('chromedriver', [])
                        url_info = next((d for d in downloads if d.get('platform') == 'linux64'), None)
                        if url_info:
                            found_driver_info = url_info
                            driver_source = f"CfT API (Exact Build: {channel} v{channel_version})"
                            download_version_info = channel_version
                            print(f"   Found exact build match: {driver_source}")
                            break

            # If no exact match or version unknown, get latest Stable
            if not found_driver_info:
                print(f"   Exact build match not found or Chrome version unknown. Getting latest Stable...")
                stable_data = versions_data.get('channels', {}).get('Stable', {})
                if stable_data:
                    stable_version = stable_data.get('version', 'Unknown Stable')
                    downloads = stable_data.get('downloads', {}).get('chromedriver', [])
                    url_info = next((d for d in downloads if d.get('platform') == 'linux64'), None)
                    if url_info:
                        found_driver_info = url_info
                        driver_source = f"CfT API (Latest Stable v{stable_version})"
                        download_version_info = stable_version
                        print(f"   Found latest Stable driver: {driver_source}")
                    else: print("   Could not find linux64 download for Stable in CfT API.")
                else: print("   Could not find 'Stable' channel data in CfT API.")

            if found_driver_info:
                chromedriver_url = found_driver_info.get('url')
            else:
                 # Trigger fallback if CfT API fails completely
                 raise ValueError("Failed to find any suitable ChromeDriver URL using CfT API.")

        except requests.exceptions.RequestException as req_err:
           print(f"   Error during CfT API request: {req_err}. Will attempt apt fallback.")
           raise ValueError(f"CfT API request failed: {req_err}") # Trigger fallback
        except Exception as e_api:
            print(f"   Unexpected error processing CfT API data: {e_api}. Will attempt apt fallback.")
            raise ValueError(f"CfT API processing error: {e_api}") # Trigger fallback

        # --- Download and Extract ChromeDriver ---
        if not chromedriver_url:
             raise ValueError("ChromeDriver URL is missing. Cannot proceed.")

        print(f"\nDownloading ChromeDriver ({driver_source}, Version: {download_version_info}) from: {chromedriver_url}")
        try:
            # Download the zip file
            zip_response = requests.get(chromedriver_url, stream=True, timeout=180)
            zip_response.raise_for_status()
            print("   Download complete.")

            # Extract to a temporary directory
            print("   Unzipping ChromeDriver...")
            zip_file_bytes = io.BytesIO(zip_response.content)
            extract_path = "/tmp/chromedriver_extract_temp"
            if os.path.exists(extract_path): # Clean up old temp dir if exists
                 shutil.rmtree(extract_path)
            os.makedirs(extract_path, exist_ok=True)

            # Find the chromedriver executable within the zip structure
            chromedriver_zip_internal_path = None
            with zipfile.ZipFile(zip_file_bytes) as zf:
                namelist = zf.namelist()
                # Common patterns: chromedriver-linux64/chromedriver or just chromedriver
                potential_paths = [m for m in namelist if m.endswith('/chromedriver') and not m.endswith('/')]
                if potential_paths:
                     chromedriver_zip_internal_path = potential_paths[0] # Take the first match
                elif 'chromedriver' in namelist:
                     chromedriver_zip_internal_path = 'chromedriver' # Check root level
                else:
                     raise FileNotFoundError(f"Could not find 'chromedriver' executable inside zip. Contents: {namelist}")

                print(f"   Extracting '{chromedriver_zip_internal_path}' to {extract_path}")
                zf.extract(chromedriver_zip_internal_path, extract_path)
                extracted_file_temp_path = os.path.join(extract_path, chromedriver_zip_internal_path)
                print(f"   Extracted temp path: {extracted_file_temp_path}")

            # Move to final destination and set permissions
            print(f"   Moving to final path: {chromedriver_final_path}")
            # Ensure destination directory exists (it should, but safer)
            os.makedirs(os.path.dirname(chromedriver_final_path), exist_ok=True)
            # Remove existing file/link at destination first
            if os.path.lexists(chromedriver_final_path):
                 subprocess.run(['sudo', 'rm', '-f', chromedriver_final_path], check=False)
            # Move the extracted file
            subprocess.run(['sudo', 'mv', extracted_file_temp_path, chromedriver_final_path], check=True)
            # Set execute permissions
            print("   Setting execute permissions...")
            subprocess.run(['sudo', 'chmod', '+x', chromedriver_final_path], check=True)

            # Clean up temporary extraction directory
            try: shutil.rmtree(extract_path); print(f"   Cleaned up temp directory: {extract_path}")
            except OSError as e_clean: print(f"   Warning: Could not cleanup temp dir {extract_path}: {e_clean}")

            # Final verification
            if os.path.exists(chromedriver_final_path) and os.access(chromedriver_final_path, os.X_OK):
                 print(f"‚úÖ ChromeDriver (from {driver_source}) successfully installed at {chromedriver_final_path}.")
                 chromedriver_installed_successfully = True
                 # Verify installed version
                 try:
                     driver_version_output = subprocess.check_output([chromedriver_final_path, '--version']).decode('utf-8').strip()
                     print(f"   Installed ChromeDriver version: {driver_version_output}")
                 except Exception as e_check_ver: print(f"   Could not verify installed ChromeDriver version: {e_check_ver}")
            else:
                 raise FileNotFoundError(f"ChromeDriver verification failed at {chromedriver_final_path} after install.")

        except requests.exceptions.RequestException as e_download:
            print(f"   ‚ùå ERROR downloading ChromeDriver: {e_download}. Will attempt apt fallback.")
            raise # Trigger fallback
        except (zipfile.BadZipFile, FileNotFoundError) as e_zip:
            print(f"   ‚ùå ERROR processing ChromeDriver zip: {e_zip}. Will attempt apt fallback.")
            raise # Trigger fallback
        except subprocess.CalledProcessError as e_mv_chmod:
            print(f"   ‚ùå ERROR moving/setting permissions: {e_mv_chmod}. Stderr: {e_mv_chmod.stderr}")
            print("   Will attempt apt fallback.")
            raise # Trigger fallback
        except Exception as e_unzip:
             print(f"   ‚ùå UNEXPECTED ERROR during download/unzip: {e_unzip}. Will attempt apt fallback.")
             raise # Trigger fallback

    # --- Fallback: Install via apt ---
    except Exception as e_install_driver: # Catch errors raised from download/install steps
        print(f"\n--- ChromeDriver download/install failed ({e_install_driver}). Attempting apt fallback. ---")
        try:
            print("   Running 'apt-get update' before apt install...")
            subprocess.run(['sudo', 'apt-get', 'update'], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)

            print("   Installing 'chromium-chromedriver' package via apt...")
            apt_install_cmd = ['sudo', 'apt-get', 'install', '-y', 'chromium-chromedriver']
            apt_result = subprocess.run(apt_install_cmd, check=False, capture_output=True, text=True)

            if apt_result.returncode != 0:
                 print(f"   ‚ùå ERROR installing chromium-chromedriver via apt (Code: {apt_result.returncode}).")
                 print(f"   Stderr: {apt_result.stderr[:500]}...")
                 raise Exception("apt install chromium-chromedriver failed") # Signal failure
            else:
                 print("   'chromium-chromedriver' package installed via apt.")

            # Find the path where apt installed chromedriver
            fallback_paths = ["/usr/lib/chromium-browser/chromedriver", "/usr/bin/chromedriver"]
            installed_fallback_path = next((p for p in fallback_paths if os.path.exists(p) and os.access(p, os.X_OK)), None)

            if installed_fallback_path:
                 print(f"   Found fallback chromedriver from apt at: {installed_fallback_path}")
                 # Ensure the standard path links to the apt version
                 link_needed = True
                 if os.path.lexists(chromedriver_final_path):
                     if os.path.islink(chromedriver_final_path) and os.readlink(chromedriver_final_path) == installed_fallback_path:
                          print(f"   Symlink already correct: {chromedriver_final_path} -> {installed_fallback_path}")
                          link_needed = False
                          chromedriver_installed_successfully = True
                     else:
                          print(f"   Removing incorrect file/link at {chromedriver_final_path}...")
                          subprocess.run(['sudo', 'rm', '-f', chromedriver_final_path], check=False)

                 if link_needed:
                     print(f"   Creating symlink: {chromedriver_final_path} -> {installed_fallback_path}")
                     subprocess.run(['sudo', 'ln', '-sf', installed_fallback_path, chromedriver_final_path], check=True)
                     # Verify symlink creation
                     if os.path.exists(chromedriver_final_path) and os.access(chromedriver_final_path, os.X_OK) and os.path.islink(chromedriver_final_path):
                         print("   Symlink created successfully.")
                         chromedriver_installed_successfully = True
                     else: print(f"   ‚ùå Failed to create/verify symlink at {chromedriver_final_path}.")

                 # Verify version of the fallback driver via the symlink
                 if chromedriver_installed_successfully:
                     try:
                         driver_version_output = subprocess.check_output([chromedriver_final_path, '--version']).decode('utf-8').strip()
                         print(f"   Fallback ChromeDriver (apt) version: {driver_version_output}")
                     except Exception as e_check_ver_fb: print(f"   Could not verify fallback driver version: {e_check_ver_fb}")

            else: # apt install ran, but executable not found
                 print(f"   ‚ùå ERROR: apt install finished, but chromedriver executable not found in {fallback_paths}.")

        except Exception as fallback_err:
            print(f"   ‚ùå‚ùå FATAL ERROR during apt fallback installation process: {fallback_err}")
            chromedriver_installed_successfully = False # Ensure failure state

    # --- Final Conclusion ---
    print("-" * 60)
    if chromedriver_installed_successfully:
        print("‚úÖ‚úÖ Chrome & ChromeDriver Installation Attempt Completed Successfully. ‚úÖ‚úÖ")
        return True
    else:
        print("‚ùå‚ùå Chrome & ChromeDriver Installation Attempt Failed after all methods. ‚ùå‚ùå")
        print("    Web crawling might not work. Check logs above for details.")
        return False
# --- End install_chrome_and_driver ---

# Cell 5.3
# === AI Model Functions               ===

import traceback
import json
import time # Th√™m import time ƒë·ªÉ d√πng sleep

# Assume 'genai' and 'google_auth_exceptions' are imported elsewhere or mocked
# Example imports if needed:
try:
    import google.generativeai as genai
    from google.auth import exceptions as google_auth_exceptions
    # Define GEMINI_AVAILABLE based on successful import
    GEMINI_AVAILABLE = True
except ImportError:
    print("Warning: google.generativeai or google.auth failed to import. Gemini features may be unavailable.")
    genai = None
    google_auth_exceptions = None
    GEMINI_AVAILABLE = False # Set flag to False

# --- Globals (Assuming defined elsewhere, provide defaults for robustness) ---
# C·∫ßn ƒë·∫£m b·∫£o c√°c bi·∫øn n√†y ƒë∆∞·ª£c g√°n gi√° tr·ªã ·ªü cell kh√°c
GEMINI_API_KEY = globals().get("GEMINI_API_KEY", None) # Example fetch from globals
GEMINI_MODEL_NAME = globals().get("GEMINI_MODEL_NAME", "gemini-1.5-flash") # Example, use your actual model
model = globals().get("model", None) # Global model variable

# Function Definitions start at Level 0 indentation
def setup_gemini():
    """Configures and initializes the global Gemini AI model instance."""
    # Inside function: Level 1 indentation (4 spaces)
    global model, GEMINI_API_KEY, GEMINI_MODEL_NAME, GEMINI_AVAILABLE # Access global vars

    print(f"\n--- Configuring Gemini AI (Model: {GEMINI_MODEL_NAME}) ---")

    # Check if library is available (using the flag set during import)
    if not GEMINI_AVAILABLE or not genai or not google_auth_exceptions:
        # Inside 'if': Level 2 indentation (8 spaces)
        print("‚ùå ERROR: google-generativeai library not available or failed to import. Cannot set up Gemini.")
        model = None
        return # Exit the function

    # Check for API Key
    if not GEMINI_API_KEY:
        # Inside 'if': Level 2 indentation (8 spaces)
        print("‚ùå ERROR: Gemini API Key is missing (Check Cell 3 configuration).")
        model = None
        return # Exit the function

    model = None # Reset model before setup (Level 1)
    try: # Level 1
        # Inside 'try': Level 2 indentation (8 spaces)
        print(f"   Using API Key ending in '...{GEMINI_API_KEY[-4:]}'")
        genai.configure(api_key=GEMINI_API_KEY)

        # Initialize the Generative Model
        print(f"   Initializing model: {GEMINI_MODEL_NAME}")
        # --- S·ª≠a ƒë·ªïi nh·ªè: ƒê·∫£m b·∫£o safety_settings m·∫∑c ƒë·ªãnh ƒë∆∞·ª£c √°p d·ª•ng khi kh·ªüi t·∫°o model ---
        # C√°c c√†i ƒë·∫∑t an to√†n ti√™u chu·∫©n
        default_safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]
        model_instance = genai.GenerativeModel(
            GEMINI_MODEL_NAME,
            safety_settings=default_safety_settings # √Åp d·ª•ng c√†i ƒë·∫∑t an to√†n m·∫∑c ƒë·ªãnh
        )

        # Verify model access with a simple, short generation
        print(f"   Verifying model access with a test prompt...")
        # Check if genai.types exists before using it
        gen_config_class = getattr(genai, 'types', None)
        if gen_config_class and hasattr(gen_config_class, 'GenerationConfig'):
            # Inside 'if': Level 3 indentation (12 spaces)
            test_config = gen_config_class.GenerationConfig(max_output_tokens=10, temperature=0.1)
            # S·ª≠ d·ª•ng safety settings ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n cho test call lu√¥n
            _ = model_instance.generate_content("Say 'test ok'", generation_config=test_config) # Safety settings ƒë√£ c√≥ trong model
        else: # Level 2
            # Inside 'else': Level 3 indentation (12 spaces)
            print("   [Warning] genai.types.GenerationConfig not found, skipping detailed verification.")
            _ = model_instance.generate_content("Say 'test ok'") # Simpler verification

        # Assign to global variable only after successful verification (Level 2)
        model = model_instance
        print(f"‚úÖ Global Gemini Model '{GEMINI_MODEL_NAME}' Initialized and Verified.")

    # CORRECTED EXCEPTION HANDLING STARTS HERE:
    # Block 1: Catch the specific error IF the module was imported
    except google_auth_exceptions.DefaultCredentialsError as cred_err: # Level 1
        print(f"‚ùå ERROR: Google Cloud Credentials Error: {cred_err}")
        print("   Ensure your environment is authenticated if not using API key directly.")
        model = None
    # Block 2: Catch ANY other exception
    except Exception as config_err: # Level 1
        error_str = str(config_err).lower()
        if "api key not valid" in error_str or "permission denied" in error_str or "authenticate" in error_str:
             print(f"‚ùå ERROR: Invalid Gemini API Key or Permission Denied. Check the key in Cell 3.")
        elif "could not find model" in error_str or "model_name is invalid" in error_str:
             print(f"‚ùå ERROR: Gemini Model '{GEMINI_MODEL_NAME}' not found or unavailable. Check the model name.")
        elif "quota" in error_str or "rate limit" in error_str or "resource exhausted" in error_str:
             print(f"‚ùå ERROR: Quota or Rate Limit likely exceeded during setup/verification: {config_err}")
        elif "deadline exceeded" in error_str:
             print(f"‚ùå ERROR: Timeout during setup/verification call to Gemini API: {config_err}")
        else:
             print(f"‚ùå UNEXPECTED ERROR Configuring/Verifying Gemini: {config_err}")
             traceback.print_exc(limit=1)
        model = None
# --- End setup_gemini ---


# Function Definition start at Level 0 indentation
def get_default_analysis_result(error_message="Analysis failed"):
    """Returns a default, structured analysis result dictionary on error."""
    # Ensure keys match the successful output structure of analyze_with_gemini
    return {
        "primary_competitor_focus": "L·ªói AI",
        "is_program_pr": False,
        "promoted_program_name": "",
        "is_brand_pr": False, # <<< TH√äM M·ªöI
        "program_details": { # Default empty details structure
            "ThoiGianTrienKhai": "Kh√¥ng r√µ", "LoaiHinhSanPham": "Kh√¥ng r√µ",
            "DieuKienThamGia": "Kh√¥ng r√µ", "DacDiemChinh": "Kh√¥ng r√µ",
            "UuDaiNeuCo": "Kh√¥ng r√µ", "DoiTuongKhachHang": "Kh√¥ng r√µ"
        },
        "analysis_summary": f"L·ªói: {error_message}"
    }
# --- End get_default_analysis_result ---


# Function Definition start at Level 0 indentation
def analyze_with_gemini(model, title, content, detected_competitors_list):
    """
    Analyzes article using LLM for focus, PR type, program details.
    Expects JSON output. Includes retry logic.
    """
    # --- Input Validation and Preparation ---
    if not model:
        print("   [AnalyzerV2] Skipping analysis: AI Model not available.")
        return get_default_analysis_result("AI Model not available")
    if not title and not content:
        print("   [AnalyzerV2] Skipping analysis: Empty title and content.")
        return get_default_analysis_result("Empty title and content")

    competitor_context = "Kh√¥ng c√≥ ƒë·ªëi th·ªß n√†o ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc"
    if detected_competitors_list:
        unique_competitors = sorted(list(set(filter(None, map(str, detected_competitors_list)))))
        if unique_competitors:
             competitor_context = ", ".join(unique_competitors)
        else:
            print("   [AnalyzerV2] Warning: Competitor list provided but contained no valid names.")

    max_content_length = 8000 # Gi·ªØ nguy√™n ho·∫∑c ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn
    truncated_content = content[:max_content_length] if content else ""
    if content and len(content) > max_content_length:
        print(f"   [AnalyzerV2] Input content truncated to {max_content_length} chars.")

    # --- Define Prompt (V2 JSON Structure) ---
    detail_fields_prompt = """
        - ThoiGianTrienKhai: (String) Th·ªùi gian b·∫Øt ƒë·∫ßu/k·∫øt th√∫c ch∆∞∆°ng tr√¨nh n·∫øu c√≥ (v√≠ d·ª•: "01/08/2024 - 30/09/2024", "Th√°ng 8/2024"). M·∫∑c ƒë·ªãnh "Kh√¥ng r√µ".
        - LoaiHinhSanPham: (String) Lo·∫°i s·∫£n ph·∫©m/d·ªãch v·ª• ch√≠nh (v√≠ d·ª•: "Vay Margin", "M·ªü t√†i kho·∫£n"). M·∫∑c ƒë·ªãnh "Kh√¥ng r√µ".
        - DieuKienThamGia: (String) ƒêi·ªÅu ki·ªán ch√≠nh ƒë·ªÉ tham gia (v√≠ d·ª•: "Kh√°ch h√†ng m·ªõi", "Giao d·ªãch t·ªëi thi·ªÉu 1 t·ª∑"). M·∫∑c ƒë·ªãnh "Kh√¥ng r√µ".
        - DacDiemChinh: (String) M√¥ t·∫£ ng·∫Øn g·ªçn ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t nh·∫•t. M·∫∑c ƒë·ªãnh "Kh√¥ng r√µ".
        - UuDaiNeuCo: (String) ∆Øu ƒë√£i c·ª• th·ªÉ n·∫øu c√≥ (v√≠ d·ª•: "L√£i su·∫•t 9%", "Mi·ªÖn ph√≠ giao d·ªãch"). M·∫∑c ƒë·ªãnh "Kh√¥ng r√µ".
        - DoiTuongKhachHang: (String) ƒê·ªëi t∆∞·ª£ng kh√°ch h√†ng m·ª•c ti√™u (v√≠ d·ª•: "Nh√† ƒë·∫ßu t∆∞ c√° nh√¢n"). M·∫∑c ƒë·ªãnh "Kh√¥ng r√µ".
    """
    prompt = f"""
Ph√¢n t√≠ch b√†i b√°o t√†i ch√≠nh ti·∫øng Vi·ªát sau. ƒê·ªëi th·ªß c·∫°nh tranh c√≥ th·ªÉ li√™n quan: [{competitor_context}].

**Ti√™u ƒë·ªÅ:** "{title}"

**N·ªôi dung (tr√≠ch ƒëo·∫°n t·ªëi ƒëa {max_content_length} k√Ω t·ª±):**
"{truncated_content}"

**Y√™u c·∫ßu Ph√¢n t√≠ch:**
1.  **Tr·ªçng t√¢m ch√≠nh (primary_competitor_focus):** C√¥ng ty n√†o trong [{competitor_context}] (n·∫øu c√≥) l√† tr·ªçng t√¢m ch√≠nh c·ªßa b√†i? (Tr·∫£ v·ªÅ t√™n c√¥ng ty ho·∫∑c "N/A").
2.  **L√† PR Ch∆∞∆°ng tr√¨nh? (is_program_pr):** B√†i vi·∫øt c√≥ ph·∫£i PR/qu·∫£ng c√°o cho m·ªôt ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m/d·ªãch v·ª• C·ª§ TH·ªÇ c·ªßa c√¥ng ty tr·ªçng t√¢m kh√¥ng? (Tr·∫£ v·ªÅ boolean `true` ho·∫∑c `false`).
3.  **T√™n Ch∆∞∆°ng tr√¨nh (promoted_program_name):** N·∫øu (2) l√† `true`, tr√≠ch xu·∫•t t√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë√≥. (Tr·∫£ v·ªÅ string, ho·∫∑c `""` n·∫øu kh√¥ng √°p d·ª•ng).
4.  **Chi ti·∫øt Ch∆∞∆°ng tr√¨nh (program_details):** N·∫øu (2) l√† `true`, tr√≠ch xu·∫•t chi ti·∫øt theo c·∫•u tr√∫c sau (ƒëi·ªÅn "Kh√¥ng r√µ" n·∫øu thi·∫øu). N·∫øu (2) l√† `false`, tr·∫£ v·ªÅ object r·ªóng {{}}.
    {detail_fields_prompt}
5.  **L√† PR Th∆∞∆°ng hi·ªáu? (is_brand_pr):** Ngay c·∫£ khi kh√¥ng ph·∫£i PR cho m·ªôt ch∆∞∆°ng tr√¨nh c·ª• th·ªÉ (t·ª©c l√† m·ª•c 2 l√† `false`), b√†i vi·∫øt n√†y c√≥ m·ª•c ƒë√≠ch ch√≠nh l√† qu·∫£ng b√° h√¨nh ·∫£nh, uy t√≠n, th√†nh t·ª±u chung, ho·∫∑c l√† m·ªôt b√†i ph·ªèng v·∫•n l√£nh ƒë·∫°o mang t√≠nh t√≠ch c·ª±c cho c√¥ng ty tr·ªçng t√¢m (·ªü m·ª•c 1) kh√¥ng? (Tr·∫£ v·ªÅ boolean `true` ho·∫∑c `false`).
6.  **T√≥m t·∫Øt Ph√¢n t√≠ch (analysis_summary):** T√≥m t·∫Øt ng·∫Øn g·ªçn (1-2 c√¢u) m·ª•c ƒë√≠ch/n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt li√™n quan ƒë·∫øn c√¥ng ty tr·ªçng t√¢m (ho·∫∑c t√≥m t·∫Øt chung n·∫øu kh√¥ng c√≥ tr·ªçng t√¢m).

**ƒê·ªãnh d·∫°ng Output:**
Ch·ªâ tr·∫£ v·ªÅ m·ªôt JSON object DUY NH·∫§T, h·ª£p l·ªá, kh√¥ng c√≥ gi·∫£i th√≠ch b√™n ngo√†i. C√°c key ph·∫£i ƒë√∫ng nh∆∞ trong v√≠ d·ª•.

**V√≠ d·ª• JSON (Tr∆∞·ªùng h·ª£p PR Ch∆∞∆°ng tr√¨nh):**
```json
{{
  "primary_competitor_focus": "VPS",
  "is_program_pr": true,
  "promoted_program_name": "∆Øu ƒë√£i ph√≠ giao d·ªãch 0%",
  "is_brand_pr": false,
  "program_details": {{
    "ThoiGianTrienKhai": "T·ª´ 01/07/2024", "LoaiHinhSanPham": "Giao d·ªãch ch·ª©ng kho√°n c∆° s·ªü",
    "DieuKienThamGia": "Kh√°ch h√†ng m·ªü t√†i kho·∫£n m·ªõi", "DacDiemChinh": "Mi·ªÖn ph√≠ giao d·ªãch ch·ª©ng kho√°n c∆° s·ªü.",
    "UuDaiNeuCo": "Ph√≠ giao d·ªãch 0%", "DoiTuongKhachHang": "Nh√† ƒë·∫ßu t∆∞ c√° nh√¢n"
  }},
  "analysis_summary": "B√†i vi·∫øt qu·∫£ng b√° ch∆∞∆°ng tr√¨nh mi·ªÖn ph√≠ giao d·ªãch 0% c·ªßa VPS d√†nh cho kh√°ch h√†ng m·ªõi."
}}

{{
  "primary_competitor_focus": "SSI",
  "is_program_pr": false,
  "promoted_program_name": "",
  "is_brand_pr": true,
  "program_details": {{}},
  "analysis_summary": "B√†i ph·ªèng v·∫•n Ch·ªß t·ªãch SSI v·ªÅ tri·ªÉn v·ªçng v√† ni·ªÅm tin v√†o th·ªã tr∆∞·ªùng, nh·∫•n m·∫°nh ti·ªÅm nƒÉng c·ªßa c√¥ng ty."
}}
```"""

    # === PH·∫¶N B·ªî SUNG: G·ªåI API V√Ä X·ª¨ L√ù K·∫æT QU·∫¢ ===
    MAX_RETRIES = 2
    RETRY_DELAY = 5 # Gi√¢y

    for attempt in range(MAX_RETRIES):
        try:
            print(f"   [AnalyzerV2 Attempt {attempt+1}/{MAX_RETRIES}] Calling AI for '{title[:50]}...'")

            # --- C·∫•u h√¨nh cho l·ªùi g·ªçi API ---
            # Y√™u c·∫ßu tr·∫£ v·ªÅ JSON v√† ƒë·∫∑t nhi·ªát ƒë·ªô th·∫•p ƒë·ªÉ b√°m s√°t s·ª± th·∫≠t
            gen_config = genai.types.GenerationConfig(
                temperature=0.2, # Gi·ªØ nguy√™n ho·∫∑c tƒÉng nh·∫π n·∫øu mu·ªën AI linh ho·∫°t h∆°n trong vi·ªác nh·∫≠n di·ªán PR th∆∞∆°ng hi·ªáu
                response_mime_type="application/json"
            )

            request_options = {'timeout': 120} # Gi·ªØ nguy√™n ho·∫∑c tƒÉng n·∫øu prompt d√†i h∆°n

            # --- G·ªçi API ---
            response = model.generate_content(
                prompt,
                generation_config=gen_config,
                request_options=request_options
            )
            print(f"   [AnalyzerV2 Attempt {attempt+1}] Response received.")

            # --- Ki·ªÉm tra n·ªôi dung b·ªã ch·∫∑n ---
            if not response.parts:
                 block_reason = "Unknown"
                 if hasattr(response, 'prompt_feedback') and hasattr(response.prompt_feedback,'block_reason'):
                      block_reason = response.prompt_feedback.block_reason
                 if block_reason == 'SAFETY':
                      print(f"   [AnalyzerV2 Attempt {attempt+1}] ‚ö†Ô∏è Content blocked by SAFETY filter. Retrying if possible...")
                      if attempt < MAX_RETRIES - 1:
                           time.sleep(RETRY_DELAY)
                           continue
                      else:
                           raise ValueError(f"Content blocked by SAFETY filter after {MAX_RETRIES} attempts.")
                 else:
                      raise ValueError(f"Content blocked by API. Reason: {block_reason}")

            # --- Parse JSON (v·ªõi fallback) ---
            response_text = response.text
            result_dict = None
            try:
                result_dict = json.loads(response_text)
                print(f"   [AnalyzerV2 Attempt {attempt+1}] ‚úÖ Parsed Direct JSON.")
            except json.JSONDecodeError:
                print(f"   [AnalyzerV2 Attempt {attempt+1}] ‚ö†Ô∏è Direct JSON parse failed. Cleaning and retrying...")
                cleaned_text = response_text.strip().lstrip('```json').rstrip('```').strip()
                if not cleaned_text:
                    if attempt < MAX_RETRIES - 1:
                        print(f"   [AnalyzerV2 Attempt {attempt+1}] Cleaned JSON empty. Retrying...")
                        time.sleep(RETRY_DELAY)
                        continue
                    else:
                        raise ValueError("Cleaned JSON response is empty after retries.")
                try:
                    result_dict = json.loads(cleaned_text)
                    print(f"   [AnalyzerV2 Attempt {attempt+1}] ‚úÖ Parsed Cleaned JSON.")
                except json.JSONDecodeError as e_json_clean:
                    if attempt < MAX_RETRIES - 1:
                        print(f"   [AnalyzerV2 Attempt {attempt+1}] Cleaned JSON parse failed: {e_json_clean}. Retrying...")
                        time.sleep(RETRY_DELAY)
                        continue
                    else:
                         raise ValueError(f"Failed to parse JSON even after cleaning and retries: {e_json_clean}. Original text: {response_text[:200]}...")

            # --- X√°c th·ª±c v√† Chu·∫©n h√≥a k·∫øt qu·∫£ ---
            final_result = {
                "primary_competitor_focus": str(result_dict.get("primary_competitor_focus", "N/A")).strip(),
                "is_program_pr": bool(result_dict.get("is_program_pr", False)),
                "promoted_program_name": str(result_dict.get("promoted_program_name", "")).strip(),
                "is_brand_pr": bool(result_dict.get("is_brand_pr", False)), # <<< TH√äM M·ªöI
                "program_details": {},
                "analysis_summary": str(result_dict.get("analysis_summary", "N/A")).strip()
            }

            if final_result["is_program_pr"]:
                raw_details = result_dict.get("program_details")
                if isinstance(raw_details, dict) and raw_details:
                    final_result["program_details"] = {
                        "ThoiGianTrienKhai": str(raw_details.get("ThoiGianTrienKhai", "Kh√¥ng r√µ")).strip(),
                        "LoaiHinhSanPham": str(raw_details.get("LoaiHinhSanPham", "Kh√¥ng r√µ")).strip(),
                        "DieuKienThamGia": str(raw_details.get("DieuKienThamGia", "Kh√¥ng r√µ")).strip(),
                        "DacDiemChinh": str(raw_details.get("DacDiemChinh", "Kh√¥ng r√µ")).strip(),
                        "UuDaiNeuCo": str(raw_details.get("UuDaiNeuCo", "Kh√¥ng r√µ")).strip(),
                        "DoiTuongKhachHang": str(raw_details.get("DoiTuongKhachHang", "Kh√¥ng r√µ")).strip()
                    }
                elif raw_details is None or not isinstance(raw_details, dict):
                    print(f"   [AnalyzerV2] Warning: program_details was not a valid dict from LLM (is_program_pr=True). Received: {type(raw_details)}. Setting to default.")
                    default_details = get_default_analysis_result()["program_details"]
                    final_result["program_details"] = default_details

            print(f"   [AnalyzerV2] ‚úÖ Analysis successful for '{title[:50]}...'")
            return final_result

        except Exception as e:
            print(f"‚ùå ERROR during AI Analysis attempt {attempt+1} for '{title[:50]}...': {e}")
            if attempt < MAX_RETRIES - 1:
                print(f"   Retrying after {RETRY_DELAY} seconds...")
                time.sleep(RETRY_DELAY)
            else:
                print(f"   [AnalyzerV2] Max retries reached. Returning error result.")
                # traceback.print_exc(limit=2)
                return get_default_analysis_result(f"L·ªói ph√¢n t√≠ch AI sau {MAX_RETRIES} l·∫ßn th·ª≠: {str(e)[:150]}")

    print(f"   [AnalyzerV2] Reached end of analysis function unexpectedly after loop for '{title[:50]}...'")
    return get_default_analysis_result("L·ªói logic kh√¥ng mong mu·ªën trong h√†m ph√¢n t√≠ch")

# --- End analyze_with_gemini ---

## Cell 5.4
import json
# Assume 'genai' is imported correctly elsewhere if needed for the API call
# import google.generativeai as genai # Example import

# Mock model object for testing if you don't have the real one
class MockModel:
    def generate_content(self, *args, **kwargs):
        # Simulate a successful response structure
        class MockResponse:
            def __init__(self):
                # Example valid JSON response
                self.text = json.dumps({
                    "best_mbs_match_name": "MBS Example Program",
                    "similarity_high": True,
                    "similarities": "Both target individual investors.",
                    "differences": "New program has higher minimum investment."
                })
                self.parts = [self.text] # Simulate having parts
        return MockResponse()

# Replace with your actual model or keep MockModel for testing
# model = genai.GenerativeModel(...)
model = MockModel() # Using the mock model for this example

def compare_with_mbs_programs_llm(model, new_program_name, new_program_details_dict, mbs_programs_list):
    """
    Compares a new program with MBS list using LLM. Expects JSON output.
    Includes error handling and clearer default results.
    """
    # --- Default Result Structure ---
    comparison_results = {
        'best_mbs_match_name': "N/A",          # Default: Not Applicable / No Comparison
        'similarity_high': False,
        'similarities': "N/A",
        'differences': "N/A"
    }

    # --- Input Validation ---
    if not model:
        print("   [Comparer] Skipping comparison: AI Model unavailable.")
        # Indicate error clearly in the result
        return {**comparison_results, 'best_mbs_match_name': "L·ªói (No AI Model)"}
    if not new_program_name:
         print("   [Comparer] Skipping comparison: New program name is empty.")
         return {**comparison_results, 'best_mbs_match_name': "L·ªói (T√™n CT m·ªõi r·ªóng)"}
    if not mbs_programs_list:
        print("   [Comparer] Skipping comparison: MBS program list is empty or unavailable.")
        # Distinguish between empty list and other issues if possible
        return {**comparison_results, 'best_mbs_match_name': "Kh√¥ng c√≥ CT MBS ƒë·ªÉ SS"}

    # --- Prepare Prompt Inputs ---
    # Create a concise description for the new program
    new_details_parts = []
    # Focus on key differentiating aspects for comparison
    key_aspects = ['LoaiHinhSanPham', 'DacDiemChinh', 'UuDaiNeuCo', 'DoiTuongKhachHang', 'ThoiGianTrienKhai']
    for key in key_aspects:
        value = new_program_details_dict.get(key)
        # Only include meaningful values
        if value and str(value).strip() and str(value).strip().lower() != "kh√¥ng r√µ":
            # Use shorter keys in the prompt for brevity
            short_key_map = {
                'LoaiHinhSanPham': 'Lo·∫°i SP', 'DacDiemChinh': 'ƒê·∫∑c ƒëi·ªÉm',
                'UuDaiNeuCo': '∆Øu ƒë√£i', 'DoiTuongKhachHang': 'ƒê·ªëi t∆∞·ª£ng',
                'ThoiGianTrienKhai': 'TG', 'DieuKienThamGia': 'ƒêK'
            }
            short_key = short_key_map.get(key, key) # Fallback to original key if not in map
            # Limit value length to avoid overly long prompts
            new_details_parts.append(f"{short_key}: {str(value)[:100]}")
    new_details_str = "; ".join(new_details_parts) if new_details_parts else "Kh√¥ng c√≥ m√¥ t·∫£ chi ti·∫øt."

    # Create list of MBS programs (Name and 'details' which should be 'ƒê·∫∑c ƒëi·ªÉm ch√≠nh')
    mbs_program_descs = []
    for prog in mbs_programs_list:
        name = prog.get('name','').strip()
        # 'details' comes from 'ƒê·∫∑c ƒëi·ªÉm ch√≠nh' during read - Make sure this key exists in your data
        details = prog.get('details','').strip()
        if name: # Only include programs with a name
             # Limit length of description per program
             mbs_program_descs.append(f"- T√™n: {name}\n  M√¥ t·∫£: {details[:150]}")

    if not mbs_program_descs:
        print("   [Comparer] Skipping comparison: No valid MBS programs with names found in the provided list.")
        return {**comparison_results, 'best_mbs_match_name': "Kh√¥ng c√≥ CT MBS h·ª£p l·ªá"}

    mbs_list_str = "\n".join(mbs_program_descs)

    # --- Define Comparison Prompt ---
    # Prompt asking for JSON output with specific keys and structure
    prompt = f"""
So s√°nh ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m t√†i ch√≠nh M·ªöI sau ƒë√¢y:
### Ch∆∞∆°ng tr√¨nh M·ªõi:
T√™n: "{new_program_name}"
M√¥ t·∫£: "{new_details_str}"

### Danh s√°ch Ch∆∞∆°ng tr√¨nh Hi·ªán c√≥ c·ªßa MBS:
{mbs_list_str}

### Y√™u c·∫ßu So s√°nh:
1.  **T√¨m Ch∆∞∆°ng tr√¨nh T∆∞∆°ng ƒë·ªìng Nh·∫•t:** T·ª´ danh s√°ch MBS, h√£y t√¨m ra ch∆∞∆°ng tr√¨nh DUY NH·∫§T c√≥ v·∫ª t∆∞∆°ng ƒë·ªìng nh·∫•t v·ªÅ m·∫∑t m·ª•c ti√™u, ƒë·ªëi t∆∞·ª£ng ho·∫∑c lo·∫°i h√¨nh s·∫£n ph·∫©m so v·ªõi ch∆∞∆°ng tr√¨nh m·ªõi. N·∫øu kh√¥ng c√≥ ch∆∞∆°ng tr√¨nh n√†o th·ª±c s·ª± t∆∞∆°ng ƒë·ªìng ƒë√°ng k·ªÉ, tr·∫£ v·ªÅ chu·ªói "Kh√¥ng t√¨m th·∫•y".
2.  **ƒê√°nh gi√° ƒê·ªô T∆∞∆°ng ƒë·ªìng:** M·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa ch∆∞∆°ng tr√¨nh m·ªõi v√† ch∆∞∆°ng tr√¨nh MBS t√¨m ƒë∆∞·ª£c (·ªü b∆∞·ªõc 1) l√† CAO (`true`) hay TH·∫§P (`false`)? Ch·ªâ ƒë√°nh gi√° l√† CAO n·∫øu ch√∫ng c√≥ c√πng √Ω t∆∞·ªüng c·ªët l√µi ho·∫∑c gi·∫£i quy·∫øt c√πng m·ªôt nhu c·∫ßu ch√≠nh c·ªßa kh√°ch h√†ng. N·∫øu kh√¥ng t√¨m th·∫•y ch∆∞∆°ng tr√¨nh MBS t∆∞∆°ng ƒë·ªìng, k·∫øt qu·∫£ ph·∫£i l√† `false`.
3.  **ƒêi·ªÉm Gi·ªëng nhau:** N·∫øu ƒë·ªô t∆∞∆°ng ƒë·ªìng l√† CAO (`true`), h√£y li·ªát k√™ 1-2 ƒëi·ªÉm gi·ªëng nhau ch√≠nh y·∫øu nh·∫•t gi·ªØa hai ch∆∞∆°ng tr√¨nh. N·∫øu ƒë·ªô t∆∞∆°ng ƒë·ªìng TH·∫§P, tr·∫£ v·ªÅ "N/A".
4.  **ƒêi·ªÉm Kh√°c bi·ªát:** N·∫øu ƒë·ªô t∆∞∆°ng ƒë·ªìng l√† CAO (`true`), h√£y li·ªát k√™ 1-2 ƒëi·ªÉm kh√°c bi·ªát ch√≠nh y·∫øu nh·∫•t (n·∫øu c√≥) gi·ªØa hai ch∆∞∆°ng tr√¨nh. N·∫øu ƒë·ªô t∆∞∆°ng ƒë·ªìng TH·∫§P, tr·∫£ v·ªÅ "N/A".

### ƒê·ªãnh d·∫°ng Output:
Ch·ªâ tr·∫£ v·ªÅ m·ªôt JSON object DUY NH·∫§T, h·ª£p l·ªá, kh√¥ng c√≥ gi·∫£i th√≠ch b√™n ngo√†i. C√°c kh√≥a ph·∫£i l√†: `best_mbs_match_name` (String), `similarity_high` (Boolean), `similarities` (String), `differences` (String).

V√≠ d·ª• JSON (T∆∞∆°ng ƒë·ªìng cao):
```json
{{
  "best_mbs_match_name": "MBS Margin 9.9",
  "similarity_high": true,
  "similarities": "C√πng l√† ch∆∞∆°ng tr√¨nh cho vay margin v·ªõi l√£i su·∫•t ∆∞u ƒë√£i; Nh·∫Øm v√†o NƒêT c√° nh√¢n.",
  "differences": "Ch∆∞∆°ng tr√¨nh m·ªõi c√≥ ƒëi·ªÅu ki·ªán tham gia kh√°c; Th·ªùi gian ∆∞u ƒë√£i c·ªßa MBS d√†i h∆°n."
}}
```""" # End of the f-string prompt

    # --- API Call ---
    try:
        print(f"   [Comparer] Comparing '{new_program_name}' with {len(mbs_program_descs)} MBS programs...")
        # Configure generation settings for comparison
        # Ensure genai is imported and configured if using the real API
        # gen_config = genai.types.GenerationConfig(
        #     temperature=0.1, # Low temperature for factual comparison
        #     response_mime_type="application/json" # Expect JSON
        # )
        # Dummy config for mock model or if genai not used directly here
        gen_config = {"temperature": 0.1, "response_mime_type": "application/json"}

        # Use less strict safety settings for comparison task if needed, or standard ones
        safety_settings_comp = [
             {"category": c, "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
             for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH",
                       "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]
        ]

        response = model.generate_content(
            prompt,
            # generation_config=gen_config, # Use this if using real genai
            # safety_settings=safety_settings_comp, # Use this if using real genai
            request_options={'timeout': 120} # Reasonable timeout for comparison task
            # Pass dummy kwargs if needed by mock model, or remove if using real API
            , generation_config_dummy=gen_config, safety_settings_dummy=safety_settings_comp
        )

        # Check for blocked content before parsing
        # Accessing prompt_feedback might differ depending on the actual library/response object
        # This is a common pattern for Google's API
        if not response.parts:
             block_reason = "Unknown"
             # Check if feedback exists (adjust based on actual library)
             if hasattr(response, 'prompt_feedback') and hasattr(response.prompt_feedback,'block_reason'):
                block_reason = response.prompt_feedback.block_reason
             raise ValueError(f"Comparer content blocked by API. Reason: {block_reason}")
        print(f"   [Comparer] Response received.")

        # --- Process and Parse Response ---
        result_dict = None
        response_text = response.text
        try:
            # Try direct JSON parsing first
            result_dict = json.loads(response_text)
            print(f"   [Comparer] ‚úÖ Comparison successful (Parsed Direct JSON).")
        except json.JSONDecodeError:
             # Fallback: Clean potential markdown ```json ... ``` wrappers
             print(f"   [Comparer] ‚ö†Ô∏è Direct JSON parse failed. Cleaning and retrying...")
             # Improved cleaning: handle potential leading/trailing whitespace and markdown fences
             cleaned_text = response_text.strip()
             if cleaned_text.startswith("```json"):
                 cleaned_text = cleaned_text[len("```json"):].strip()
             if cleaned_text.startswith("```"):
                  cleaned_text = cleaned_text[len("```"):].strip()
             if cleaned_text.endswith("```"):
                 cleaned_text = cleaned_text[:-len("```")].strip()

             if not cleaned_text: raise ValueError("Comparer cleaned JSON response is empty.")
             result_dict = json.loads(cleaned_text) # Try parsing cleaned text
             print(f"   [Comparer] ‚úÖ Comparison successful (Parsed Cleaned JSON).")

        # --- Normalize and Validate Result Dictionary ---
        # Ensure keys exist before processing, provide defaults
        final_match = str(result_dict.get('best_mbs_match_name', "Kh√¥ng t√¨m th·∫•y")).strip()
        final_sim_high = bool(result_dict.get('similarity_high', False))
        final_sim = "N/A" # Default for similarities/differences
        final_diff = "N/A"

        # Only populate similarities/differences if similarity is high
        if final_sim_high:
            sim_text = str(result_dict.get('similarities', "N/A")).strip()
            diff_text = str(result_dict.get('differences', "N/A")).strip()
            # Use more descriptive defaults if AI returns "N/A" despite high similarity
            final_sim = sim_text if sim_text and sim_text.lower() != "n/a" else "Kh√¥ng c√≥ ƒëi·ªÉm gi·ªëng n·ªïi b·∫≠t ƒë∆∞·ª£c n√™u."
            final_diff = diff_text if diff_text and diff_text.lower() != "n/a" else "Kh√¥ng c√≥ ƒëi·ªÉm kh√°c bi·ªát n·ªïi b·∫≠t ƒë∆∞·ª£c n√™u."

            # Ensure a match name is present if similarity is high, unless it was explicitly "Kh√¥ng t√¨m th·∫•y"
            if not final_match or final_match.lower() == "n/a" or final_match == "Kh√¥ng t√¨m th·∫•y":
                 print(f"   [Comparer] Warning: Similarity rated high, but no valid match name provided ('{final_match}'). Setting default.")
                 # Decide on a default or leave as is depending on desired behavior
                 final_match = "T∆∞∆°ng ƒë·ªìng cao nh∆∞ng kh√¥ng r√µ t√™n CT kh·ªõp" # Example default
                 # Or maybe force similarity low if name is missing? Depends on strictness needed.
                 # final_sim_high = False # Option: Force low similarity if name is missing

        else: # similarity_high is False
            # Ensure match is "Kh√¥ng t√¨m th·∫•y" if similarity is low
            if final_match != "Kh√¥ng t√¨m th·∫•y":
                 print(f"   [Comparer] Warning: Similarity rated low, but match found ('{final_match}'). Overriding match to 'Kh√¥ng t√¨m th·∫•y'.")
                 final_match = "Kh√¥ng t√¨m th·∫•y"
            final_sim = "N/A (ƒê·ªô t∆∞∆°ng ƒë·ªìng th·∫•p)"
            final_diff = "N/A (ƒê·ªô t∆∞∆°ng ƒë·ªìng th·∫•p)"


        # Return the final, validated comparison dictionary
        return {
            'best_mbs_match_name': final_match,
            'similarity_high': final_sim_high,
            'similarities': final_sim,
            'differences': final_diff
        }

    # --- Error Handling for API Call / Processing ---
    except Exception as e:
        print(f"   [Comparer] ‚ùå ERROR during comparison for '{new_program_name}': {e}")
        # Log the full traceback for detailed debugging if needed
        # import traceback
        # print(traceback.format_exc())
        # Return a clear error state
        return {
            'best_mbs_match_name': "L·ªói So S√°nh",
            'similarity_high': False,
            'similarities': f"L·ªói: {str(e)[:100]}", # Include brief error message
            'differences': "N/A"
        }
# --- End compare_with_mbs_programs_llm ---

# --- Example Usage ---
# Mock Input Data
new_prog_name_example = "Super Saver Account"
new_prog_details_example = {
    'LoaiHinhSanPham': 'T√†i kho·∫£n ti·∫øt ki·ªám',
    'DacDiemChinh': 'L√£i su·∫•t b·∫≠c thang theo s·ªë d∆∞, mi·ªÖn ph√≠ qu·∫£n l√Ω',
    'UuDaiNeuCo': 'T·∫∑ng voucher du l·ªãch cho s·ªë d∆∞ > 1 t·ª∑',
    'DoiTuongKhachHang': 'Kh√°ch h√†ng c√° nh√¢n c√≥ thu nh·∫≠p ·ªïn ƒë·ªãnh',
    'ThoiGianTrienKhai': '01/01/2024 - 31/12/2024',
    'DieuKienThamGia': 'S·ªë d∆∞ t·ªëi thi·ªÉu 10 tri·ªáu VND'
}
mbs_programs_example = [
    {'name': 'MBS Saving Plus', 'details': 'T√†i kho·∫£n ti·∫øt ki·ªám l√£i su·∫•t c·ªë ƒë·ªãnh 6%/nƒÉm.'},
    {'name': 'MBS Invest Pro', 'details': 'S·∫£n ph·∫©m ƒë·∫ßu t∆∞ tr√°i phi·∫øu doanh nghi·ªáp.'},
    {'name': 'MBS Margin 9.9', 'details': 'Cho vay k√Ω qu·ªπ l√£i su·∫•t ∆∞u ƒë√£i 9.9%.'},
    {'name': '  ', 'details': 'Invalid program with no name'}, # Test invalid entry
    {'name': 'MBS StepUp Savings', 'details': 'Ti·∫øt ki·ªám l√£i su·∫•t tƒÉng d·∫ßn theo th·ªùi gian g·ª≠i.'}, # Potentially similar
]

# Run the comparison
comparison_result = compare_with_mbs_programs_llm(
    model, # Using the MockModel defined earlier
    new_prog_name_example,
    new_prog_details_example,
    mbs_programs_example
)

print("\n--- Comparison Result ---")
print(json.dumps(comparison_result, indent=2, ensure_ascii=False))

## Cell 5.5
import pytz
import re
import locale # C·∫ßn import locale n·∫øu b·∫°n d√πng %A
from datetime import datetime, timedelta
import traceback

# Gi·∫£ s·ª≠ TARGET_TIMEZONE ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü ƒë√¢u ƒë√≥
# TARGET_TIMEZONE = 'Asia/Ho_Chi_Minh' # V√≠ d·ª•

def parse_time(time_text: str, now: datetime, time_limit: datetime) -> datetime | None:
    """
    Ph√¢n t√≠ch c√°c ƒë·ªãnh d·∫°ng th·ªùi gian kh√°c nhau, tr·∫£ v·ªÅ datetime c√≥ m√∫i gi·ªù
    ho·∫∑c None n·∫øu kh√¥ng h·ª£p l·ªá/qu√° c≈©. ƒê√£ t√°i c·∫•u tr√∫c x·ª≠ l√Ω m√∫i gi·ªù.
    """
    if not time_text: return None
    time_text = time_text.strip().replace('\xa0', ' ') # Clean input text

    try:
        # ƒê·∫£m b·∫£o TARGET_TIMEZONE c√≥ s·∫µn, fallback v·ªÅ UTC
        target_tz_name = TARGET_TIMEZONE if 'TARGET_TIMEZONE' in globals() else 'Asia/Ho_Chi_Minh'
        try:
            tz_vn = pytz.timezone(target_tz_name)
        except pytz.exceptions.UnknownTimeZoneError:
            print(f"‚ö†Ô∏è Kh√¥ng r√µ m√∫i gi·ªù '{target_tz_name}', d√πng UTC.")
            tz_vn = pytz.utc
    except Exception as e_tz:
        print(f"‚ö†Ô∏è L·ªói khi c√†i ƒë·∫∑t m√∫i gi·ªù, d√πng UTC: {e_tz}")
        tz_vn = pytz.utc

    # --- ƒê·∫£m b·∫£o 'now' v√† 'time_limit' l√† timezone-aware ---
    # R·∫•t quan tr·ªçng: ƒë·∫£m b·∫£o c√°c tham s·ªë ƒë·∫ßu v√†o ƒë√£ c√≥ m√∫i gi·ªù
    if now.tzinfo is None or now.tzinfo.utcoffset(now) is None:
        print(f"‚ö†Ô∏è Tham s·ªë 'now' ({now}) kh√¥ng c√≥ m√∫i gi·ªù. G√°n m√∫i gi·ªù {tz_vn.zone}.")
        now = tz_vn.localize(now)
    else:
        now = now.astimezone(tz_vn) # Chuy·ªÉn v·ªÅ m√∫i gi·ªù target n·∫øu kh√°c

    if time_limit.tzinfo is None or time_limit.tzinfo.utcoffset(time_limit) is None:
        print(f"‚ö†Ô∏è Tham s·ªë 'time_limit' ({time_limit}) kh√¥ng c√≥ m√∫i gi·ªù. G√°n m√∫i gi·ªù {tz_vn.zone}.")
        time_limit = tz_vn.localize(time_limit)
    else:
        time_limit = time_limit.astimezone(tz_vn) # Chuy·ªÉn v·ªÅ m√∫i gi·ªù target n·∫øu kh√°c
    # --- Xong ph·∫ßn ki·ªÉm tra tham s·ªë ƒë·∫ßu v√†o ---

    parsed_dt = None          # K·∫øt qu·∫£ cu·ªëi c√πng sau khi chu·∫©n h√≥a m√∫i gi·ªù
    dt_intermediate = None    # K·∫øt qu·∫£ t·∫°m th·ªùi t·ª´ c√°c h√†m parse
    parsed_success = False    # C·ªù ƒë√°nh d·∫•u parse th√†nh c√¥ng

    try:
        time_text_lower = time_text.lower() # ƒê·ªÉ kh·ªõp kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng

        # 1. X·ª≠ l√Ω Th·ªùi gian T∆∞∆°ng ƒë·ªëi (d√πng regex)
        is_relative = False
        if "v√†i gi√¢y tr∆∞·ªõc" in time_text_lower or "v·ª´a xong" in time_text_lower:
            dt_intermediate = now; is_relative = True; parsed_success = True
        elif (match := re.search(r'(\d+)\s+ng√†y\s+tr∆∞·ªõc', time_text_lower)):
            dt_intermediate = now - timedelta(days=int(match.group(1))); is_relative = True; parsed_success = True
        elif (match := re.search(r'(\d+)\s+gi·ªù\s+tr∆∞·ªõc', time_text_lower)):
            dt_intermediate = now - timedelta(hours=int(match.group(1))); is_relative = True; parsed_success = True
        elif (match := re.search(r'(\d+)\s+ph√∫t\s+tr∆∞·ªõc', time_text_lower)):
            dt_intermediate = now - timedelta(minutes=int(match.group(1))); is_relative = True; parsed_success = True
        elif "h√¥m nay" in time_text_lower:
            if (time_match := re.search(r'(\d{1,2})[h:](\d{1,2})', time_text_lower)):
                hour, minute = map(int, time_match.groups())
                if 0 <= hour < 24 and 0 <= minute < 60:
                    # T·∫°o datetime aware ngay t·ª´ ƒë·∫ßu
                    dt_intermediate = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
                    is_relative = True
                    parsed_success = True
            # N·∫øu kh√¥ng parse ƒë∆∞·ª£c gi·ªù ph√∫t t·ª´ "h√¥m nay", s·∫Ω th·ª≠ ƒë·ªãnh d·∫°ng tuy·ªát ƒë·ªëi sau

        # 2. X·ª≠ l√Ω Th·ªùi gian Tuy·ªát ƒë·ªëi (n·∫øu ch∆∞a ph·∫£i t∆∞∆°ng ƒë·ªëi)
        if not is_relative:
            # L√†m s·∫°ch v√† chu·∫©n h√≥a chu·ªói
            time_text_proc = re.sub(r'\s+', ' ', time_text.replace(",", "").replace(" SA", " AM").replace(" CH", " PM").strip())
            formats = [
                "%d/%m/%Y - %H:%M", "%d-%m-%Y - %H:%M",
                "%d/%m/%Y %H:%M", "%d-%m-%Y %H:%M",
                "%H:%M %d/%m/%Y",
                "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S.%f%z", # ISO c√≥ timezone
                "%Y-%m-%dT%H:%M:%SZ",                        # ISO UTC Zulu
                "%Y-%m-%d %H:%M:%S",                        # Format ph·ªï bi·∫øn
                "%Y-%m-%dT%H:%M:%S",                        # ISO kh√¥ng c√≥ timezone (s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω sau)
                "%d/%m/%Y", "%d-%m-%Y",                      # Ch·ªâ c√≥ ng√†y
                "%d/%m/%Y %I:%M %p", "%d-%m-%Y %I:%M %p",    # AM/PM
                "%I:%M %p %d/%m/%Y",
            ]

            # Th·ª≠ c√°c ƒë·ªãnh d·∫°ng strptime
            for fmt in formats:
                try:
                    # X·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng c√≥ timezone tr∆∞·ªõc
                    if "%z" in fmt or fmt.endswith("Z"):
                        time_str_tz = time_text_proc.replace("Z", "+0000")
                        time_str_tz = re.sub(r'([+-])(\d{2}):(\d{2})(?:\.\d+)?$', r'\1\2\3', time_str_tz)
                        fmt_to_use = fmt.replace('.%f', '') if '.' not in time_str_tz and '.%f' in fmt else fmt
                        # K·∫øt qu·∫£ t·ª´ strptime v·ªõi %z ƒë√£ l√† aware
                        dt_intermediate = datetime.strptime(time_str_tz, fmt_to_use.replace("Z", "%z"))
                    # X·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng naive (bao g·ªìm c·∫£ AM/PM v√¨ strptime x·ª≠ l√Ω n√≥ th√†nh naive)
                    else:
                        dt_naive = datetime.strptime(time_text_proc, fmt)
                        if "%H" not in fmt and "%M" not in fmt and "%S" not in fmt and "%I" not in fmt:
                            dt_naive = dt_naive.replace(hour=0, minute=0, second=0)
                        # K·∫øt qu·∫£ v·∫´n l√† naive ·ªü ƒë√¢y
                        dt_intermediate = dt_naive

                    # ƒê√°nh d·∫•u th√†nh c√¥ng v√† tho√°t v√≤ng l·∫∑p
                    parsed_success = True
                    # print(f"   [Time] Parsed '{time_text}' with format '{fmt}' -> {dt_intermediate} (Intermediate)") # Debug
                    break
                except (ValueError, TypeError):
                    continue # Th·ª≠ format ti·∫øp theo

            # N·∫øu c√°c ƒë·ªãnh d·∫°ng strptime kh√¥ng th√†nh c√¥ng, th·ª≠ fromisoformat
            if not parsed_success:
                try:
                    # Th·ª≠ parse tr·ª±c ti·∫øp (c√≥ th·ªÉ tr·∫£ v·ªÅ naive ho·∫∑c aware t√πy input)
                    dt_intermediate = datetime.fromisoformat(time_text_proc.replace("Z", "+00:00"))
                    parsed_success = True
                    # print(f"   [Time] Parsed '{time_text}' using direct ISO -> {dt_intermediate} (Intermediate)") # Debug
                except ValueError:
                    # Th·ª≠ thay ' ' b·∫±ng 'T'
                    try:
                        dt_intermediate = datetime.fromisoformat(time_text_proc.replace(" ", "T").replace("Z", "+00:00"))
                        parsed_success = True
                        # print(f"   [Time] Parsed '{time_text}' using ISO (space->T) -> {dt_intermediate} (Intermediate)") # Debug
                    except ValueError:
                        pass # Th·∫•t b·∫°i ho√†n to√†n

        # 3. === Chu·∫©n h√≥a M√∫i gi·ªù T·∫¨P TRUNG ===
        if parsed_success and dt_intermediate is not None:
            try:
                if dt_intermediate.tzinfo is None or dt_intermediate.tzinfo.utcoffset(dt_intermediate) is None:
                    # N·∫øu k·∫øt qu·∫£ l√† naive, g√°n m√∫i gi·ªù target
                    parsed_dt = tz_vn.localize(dt_intermediate, is_dst=None)
                    # print(f"   [Time] Localized naive result: {parsed_dt}") # Debug
                else:
                    # N·∫øu k·∫øt qu·∫£ ƒë√£ aware, chuy·ªÉn ƒë·ªïi v·ªÅ m√∫i gi·ªù target
                    parsed_dt = dt_intermediate.astimezone(tz_vn)
                    # print(f"   [Time] Converted aware result: {parsed_dt}") # Debug
            except (pytz.exceptions.NonExistentTimeError, pytz.exceptions.AmbiguousTimeError) as e_tz_final:
                 print(f"   [Time] Warning: L·ªói khi chu·∫©n h√≥a m√∫i gi·ªù cu·ªëi c√πng cho '{time_text}': {e_tz_final}. B·ªè qua.")
                 parsed_dt = None # Kh√¥ng th·ªÉ chu·∫©n h√≥a m√∫i gi·ªù
                 parsed_success = False # ƒê√°nh d·∫•u l·∫°i l√† th·∫•t b·∫°i
            except Exception as e_conv: # B·∫Øt l·ªói kh√¥ng ng·ªù kh√°c khi chu·∫©n h√≥a
                 print(f"   [Time] Warning: L·ªói kh√¥ng mong mu·ªën khi chu·∫©n h√≥a m√∫i gi·ªù cu·ªëi c√πng cho '{time_text}': {e_conv}. B·ªè qua.")
                 parsed_dt = None
                 parsed_success = False

        # 4. X√°c th·ª±c cu·ªëi c√πng: ƒê√£ parse th√†nh c√¥ng v√† trong gi·ªõi h·∫°n th·ªùi gian?
        if parsed_success and parsed_dt:
            # So s√°nh v·ªõi time_limit (c·∫£ hai ƒë·ªÅu ƒë√£ aware v√† c√πng m√∫i gi·ªù tz_vn)
            if parsed_dt >= time_limit:
                return parsed_dt # Tr·∫£ v·ªÅ k·∫øt qu·∫£ h·ª£p l·ªá
            else:
                # print(f"   [Time] Parsed time {parsed_dt} is older than limit {time_limit}.") # Debug
                return None # Qu√° c≈©
        else:
            # print(f"   [Time] Failed to parse or normalize time: '{time_text}'") # Debug
            return None # Kh√¥ng parse/chu·∫©n h√≥a ƒë∆∞·ª£c

    except ValueError as ve:
        # X·ª≠ l√Ω l·ªói ValueError chung (v√≠ d·ª• t·ª´ int() trong ph·∫ßn relative)
        if "invalid literal for int()" in str(ve):
             print(f"‚ö†Ô∏è L·ªói chuy·ªÉn ƒë·ªïi s·ªë trong th·ªùi gian t∆∞∆°ng ƒë·ªëi cho '{time_text}': {ve}")
        else:
             print(f"‚ö†Ô∏è L·ªói ValueError trong qu√° tr√¨nh parse_time cho '{time_text}': {ve}")
        return None
    except Exception as e: # B·∫Øt c√°c l·ªói kh√¥ng mong mu·ªën kh√°c
        print(f"‚ö†Ô∏è L·ªói KH√îNG MONG MU·ªêN trong parse_time cho '{time_text}': {e}")
        traceback.print_exc(limit=1)
        return None
# --- End parse_time ---

# Cell 5.6
def contains_keyword(text, keywords_list):
    """Checks if any keyword from the list exists in the text (case-insensitive, whole word)."""
    if not text or not keywords_list: return False
    text_lower = text.lower() # Convert text to lower once
    for keyword in keywords_list:
        if not keyword: continue # Skip empty keywords
        # Use word boundaries (\b) to match whole words only
        # Escape special regex characters in the keyword itself
        pattern = r'\b' + re.escape(keyword.lower()) + r'\b'
        if re.search(pattern, text_lower):
            return True # Found a match
    return False # No match found in the entire list
# --- End contains_keyword ---


def get_detailed_article_time(driver):
    """Gets publication time from detail page, prioritizing structured data."""
    # Selectors ordered by reliability (structured data first, then common patterns)
    selectors = [
        "time[datetime]",                       # Standard HTML5 time element
        "meta[itemprop='datePublished'][content]", # Schema.org metadata
        "meta[property='article:published_time'][content]", # Open Graph metadata
        "span[pubdate='pubdate']",              # Older pubdate markers
        "abbr.timeago[title]",                  # Relative time with full date in title
        "span.time[title]",                     # Spans with class 'time' and title attribute
        "span.date[title]",                     # Spans with class 'date' and title attribute
        # Common class names from news sites / CMS
        ".article-publish-date time", ".article-timestamp", ".entry-date",
        ".published-date", ".posted-on time", ".timestamp", ".byline__datetime",
        # CafeF specific / Vietnamese common
        "span.detail-time", "span.pdate", ".pdate", "span.date", ".date",
        # Generic text content search (less reliable, last resort)
        # Look for spans/divs containing typical date/time characters or relative terms
        "//span[contains(text(),'/') or contains(text(),'-') or contains(text(),':') or contains(text(),'ng√†y') or contains(text(),'gi·ªù') or contains(text(),'ph√∫t')][string-length(normalize-space(text())) > 5 and string-length(normalize-space(text())) < 50]",
        "//div[contains(@class,'time') or contains(@class,'date') or contains(@class,'meta')][normalize-space() and string-length(normalize-space(text())) < 50]",
    ]
    # print("   [Time Detail] Searching for time element...")
    for i, selector in enumerate(selectors):
         try:
            # Use a short wait for each selector attempt
            element = WebDriverWait(driver, 0.15).until(
                EC.presence_of_element_located((By.XPATH, selector) if selector.startswith("//") else (By.CSS_SELECTOR, selector))
            )
            # Prioritize extracting structured data from attributes
            datetime_attr = element.get_attribute("datetime")
            if datetime_attr: return datetime_attr # ISO format preferred
            content_attr = element.get_attribute("content") # For meta tags
            if content_attr: return content_attr
            title_attr = element.get_attribute("title")
            if title_attr and len(title_attr) > 5: return title_attr # title often has full datetime

            # Fallback to the element's visible text content
            text = element.text.strip()
            if text: return text
            # If element found but no useful data, continue to next selector
            # print(f"   [Time Detail] Found element with selector {i+1}, but no usable attribute/text.")

         except (TimeoutException, NoSuchElementException, StaleElementReferenceException):
             continue # Element not found or became stale, try next selector
         except Exception as e:
             # Log minor errors if needed for debugging selectors
             # print(f"   [Time Detail] Minor error on selector {i+1} ('{selector}'): {e}")
             continue # Ignore minor errors and proceed

    # print("   [Time Detail] ‚ùå Could not find time element using any selector.")
    return None # Return None if no time information was found
# --- End get_detailed_article_time ---

# Cell 5.7
def get_article_content(driver):
    """Extracts the main article content, cleaning unwanted elements."""
    content = ""
    # Selectors for main content containers, ordered by likely relevance
    content_selectors = [
        "article[itemprop='articleBody']", "div[itemprop='articleBody']", # Schema.org standard
        "div#abody.content-detail", "div.content-detail.text-content", # CafeF likely
        "div.article__content", "div.article-content", # Common article classes
        "div.entry-content", "div.post-content", "div.post-body", # Common CMS/Blog classes
        "div.main-content", "div.story-content", "div.article-body", "div.news-body",
        "div.noidung", # Vietnamese specific
        "div.contentdetail", "div.content_detail",
        "td.detailsView", # Less common table-based layout
        "div[class*='content']", # Generic wildcard (lower priority)
    ]
    main_content_element = None
    # print("   [Content Detail] Searching for main content container...")
    # 1. Find the best matching main content container
    for selector in content_selectors:
        try:
            # Use a short wait, assuming the element is already mostly loaded
            main_content_element = WebDriverWait(driver, 0.2).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            # print(f"   [Content Detail] Found main container with selector: {selector}")
            break # Use the first container found
        except (TimeoutException, NoSuchElementException):
            continue # Try the next selector

    # 2. Extract and clean text if a container was found
    if main_content_element:
        paragraphs_text = []
        try:
            # --- Remove unwanted elements using JavaScript ---
            selectors_to_remove = [
                'script', 'style', 'iframe', 'figure > figcaption', 'noscript', 'aside', 'nav', 'form', 'button', 'input',
                '.ad', '.ads', '.advert', '.advertisement', '.banner', '.quangcao', '.qcunbuyable', '.sponsor', # Ads & Sponsors
                '.related', '.related-posts', '.yarpp-related', '.tags', '.categories', # Related, Tags, Cats
                '.social', '.share', '.sharing', '.socail-wrapper-content', # Social sharing
                '.author', '.byline', '.meta', '.credit', '.timestamp', '.source', # Author/Meta/Source info
                '.comment', '.feedback', '#comments', # Comments sections
                '.video', '.player', '.viewbox-video', 'video', 'audio', # Media players
                '.boxcheckstock', '.VCSortableInPreviewMode', # Specific site elements
                '.print-button', '.email-button', '.subscribe-box', # Action buttons/boxes
                'footer', '.footer', '#footer' # Footer elements within the main content (less likely but possible)
            ]
            try:
                 js_remove_script = f"""
                     var container = arguments[0];
                     var selectors = {json.dumps(selectors_to_remove)};
                     selectors.forEach(function(sel) {{
                         try {{ container.querySelectorAll(sel).forEach(el => el.remove()); }}
                         catch (e) {{ console.warn('Minor error removing selector:', sel, e); }}
                     }});
                 """
                 driver.execute_script(js_remove_script, main_content_element)
                 # print("   [Content Detail] Attempted JS removal of unwanted elements.")
            except Exception as e_remove:
                 # Don't stop if removal fails, just log a warning
                 print(f"   [Content Detail] Warning: JavaScript removal script failed: {e_remove}")

            # --- Extract text primarily from <p> tags ---
            paragraphs = main_content_element.find_elements(By.TAG_NAME, "p")
            if paragraphs:
                # print(f"   [Content Detail] Found {len(paragraphs)} <p> elements after cleaning.")
                seen_texts = set()
                for p in paragraphs:
                    try:
                        p_text = p.text
                        if p_text:
                            p_text_stripped = p_text.strip()
                            # Filter out very short paragraphs and common noisy lines
                            p_lower = p_text_stripped.lower()
                            min_para_length = 40 # Minimum characters for a meaningful paragraph
                            is_meaningful = (
                                len(p_text_stripped) >= min_para_length and
                                p_text_stripped not in seen_texts and # Avoid duplicates
                                not p_lower.startswith(">>") and
                                not p_lower.startswith("·∫£nh:") and
                                not p_lower.startswith("ngu·ªìn:") and
                                not p_lower.startswith("b√†i vi·∫øt li√™n quan") and
                                "tag :" not in p_lower and
                                "copyright" not in p_lower and
                                "li√™n h·ªá qu·∫£ng c√°o" not in p_lower and
                                ("theo " not in p_lower or len(p_lower) > 60) # Allow "theo xyz" if longer sentence
                            )
                            if is_meaningful:
                                paragraphs_text.append(p_text_stripped)
                                seen_texts.add(p_text_stripped)
                    except StaleElementReferenceException: continue # Skip if element goes stale

                # Join the extracted paragraphs
                content = "\n".join(paragraphs_text)
            else:
                # Fallback: If no <p> tags, get the full inner text of the container
                print("   [Content Detail] No <p> tags found inside container, getting container's full text (fallback).")
                try: content = main_content_element.text.strip()
                except StaleElementReferenceException: content = "L·ªói Stale Element (Container Fallback)"

        except Exception as e_process:
            print(f"   [Content Detail] Error processing container: {e_process}. Trying final text fallback.")
            try: content = main_content_element.text.strip() # Final fallback
            except Exception: content = "L·ªói l·∫•y text container (Final Fallback)"
    else:
        # 3. Last Resort: No specific container found, get text from <body>
        print("   [Content Detail] ‚ùå No specific content container found. Getting text from <body>.")
        try:
            body_element = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            # Ideally, remove header/footer from body text too, but it's complex. Get raw text.
            content = body_element.text.strip()
        except Exception as e_body:
            print(f"   [Content Detail] Error getting body text: {e_body}")
            content = "Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung chi ti·∫øt (Body Error)"

    # --- Final Content Check ---
    min_len_threshold = 100 # Minimum acceptable content length
    if content and len(content) >= min_len_threshold:
        # print(f"   [Content Detail] ‚úÖ Extracted content length: {len(content)}")
        # Limit overall length just in case (e.g., for database storage)
        max_overall_len = 25000
        return content[:max_overall_len] if len(content) > max_overall_len else content
    elif content: # Extracted but too short
         print(f"   [Content Detail] ‚ö†Ô∏è Extracted content is too short ({len(content)} chars).")
         return content # Return it anyway, might still be useful
    else: # Nothing extracted
         print(f"   [Content Detail] ‚ùå Failed to extract sufficient content.")
         return "Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung chi ti·∫øt"
# --- End get_article_content ---

## Cell 5.8
def open_spreadsheet(gc, url):
    """Opens Google Sheet using the authenticated gspread client."""
    if not gc:
        print("‚ùå Cannot open sheet: Google client (gc) not authenticated.")
        return None
    if not url or not url.startswith("https://docs.google.com/spreadsheets/d/"):
        print(f"‚ùå Invalid Google Sheet URL provided: {url}")
        return None
    try:
        # Extract Sheet ID for logging, making it slightly less revealing
        sheet_id = url.split('/d/')[1].split('/')[0]
        print(f"üìä Opening Google Sheet (ID: ...{sheet_id[-6:]})...")
        spreadsheet = gc.open_by_url(url)
        print(f"‚úÖ Spreadsheet '{spreadsheet.title}' opened successfully.")
        return spreadsheet
    except gspread.exceptions.APIError as api_error:
         error_details = str(api_error)
         # Check for common, informative errors
         if 'PERMISSION_DENIED' in error_details or '403' in error_details:
             # Try to get the service account email for a more helpful message
             sa_email = "Unknown"
             if hasattr(gc, 'auth') and hasattr(gc.auth, 'service_account_email'):
                 sa_email = gc.auth.service_account_email
             print(f"‚ùå PERMISSION DENIED: Cannot open Google Sheet. Ensure Service Account ({sa_email}) has 'Editor' access to this specific Sheet.")
         elif 'NOT_FOUND' in error_details or '404' in error_details:
              print(f"‚ùå NOT FOUND: Google Sheet URL seems invalid or the Sheet was deleted.")
         else: # General API error
             print(f"‚ùå Google Sheets API Error opening sheet: {api_error}")
         return None
    except Exception as e:
        # Catch any other unexpected exceptions during the open process
        print(f"‚ùå UNEXPECTED Error opening sheet: {e}")
        traceback.print_exc(limit=1) # Print traceback for unexpected errors
        return None
# --- End open_spreadsheet ---

# Cell 5.9
def read_competitor_programs(gc, sheet_url, competitor_sheets_map):
    """Reads program names and details from specified tabs in a Google Sheet."""
    # Use global constants for column names if available, otherwise use defaults
    # Ensure these match the actual headers in your _Ch∆∞∆°ng tr√¨nh sheets
    name_col_header = "t√™n"
    details_col_header = "ƒë·∫∑c ƒëi·ªÉm ch√≠nh"
    name_col_fallback_idx = 1 # Column B (0-based index)
    details_col_fallback_idx = 6 # Column G (0-based index)

    print("\n--- Reading Competitor Program Data from Tabs ---")
    programs_data = {} # { "CompetitorKey": [{"name": "ProgA", "details": "..."}, ...], ... }

    # Input validation
    if not gc: print("   ‚ö†Ô∏è Warning: Google Sheets client unavailable. Cannot read programs."); return programs_data
    if not sheet_url: print("   ‚ö†Ô∏è Warning: Google Sheet URL missing. Cannot read programs."); return programs_data
    if not competitor_sheets_map: print("   ‚ÑπÔ∏è Info: No competitor sheet map provided. Skipping program read."); return programs_data

    spreadsheet = None
    try: # Open the spreadsheet once
        spreadsheet = gc.open_by_url(sheet_url)
        print(f"   Accessing Spreadsheet: '{spreadsheet.title}' for reading programs.")
    except Exception as e:
        print(f"   ‚ùå ERROR: Cannot access Google Sheet at {sheet_url}: {e}. Aborting program read."); return programs_data

    # Iterate through the map to read each specified tab
    for competitor_key, sheet_name in competitor_sheets_map.items():
        programs_data[competitor_key] = [] # Initialize list for this competitor
        try:
            print(f"   Reading tab for '{competitor_key}': '{sheet_name}'...")
            worksheet = spreadsheet.worksheet(sheet_name)
            # Optimize: Get only necessary columns if possible? No, get_all_values is often simpler.
            all_values = worksheet.get_all_values() # Get all data at once

            if len(all_values) < 2: # Need header + at least one data row
                print(f"      Tab '{sheet_name}' is empty or has only headers. Skipping."); continue

            # Find column indices (case-insensitive)
            header = [h.strip().lower() for h in all_values[0]]
            data_rows = all_values[1:]
            try: name_col_idx = header.index(name_col_header)
            except ValueError: name_col_idx = name_col_fallback_idx; print(f"      ‚ö†Ô∏è Warning: '{name_col_header}' header not found in '{sheet_name}', using fallback Col index {name_col_idx+1}.")
            try: details_col_idx = header.index(details_col_header)
            except ValueError: details_col_idx = details_col_fallback_idx; print(f"      ‚ö†Ô∏è Warning: '{details_col_header}' header not found in '{sheet_name}', using fallback Col index {details_col_idx+1}.")

            # Validate indices
            max_cols_header = len(header)
            if not (0 <= name_col_idx < max_cols_header and 0 <= details_col_idx < max_cols_header):
                 print(f"      ‚ùå Error: Invalid column indices ({name_col_idx}, {details_col_idx}) for '{sheet_name}'. Skipping tab."); continue

            count = 0
            for r_idx, row in enumerate(data_rows):
                 row_len = len(row)
                 # Safely get name and details
                 name = str(row[name_col_idx]).strip() if row_len > name_col_idx and row[name_col_idx] else ""
                 details = str(row[details_col_idx]).strip() if row_len > details_col_idx and row[details_col_idx] else ""

                 if name: # Only add if program name exists
                     programs_data[competitor_key].append({"name": name, "details": details})
                     count += 1

            print(f"      Read {count} programs for '{competitor_key}'.")

        except gspread.exceptions.WorksheetNotFound:
            print(f"   ‚ö†Ô∏è Warning: Worksheet '{sheet_name}' for '{competitor_key}' not found.")
        except gspread.exceptions.APIError as api_error:
             print(f"   ‚ùå API Error reading sheet '{sheet_name}': {api_error}")
        except Exception as e:
            print(f"   ‚ùå Unexpected Error reading sheet '{sheet_name}': {e}")

    print(f"--- Finished reading program data. ---")
    return programs_data
# --- End read_competitor_programs ---


def add_or_update_program_sheet(gc, spreadsheet, competitor_key, program_name, program_details_dict,
                                article_url, crawl_time_str, model, mbs_programs_list):
    """Appends new program info to the competitor's program sheet, including LLM comparison results."""
    # Use global constants for sheet map and header
    # Add safety checks in case globals weren't defined/loaded properly
    competitor_map = COMPETITOR_SHEET_MAP if 'COMPETITOR_SHEET_MAP' in globals() else {}
    output_header = PROGRAM_SHEET_HEADER if 'PROGRAM_SHEET_HEADER' in globals() else []
    source_name_global = SOURCE_NAME if 'SOURCE_NAME' in globals() else 'UnknownSource'

    # --- Input Validation ---
    if not gc: print("‚ùå Cannot add program: Google client unavailable."); return
    if not spreadsheet: print("‚ùå Cannot add program: Spreadsheet object invalid."); return
    if not competitor_key or not program_name: print("‚ùå Cannot add program: Missing Competitor Key or Program Name."); return
    if not competitor_map: print("‚ùå Cannot add program: COMPETITOR_SHEET_MAP not defined."); return
    if not output_header: print("‚ùå Cannot add program: PROGRAM_SHEET_HEADER not defined."); return
    if competitor_key not in competitor_map: print(f"‚ùå Cannot add program: Key '{competitor_key}' not in COMPETITOR_SHEET_MAP."); return

    target_sheet_name = competitor_map[competitor_key]
    worksheet = None

    try:
        print(f"   [Program Sheet] Processing '{program_name}' for tab '{target_sheet_name}'...")
        # 1. Get or Create Worksheet & Verify Header
        try:
            worksheet = spreadsheet.worksheet(target_sheet_name)
            print(f"      Found program tab '{target_sheet_name}'.")
            # Check/Update Header (similar to write_to_google_sheet_v3)
            try:
                current_header = worksheet.row_values(1) if worksheet.row_count >= 1 else []
                # Simple check: ensure first column matches and length is sufficient
                if not current_header or len(current_header) < len(output_header) or current_header[0] != output_header[0]:
                    print(f"      Warning: Header mismatch/incomplete in '{target_sheet_name}'. Updating...")
                    worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
                    worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(output_header))}', {'textFormat': {'bold': True}})
            except Exception as e_hdr: print(f"      Warning: Could not check/update header: {e_hdr}")
        except gspread.exceptions.WorksheetNotFound:
            print(f"      Tab not found, creating '{target_sheet_name}'...")
            worksheet = spreadsheet.add_worksheet(title=target_sheet_name, rows=100, cols=len(output_header) + 2)
            worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
            worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(output_header))}', {'textFormat': {'bold': True}})
            print(f"      Created tab and wrote headers.")

        if not worksheet: print(f"   ‚ùå Failed to access/create worksheet '{target_sheet_name}'."); return

        # 2. Compare with MBS Programs using LLM
        comparison_result = {'best_mbs_match_name': "N/A (Kh√¥ng SS)", 'similarity_high': False, 'similarities': "N/A", 'differences': "N/A"}
        if model and mbs_programs_list:
             comparison_result = compare_with_mbs_programs_llm(model, program_name, program_details_dict, mbs_programs_list)
             # Optional short delay after comparison API call can be added here if needed
             # time.sleep(1.0)
        else: print("      Skipping MBS comparison (AI model or MBS list unavailable).")

        # 3. Prepare Data Row
        new_row_dict = {
            'Th·ªùi gian c·∫≠p nh·∫≠t': crawl_time_str, 'T√™n': program_name, 'K√™nh ti·∫øp nh·∫≠n': source_name_global,
            'Th·ªùi gian tri·ªÉn khai': program_details_dict.get('ThoiGianTrienKhai', "Kh√¥ng r√µ"),
            'Lo·∫°i h√¨nh s·∫£n ph·∫©m': program_details_dict.get('LoaiHinhSanPham', "Kh√¥ng r√µ"),
            'ƒêi·ªÅu ki·ªán tham gia': program_details_dict.get('DieuKienThamGia', "Kh√¥ng r√µ"),
            'ƒê·∫∑c ƒëi·ªÉm ch√≠nh': program_details_dict.get('DacDiemChinh', "Kh√¥ng r√µ"),
            '∆Øu ƒë√£i n·∫øu c√≥': program_details_dict.get('UuDaiNeuCo', "Kh√¥ng r√µ"),
            'ƒê·ªëi t∆∞·ª£ng kh√°ch h√†ng m·ª•c ti√™u': program_details_dict.get('DoiTuongKhachHang', "Kh√¥ng r√µ"),
            'URL B√†i vi·∫øt ngu·ªìn': article_url,
            'T√™n ch∆∞∆°ng tr√¨nh t∆∞∆°ng ƒë·ªìng ·ªü MBS': comparison_result.get('best_mbs_match_name', "L·ªói/Kh√¥ng SS"),
            'ƒêi·ªÉm gi·ªëng s∆° b·ªô': comparison_result.get('similarities', "N/A"),
            'ƒêi·ªÉm kh√°c s∆° b·ªô': comparison_result.get('differences', "N/A") }
        row_to_append = [str(new_row_dict.get(col, "")).strip() for col in output_header]

        # 4. Append Row
        print(f"      Appending program data to '{target_sheet_name}'...")
        # Use insert_rows to avoid potential overwrites if sheet grows unexpectedly
        worksheet.append_row(row_to_append, value_input_option='USER_ENTERED', insert_data_option='INSERT_ROWS', table_range='A1')
        print(f"      ‚úÖ Appended '{program_name}' successfully.")

    except gspread.exceptions.APIError as api_error:
         # Handle specific API errors gracefully
         err_str = str(api_error).lower()
         if 'permission denied' in err_str or '403' in err_str: print(f"   ‚ùå PERMISSION DENIED writing to '{target_sheet_name}'.")
         elif 'rate limit' in err_str or '429' in err_str: print(f"   ‚ùå RATE LIMIT Hit writing to '{target_sheet_name}'.")
         else: print(f"   ‚ùå Google Sheets API Error writing to '{target_sheet_name}': {api_error}")
    except Exception as e:
        print(f"   ‚ùå UNEXPECTED Error adding program to sheet '{target_sheet_name}': {e}")
# --- End add_or_update_program_sheet ---


def write_to_google_sheet_v3(gc, spreadsheet, worksheet_name, header, data_to_write):
    """Writes data to a specified worksheet using V3 header structure, with batching and retries."""
    # --- Input Validation ---
    if not gc: print("‚ùå Cannot write V3: Google client unavailable."); return False
    if not spreadsheet: print("‚ùå Cannot write V3: Spreadsheet object invalid."); return False
    if not worksheet_name: print("‚ùå Cannot write V3: Target worksheet name empty."); return False
    if not header: print("‚ùå Cannot write V3: Header list empty."); return False
    if not data_to_write: print("   [Sheet Write V3] ü§∑ No new data to write."); return True

    # --- Setup ---
    worksheet = None
    output_header = header
    num_cols = len(output_header)
    # Get batch size/delay from globals or use defaults
    # Ensure globals are accessed safely with defaults
    batch_size = SHEET_WRITE_BATCH_SIZE if 'SHEET_WRITE_BATCH_SIZE' in globals() and isinstance(SHEET_WRITE_BATCH_SIZE, int) and SHEET_WRITE_BATCH_SIZE > 0 else 50
    write_delay = SHEET_WRITE_DELAY if 'SHEET_WRITE_DELAY' in globals() and isinstance(SHEET_WRITE_DELAY, (int, float)) and SHEET_WRITE_DELAY >= 0 else 1.0

    try:
        print(f"   [Sheet Write V3] Processing write to tab: '{worksheet_name}'...")
        # 1. Get or Create Worksheet & Verify Header
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
            print(f"      Found target tab '{worksheet_name}'.")
            # Check/Update Header (similar logic as before)
            try:
                current_header = worksheet.row_values(1) if worksheet.row_count >= 1 else []
                if not current_header or len(current_header) < num_cols or current_header[:num_cols] != output_header:
                    print(f"      Warning: Header mismatch/incomplete. Updating...")
                    worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
                    worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, num_cols)}', {'textFormat': {'bold': True}})
            except Exception as e_hdr: print(f"      ‚ö†Ô∏è Warning: Error checking/updating header: {e_hdr}")
        except gspread.exceptions.WorksheetNotFound:
            print(f"      Tab not found, creating '{worksheet_name}'...")
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=len(data_to_write) + 100, cols=num_cols + 5)
            worksheet.update('A1', [output_header], value_input_option='USER_ENTERED')
            worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, num_cols)}', {'textFormat': {'bold': True}})
            print(f"      Created tab and wrote headers.")

        if not worksheet: print(f"   ‚ùå Failed to access/create target worksheet '{worksheet_name}'."); return False

        # 2. Prepare Data Rows
        print(f"      Preparing {len(data_to_write)} rows for V3 append...")
        rows_to_append = [[str(item_dict.get(col, "")).strip() for col in output_header] for item_dict in data_to_write]

        # 3. Append Data in Batches
        if rows_to_append:
             num_rows = len(rows_to_append)
             total_batches = (num_rows + batch_size - 1) // batch_size
             print(f"      Appending {num_rows} rows in {total_batches} batches (size: {batch_size})...")
             for i in range(0, num_rows, batch_size):
                 batch = rows_to_append[i:min(i + batch_size, num_rows)]
                 batch_num = (i // batch_size) + 1
                 print(f"         Writing batch {batch_num}/{total_batches} ({len(batch)} rows)...")
                 write_attempt = 0
                 max_write_retries = 2 # Try original + 2 retries on rate limit
                 while write_attempt <= max_write_retries:
                     try:
                          worksheet.append_rows(batch, value_input_option='USER_ENTERED', insert_data_option='INSERT_ROWS', table_range='A1')
                          print(f"         ‚úÖ Batch {batch_num} written.")
                          # Add delay between successful batches if needed
                          if write_delay > 0 and i + batch_size < num_rows: time.sleep(write_delay)
                          break # Success, exit retry loop for this batch
                     except gspread.exceptions.APIError as write_err:
                          err_str = str(write_err).lower()
                          is_rate_limit = ('rate limit' in err_str or '429' in err_str or 'quota' in err_str)
                          if is_rate_limit and write_attempt < max_write_retries:
                               wait_time = (write_delay + 1) * (2 ** write_attempt) # Exponential backoff
                               print(f"         ‚ö†Ô∏è Rate limit/Quota hit (Attempt {write_attempt+1}). Waiting {wait_time:.1f}s and retrying...")
                               time.sleep(wait_time)
                               write_attempt += 1
                          else: # Fatal API error or max retries reached
                               print(f"         ‚ùå API Error on batch {batch_num} (Attempt {write_attempt+1}): {write_err}")
                               print(f"            Skipping remaining batches.")
                               return False # Abort
                     except Exception as general_write_err: # Catch other unexpected errors
                          print(f"         ‚ùå UNEXPECTED Error writing batch {batch_num} (Attempt {write_attempt+1}): {general_write_err}")
                          print(f"            Skipping remaining batches.")
                          return False # Abort

             print(f"   ‚úÖ Appended {num_rows} rows successfully to '{worksheet_name}'.")
             return True
        else:
             print("      No valid rows prepared.")
             return True

    except gspread.exceptions.APIError as api_error:
         # Handle errors during worksheet access/creation
         err_details = str(api_error).lower()
         if 'permission denied' in err_details or '403' in err_details: print(f"   ‚ùå PERMISSION DENIED preparing V3 write to '{worksheet_name}'.")
         else: print(f"   ‚ùå Google Sheets API Error preparing V3 write: {api_error}")
         return False
    except Exception as e:
        print(f"   ‚ùå UNEXPECTED Error during V3 Sheet Write process: {e}")
        return False
# --- End write_to_google_sheet_v3 ---

## Cell 5.10
# === Data Saving & Export Functions   ===
# ========================================

def save_data_local_v3(non_keyword_data, analyzed_data_v3, sheets_success_flag):
    """Combines non-keyword and analyzed keyword data into a single V3-formatted Excel file."""
    print("\n--- Saving All Articles Locally (V3 Format Excel) ---")

    # Prepare timestamp and target timezone safely
    try:
        # Access global TARGET_TIMEZONE if defined, else default
        target_tz_name = TARGET_TIMEZONE if 'TARGET_TIMEZONE' in globals() else 'Asia/Ho_Chi_Minh'
        tz = pytz.timezone(target_tz_name)
    except Exception: tz = pytz.utc
    timestamp = datetime.now(tz).strftime("%Y%m%d_%H%M%S")

    # Define Excel header safely
    base_header_fallback = [ # Fallback header
        'Th·ªùi gian c·∫≠p nh·∫≠t', 'Ti√™u ƒë·ªÅ b√†i vi·∫øt', 'C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch', 'L√† c√¥ng ty ƒë·ªëi th·ªß?', 'L√† qu·∫£ng c√°o?',
        'T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn', 'Th√¥ng ƒëi·ªáp ch√≠nh (AI)', 'T√≥m t·∫Øt ph√¢n t√≠ch LLM (AI)', 'URL', 'Tr·∫°ng th√°i x·ª≠ l√Ω',
        'Tr·ªçng t√¢m b√†i vi·∫øt (AI)', 'Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)', 'Gi·ªçng ƒëi·ªáu (AI)', 'N·ªôi dung g·ªëc' ]
    base_header = SHEET_HEADER_V3 if 'SHEET_HEADER_V3' in globals() else base_header_fallback
    if not base_header: print("   ‚ùå ERROR: Could not determine V3 header. Cannot create Excel."); return
    excel_header = ['STT'] + base_header

    all_data_for_excel = []
    stt_counter = 1

    # 1. Process Non-Keyword Articles
    print(f"   Processing {len(non_keyword_data)} non-keyword articles...")
    for item in non_keyword_data:
        row_dict = {col: '' for col in excel_header} # Initialize row
        row_dict['STT'] = stt_counter
        # Map basic info
        row_dict['Th·ªùi gian c·∫≠p nh·∫≠t'] = item.get('Th·ªùi gian', '')
        row_dict['Ti√™u ƒë·ªÅ b√†i vi·∫øt'] = item.get('Ti√™u ƒë·ªÅ', 'N/A')
        row_dict['URL'] = item.get('URL', 'N/A')
        # Set defaults for non-keyword
        row_dict['C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch'] = 'N/A (Non-KW)'
        row_dict['L√† c√¥ng ty ƒë·ªëi th·ªß?'] = 'Kh√¥ng'
        row_dict['L√† qu·∫£ng c√°o?'] = 'Kh√¥ng'
        row_dict['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = 'Non-KW'
        content_or_error = item.get('N·ªôi dung', '')
        if isinstance(content_or_error, str) and content_or_error.startswith("L·ªñI"):
            row_dict['Tr·∫°ng th√°i x·ª≠ l√Ω'] = content_or_error # Put error in status
            row_dict['N·ªôi dung g·ªëc'] = '' # Clear content
        else:
            row_dict['Tr·∫°ng th√°i x·ª≠ l√Ω'] = 'B·ªè qua (Non-KW)'
            # Limit content length for Excel
            row_dict['N·ªôi dung g·ªëc'] = str(content_or_error)[:32764] + '...' if len(str(content_or_error)) > 32767 else str(content_or_error)
        # Set remaining AI/Analysis columns to N/A
        for col in ['T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn', 'Th√¥ng ƒëi·ªáp ch√≠nh (AI)',
                    'T√≥m t·∫Øt ph√¢n t√≠ch LLM (AI)', 'Tr·ªçng t√¢m b√†i vi·∫øt (AI)', 'Gi·ªçng ƒëi·ªáu (AI)']:
            if col in row_dict: row_dict[col] = 'N/A' # Check if col exists in header
        all_data_for_excel.append(row_dict)
        stt_counter += 1

    # 2. Process Analyzed Keyword Articles
    print(f"   Processing {len(analyzed_data_v3)} analyzed keyword articles...")
    for item_v3 in analyzed_data_v3:
        row_dict = {col: item_v3.get(col, '') for col in base_header} # Get V3 data
        row_dict['STT'] = stt_counter # Add STT
        # Limit content length again just in case
        content_kw = str(row_dict.get('N·ªôi dung g·ªëc', ''))
        if len(content_kw) >= 32767: row_dict['N·ªôi dung g·ªëc'] = content_kw[:32764] + '...'
        all_data_for_excel.append(row_dict)
        stt_counter += 1

    # 3. Create DataFrame and Save to Excel
    if all_data_for_excel:
        print(f"   Creating DataFrame with {len(all_data_for_excel)} total rows...")
        try:
            df_all = pd.DataFrame(all_data_for_excel)
            # Ensure columns are in the correct order specified by excel_header
            # Filter excel_header to only include columns present in the DataFrame to avoid KeyError
            valid_excel_header = [col for col in excel_header if col in df_all.columns]
            df_all = df_all[valid_excel_header]
        except Exception as e_df:
            print(f"   ‚ùå ERROR creating Pandas DataFrame: {e_df}. Cannot save Excel."); return

        # Define filename and path
        sheet_status = "SheetOK" if sheets_success_flag else "SheetFailOrSkipped"
        # Safely access global SOURCE_NAME
        source_prefix = SOURCE_NAME if 'SOURCE_NAME' in globals() else "Crawl"
        filename = f"{source_prefix}_all_articles_v3_{sheet_status}_{timestamp}.xlsx"
        # Save to /content/ in Colab, current dir otherwise (check IS_COLAB flag)
        output_path = os.path.join('/content/', filename) if IS_COLAB else filename

        try:
            print(f"   Saving combined data to Excel file: {output_path} ...")
            df_all.to_excel(output_path, index=False, engine="openpyxl")
            print(f"‚úÖ Successfully saved {len(all_data_for_excel)} rows to: {output_path}")

            # 4. Trigger Colab download if applicable
            # Access IS_COLAB_FILES and files globals safely
            if 'IS_COLAB_FILES' in globals() and IS_COLAB_FILES and 'files' in globals() and files:
                print(f"\n   üì• Triggering file download for '{filename}' (Colab)...")
                try: files.download(output_path); print(f"      Download initiated.")
                except Exception as e_dl: print(f"      ‚ùå Error triggering download: {e_dl}")
            elif 'IS_COLAB' in globals() and not IS_COLAB:
                 print(f"\n   ‚ÑπÔ∏è File saved locally: {os.path.abspath(output_path)}")

        except Exception as e_save: print(f"   ‚ùå ERROR saving data to Excel: {e_save}")
    else: print("   ‚ÑπÔ∏è No articles processed. Excel file not created.")
# --- End save_data_local_v3 ---

# Cell 5.10
# === Main Crawler Function            ===
# ========================================

def crawl_cafef(hours_to_check, existing_urls_from_sheet):
    """
    Crawls CafeF news, extracts articles, parses details, identifies keywords.
    Returns lists of keyword/non-keyword articles and the set of all processed URLs.
    """
    # --- Access Global Config Safely ---
    # Use globals() to check existence and provide defaults if missing
    source_name = globals().get('SOURCE_NAME', 'CafeF')
    target_url = globals().get('TARGET_URL')
    base_url = globals().get('BASE_URL')
    max_scrolls_limit = globals().get('MAX_SCROLLS', 5)
    target_tz_name = globals().get('TARGET_TIMEZONE', 'Asia/Ho_Chi_Minh')
    keywords_global = globals().get('KEYWORDS', {})

    print(f"\n--- Starting {source_name} Crawl ---")
    if not target_url: print("‚ùå ERROR: TARGET_URL not defined. Cannot start crawl."); return [], [], set()
    if not base_url: base_url = target_url # Fallback base URL

    # --- Setup Timezone and Limit ---
    try: tz_vn = pytz.timezone(target_tz_name)
    except Exception: tz_vn = pytz.utc
    now = datetime.now(tz=tz_vn)
    time_limit = now - timedelta(hours=hours_to_check)
    print(f"   Crawling articles published after: {time_limit.strftime('%Y-%m-%d %H:%M:%S %Z')}")
    print(f"   Target URL: {target_url}")
    print(f"   Max Scrolls: {max_scrolls_limit}")

    # --- Initialize Results ---
    keyword_articles = []
    non_keyword_articles = []
    processed_urls_this_run = set(existing_urls_from_sheet) # Start with known URLs
    print(f"   Ignoring {len(processed_urls_this_run)} URLs from sheet initially.")

    # --- WebDriver Setup ---
    if not SELENIUM_AVAILABLE or not webdriver: # Check if Selenium is ready
        print("‚ùå ERROR: Selenium WebDriver is not available. Crawl aborted.")
        return [], [], processed_urls_this_run

    chrome_options = Options()
    # Apply standard options
    arguments = ["--headless=new", "--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu",
                 "--window-size=1920,1080", "--disable-blink-features=AutomationControlled",
                 "--blink-settings=imagesEnabled=false", "--log-level=3",
                 "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
                ]
    for arg in arguments: chrome_options.add_argument(arg)
    chrome_options.page_load_strategy = 'eager' # Faster load
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

    driver = None
    try:
        print(f"   Initializing WebDriver...")
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(90) # Increased timeout
        driver.implicitly_wait(5)
        print("   WebDriver initialized.")
    except Exception as e_init:
        print(f"‚ùå CRITICAL Error initializing WebDriver: {e_init}. Crawl aborted.")
        traceback.print_exc(limit=1)
        return [], [], processed_urls_this_run

    # --- Main Crawl Logic ---
    articles_processed_count = 0
    try:
        print(f"   Opening main page: {target_url}")
        driver.get(target_url)

        # Wait for initial content load
        article_link_selector = ".tlitem.box-category-item h3 > a" # Adjust if needed
        print(f"   Waiting for initial articles ('{article_link_selector}', 60s)...")
        WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, article_link_selector)))
        print("   Initial articles loaded.")

        # --- Scroll Loop ---
        print("   Starting scroll loop...")
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        consecutive_no_change = 0
        stop_scrolling_flag = False # Stop if old article found

        while scroll_count < max_scrolls_limit and not stop_scrolling_flag:
            scroll_count += 1
            print(f"\n   --- Scroll attempt {scroll_count}/{max_scrolls_limit} ---")
            # Scroll down and wait for JS loading
            try:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                print(f"      Scrolled down. Waiting...")
                time.sleep(random.uniform(3.5, 5.5)) # Wait longer
            except WebDriverException as scroll_err: print(f"      Scroll error: {scroll_err}. Stopping."); break

            # Check height change
            try: current_height = driver.execute_script("return document.body.scrollHeight")
            except WebDriverException as height_err: print(f"      Height check error: {height_err}. Stopping."); break
            if current_height == last_height:
                consecutive_no_change += 1
                print(f"      Scroll height unchanged ({consecutive_no_change}/3).")
                if consecutive_no_change >= 3: print("      -> End of page/no new content. Stopping."); break
            else: print(f"      Page height: {last_height} -> {current_height}"); consecutive_no_change = 0
            last_height = current_height

            # --- Process Articles Visible ---
            article_item_selector = ".tlitem.box-category-item" # Adjust if needed
            articles_on_page = []
            try: articles_on_page = driver.find_elements(By.CSS_SELECTOR, article_item_selector)
            except Exception as e_find: print(f"      Error finding items: {e_find}"); continue
            print(f"      Found {len(articles_on_page)} article items in view.")
            if not articles_on_page and scroll_count == 1: print("      Warning: No articles on first load?"); break

            newly_processed_on_scroll = 0
            for index, article_element in enumerate(articles_on_page):
                article_data = {"Ngu·ªìn": source_name, "URL": "", "Th·ªùi gian": "N/A", "Ti√™u ƒë·ªÅ": "N/A", "N·ªôi dung": "N/A", "Keywords": ""}
                is_new_url_in_run = False

                try:
                    # 1. Extract Link & Title
                    link_elem = article_element.find_element(By.CSS_SELECTOR, article_link_selector)
                    raw_url = link_elem.get_attribute("href")
                    if raw_url and not raw_url.startswith('http'): article_data["URL"] = requests.compat.urljoin(base_url, raw_url).strip()
                    elif raw_url: article_data["URL"] = raw_url.strip()
                    else: continue
                    article_data["Ti√™u ƒë·ªÅ"] = (link_elem.get_attribute("title") or link_elem.text or "").strip()
                    if not article_data["Ti√™u ƒë·ªÅ"]: continue

                    # 2. Check if URL Processed
                    article_url = article_data["URL"]
                    if not article_url or article_url in processed_urls_this_run: continue

                    # 3. Process New URL
                    is_new_url_in_run = True; newly_processed_on_scroll += 1; articles_processed_count += 1
                    processed_urls_this_run.add(article_url)
                    log_prefix = f"      [Article #{articles_processed_count} Sc{scroll_count}-Idx{index}]"
                    print(f"\n{log_prefix} Processing NEW: {article_data['Ti√™u ƒë·ªÅ'][:65]}...")
                    print(f"{log_prefix} URL: {article_url}")

                    # 4. Open Detail Tab
                    content = "L·ªói m·ªü tab chi ti·∫øt"
                    time_obj = None; detail_tab_opened = False
                    original_window = driver.current_window_handle; new_window = None
                    try:
                        driver.execute_script("window.open(arguments[0], '_blank');", article_url)
                        detail_tab_opened = True
                        WebDriverWait(driver, 15).until(EC.number_of_windows_to_be(len(driver.window_handles)))
                        new_window = [w for w in driver.window_handles if w != original_window][-1]
                        driver.switch_to.window(new_window)

                        detail_page_wait_selector = "body h1, body article, body div.content-detail, div#mainContent"
                        WebDriverWait(driver, 75).until(EC.presence_of_element_located((By.CSS_SELECTOR, detail_page_wait_selector)))

                        # 5. Get Time
                        time_text = get_detailed_article_time(driver)
                        if time_text:
                            time_obj = parse_time(time_text, now, time_limit)
                            if time_obj:
                                article_data["Th·ªùi gian"] = time_obj.strftime("%d/%m/%Y %H:%M")
                                print(f"{log_prefix} ‚úÖ Time OK: {article_data['Th·ªùi gian']}")
                            else:
                                time_check_obj = parse_time(time_text, now, now - timedelta(days=730))
                                if time_check_obj:
                                    print(f"{log_prefix} üõë STOP: Article too old (Parsed: {time_check_obj.strftime('%d/%m/%Y')}).")
                                    stop_scrolling_flag = True; raise StopIteration
                                else: raise ValueError(f"Time parsing failed for '{time_text}'")
                        else: raise ValueError("Time not found on detail page")

                        # 6. Get Content
                        content = get_article_content(driver)
                        article_data["N·ªôi dung"] = content
                        print(f"{log_prefix} Content Len: {len(content)}. Checking Keywords...")
                        if content == "Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung chi ti·∫øt" or len(content) < 100:
                            print(f"{log_prefix} ‚ö†Ô∏è Warning: Insufficient content.")

                        # 7. Check Keywords
                        keywords_found = set(); has_kw = False
                        title_low = article_data["Ti√™u ƒë·ªÅ"].lower()
                        content_low = article_data["N·ªôi dung"].lower() if article_data["N·ªôi dung"] != "Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung chi ti·∫øt" else ""
                        for comp_key, kw_list in keywords_global.items():
                            if contains_keyword(title_low, kw_list):
                                has_kw = True; keywords_found.update(kw for kw in kw_list if re.search(r'\b'+re.escape(kw.lower())+r'\b', title_low))
                            if content_low and contains_keyword(content_low, kw_list):
                                has_kw = True; keywords_found.update(kw for kw in kw_list if re.search(r'\b'+re.escape(kw.lower())+r'\b', content_low))

                        # 8. Classify and Store
                        if has_kw:
                            article_data["Keywords"] = ", ".join(sorted(list(keywords_found)))
                            keyword_articles.append(article_data)
                            print(f"{log_prefix} üî• KEYWORD Article Added (KW: {article_data['Keywords']})")
                        else:
                            if not str(article_data["N·ªôi dung"]).startswith("L·ªñI"):
                                non_keyword_articles.append(article_data)

                    # Detail Tab Error Handling
                    except StopIteration: print(f"{log_prefix} StopIteration: Halting page."); break
                    except ValueError as e_skip:
                         print(f"{log_prefix} ‚è≠Ô∏è Skipping article: {e_skip}")
                         article_data["N·ªôi dung"] = f"L·ªñI: {e_skip}"
                         non_keyword_articles.append(article_data)
                    except (TimeoutException, WebDriverException, NoSuchWindowException) as e_wd_detail:
                         print(f"{log_prefix} ‚ùå WebDriver Error (Detail): {e_wd_detail}")
                         article_data["N·ªôi dung"] = f"L·ªñI WebDriver Detail: {e_wd_detail}"
                         non_keyword_articles.append(article_data)
                    except Exception as e_detail_other:
                         print(f"{log_prefix} ‚ùå Unexpected Error (Detail): {e_detail_other}")
                         article_data["N·ªôi dung"] = f"L·ªñI KH√ÅC Detail: {e_detail_other}"
                         non_keyword_articles.append(article_data)
                    finally: # Close Detail Tab
                        if detail_tab_opened:
                            try:
                                if new_window in driver.window_handles and driver.current_window_handle == new_window: driver.close()
                                if original_window in driver.window_handles: driver.switch_to.window(original_window)
                                elif driver.window_handles: driver.switch_to.window(driver.window_handles[0])
                            except Exception: pass
                        time.sleep(0.1)

                # List Item Error Handling
                except Exception as e_item_level:
                    print(f"      [Article Sc{scroll_count}-Idx{index}] ‚ùå Error processing item: {e_item_level}")
                    if is_new_url_in_run:
                         article_data["N·ªôi dung"] = f"L·ªñI Item Level: {e_item_level}"
                         non_keyword_articles.append(article_data)

            # End article loop
            print(f"   --- Processed {newly_processed_on_scroll} new articles on scroll {scroll_count} ---")
            if stop_scrolling_flag: break # Exit scroll loop

        # End scroll loop
        if not stop_scrolling_flag and scroll_count >= max_scrolls_limit:
            print(f"   Reached max scrolls ({max_scrolls_limit}).")

    # Main Crawl Error Handling
    except TimeoutException as e_timeout_main: print(f"‚ùå FATAL TIMEOUT waiting for initial page load. Crawl aborted.")
    except WebDriverException as e_main_wd: print(f"‚ùå FATAL WebDriver Error during crawl: {e_main_wd}")
    except Exception as e_main_other: print(f"‚ùå FATAL UNEXPECTED Error during crawl: {e_main_other}"); traceback.print_exc()
    finally: # Ensure WebDriver Quits
        if driver:
            print("\n   Closing WebDriver session...")
            try: driver.quit()
            except Exception as e_quit: print(f"      Error quitting WebDriver: {e_quit}")
            print("   WebDriver closed.")

    # Final Summary
    print(f"\n‚úÖ Crawl Finished for {source_name}.")
    print(f"   - Keyword Articles Found: {len(keyword_articles)}")
    print(f"   - Non-Keyword/Error Articles Logged: {len(non_keyword_articles)}")
    # Calculate new URLs processed correctly
    new_urls_count = len(processed_urls_this_run) - len(existing_urls_from_sheet)
    print(f"   - New URLs Processed in this run: {new_urls_count}")
    return keyword_articles, non_keyword_articles, processed_urls_this_run
# --- End crawl_cafef ---

## Cell 5.11:
print("\n--- All Helper and Crawler Functions Defined in Cell 5 ---")

# Cell 6: Ch·∫°y code

# --- Imports c·∫ßn thi·∫øt cho cell n√†y (n·∫øu ch∆∞a import ·ªü Cell 2) ---
import time
from datetime import datetime
import pytz
import os
import sys
import traceback # Th√™m import traceback ƒë·ªÉ log l·ªói chi ti·∫øt h∆°n n·∫øu c·∫ßn

# --- H√†m th·ª±c thi ch√≠nh ---
def main():
    # S·ª≠ d·ª•ng c√°c bi·∫øn global ƒë√£ ƒë∆∞·ª£c khai b√°o/c·∫•u h√¨nh ·ªü Cell 3 v√† c√°c client/model ·ªü Cell 5
    global gc, model, mbs_program_data_for_comparison, config_valid
    global SERVICE_ACCOUNT_JSON_PATH, SPREADSHEET_URL, TARGET_WORKSHEET_NAME, COMPETITOR_SHEET_MAP
    global HOURS_TO_CHECK, MAX_SCROLLS, SLEEP_BETWEEN_AI_CALLS, SHEET_HEADER_V3, TARGET_TIMEZONE, SOURCE_NAME
    global KEYWORDS, IS_COLAB, files # Th√™m IS_COLAB, files n·∫øu d√πng trong save_data_local_v3

    start_time_total = time.time()
    try: target_tz = pytz.timezone(TARGET_TIMEZONE)
    except Exception: target_tz = pytz.utc
    start_dt = datetime.now(target_tz)
    print(f"--- Starting Main Process at {start_dt.strftime('%Y-%m-%d %H:%M:%S %Z')} ---")

    if not config_valid:
        print("‚ùå Aborting execution due to configuration errors found in Cell 3.")
        return
    print("‚úÖ Configuration appears valid. Proceeding...")

    # C√°c bi·∫øn ƒë·ªÉ l∆∞u th·ªùi gian t·ª´ng b∆∞·ªõc
    install_duration, conn_duration, sheet_open_duration, read_mbs_duration, read_urls_duration = 0,0,0,0,0
    crawl_duration, analysis_duration, sheet_write_duration, save_backup_duration = 0,0,0,0


    try:
        # --- 1. C√†i ƒë·∫∑t Chrome & Driver ---
        print("\n--- Step 1: Installing Chrome & ChromeDriver ---")
        install_start_step = time.time()
        install_success = install_chrome_and_driver()
        install_duration = time.time() - install_start_step
        print(f"--- Step 1 Finished (Duration: {install_duration:.2f}s) ---")
        if not install_success:
            print("‚ùå Aborting: Failed to install Chrome/ChromeDriver. Cannot continue.")
            return

        # --- 2. Kh·ªüi t·∫°o c√°c k·∫øt n·ªëi ---
        print("\n--- Step 2: Initializing Connections (Google Sheets, Gemini AI) ---")
        conn_start_step = time.time()
        if not gc:
            print("   Authenticating Google Sheets...")
            gc = authenticate_google_sheets(SERVICE_ACCOUNT_JSON_PATH)
        if not gc:
            print("‚ùå Aborting: Google Sheet client initialization failed.")
            return
        if not model:
             print("   Setting up Gemini AI Model...")
             setup_gemini()
        if not model:
            print("‚ö†Ô∏è Warning: Gemini model initialization failed. Proceeding without AI analysis features.")
        conn_duration = time.time() - conn_start_step
        print(f"--- Step 2 Finished (Duration: {conn_duration:.2f}s) ---")

        # --- 3. M·ªü Spreadsheet ch√≠nh ---
        print("\n--- Step 3: Opening Main Google Spreadsheet ---")
        sheet_open_start_step = time.time()
        spreadsheet = open_spreadsheet(gc, SPREADSHEET_URL)
        if not spreadsheet:
            print("‚ùå Aborting: Failed to open the main Google Spreadsheet.")
            return
        sheet_open_duration = time.time() - sheet_open_start_step
        print(f"--- Step 3 Finished (Duration: {sheet_open_duration:.2f}s) ---")

        # --- 4. ƒê·ªçc d·ªØ li·ªáu ch∆∞∆°ng tr√¨nh MBS ƒë·ªÉ so s√°nh ---
        print("\n--- Step 4: Reading MBS Program Data for Comparison ---")
        read_mbs_start_step = time.time()
        mbs_program_data_for_comparison = []
        temp_mbs_map = {k: v for k, v in COMPETITOR_SHEET_MAP.items() if k == "MBS"}
        if temp_mbs_map:
            all_program_data = read_competitor_programs(gc, SPREADSHEET_URL, temp_mbs_map)
            mbs_program_data_for_comparison = all_program_data.get("MBS", [])
            if not mbs_program_data_for_comparison: print("   ‚ö†Ô∏è Warning: Could not read MBS program list.")
            else: print(f"   ‚úÖ Read {len(mbs_program_data_for_comparison)} MBS programs.")
        else: print("   ‚ö†Ô∏è Warning: 'MBS' key not found in COMPETITOR_SHEET_MAP.")
        read_mbs_duration = time.time() - read_mbs_start_step
        print(f"--- Step 4 Finished (Duration: {read_mbs_duration:.2f}s) ---")

        # --- 5. ƒê·ªçc URL ƒë√£ c√≥ tr√™n Sheet ch√≠nh ---
        print(f"\n--- Step 5: Reading Existing URLs from Main Sheet ('{TARGET_WORKSHEET_NAME}') ---")
        read_urls_start_step = time.time()
        existing_urls_on_sheet = set()
        try:
            worksheet_read = spreadsheet.worksheet(TARGET_WORKSHEET_NAME)
            headers_list = worksheet_read.row_values(1)
            url_col_name_lower = 'url'
            url_col_index = -1
            try: url_col_index = [h.strip().lower() for h in headers_list].index(url_col_name_lower) + 1
            except ValueError: url_col_index = 9; print(f"   ‚ö†Ô∏è Warning: Header '{url_col_name_lower}' not found. Assuming URL in Col I.")
            if url_col_index > 0:
                print(f"   Reading URLs from column {url_col_index}...")
                urls_raw = worksheet_read.col_values(url_col_index)[1:]
                existing_urls_on_sheet = set(str(url).strip() for url in urls_raw if isinstance(url, str) and url.strip().startswith('http'))
                print(f"   Found {len(existing_urls_on_sheet)} unique existing URLs.")
            else: print(f"   Could not determine URL column. Crawling all.")
        except gspread.exceptions.WorksheetNotFound: print(f"   Target tab '{TARGET_WORKSHEET_NAME}' not found.")
        except Exception as e_read: print(f"   ‚ö†Ô∏è Unexpected Error reading existing URLs: {e_read}")
        read_urls_duration = time.time() - read_urls_start_step
        print(f"--- Step 5 Finished (Duration: {read_urls_duration:.2f}s) ---")

        # --- 6. Th·ª±c hi·ªán Crawl ---
        print("\n" + "="*20 + f" Step 6: Starting {SOURCE_NAME} Crawl " + "="*20)
        crawl_start_step = time.time()
        keyword_articles, non_keyword_articles, processed_urls_total = crawl_cafef(HOURS_TO_CHECK, existing_urls_on_sheet)
        crawl_duration = time.time() - crawl_start_step
        print(f"--- Step 6: Web Crawl Finished (Duration: {crawl_duration:.2f}s) ---")
        print(f"   - Found {len(keyword_articles)} new articles with keywords.")
        print(f"   - Found {len(non_keyword_articles)} new articles without keywords (or with errors).")
        print("="* (44 + len(SOURCE_NAME)) + "\n")

        # --- 7. Ph√¢n t√≠ch c√°c b√†i c√≥ Keyword v√† chu·∫©n b·ªã non-keyword ---
        print(f"--- Step 7: Processing {len(keyword_articles)} Keyword Articles & {len(non_keyword_articles)} Non-Keyword Articles ---")
        all_results_for_sheet = [] # List ch·ª©a TO√ÄN B·ªò k·∫øt qu·∫£ cho sheet
        analysis_start_step = time.time()
        current_run_update_time_str = start_dt.strftime('%Y-%m-%d %H:%M:%S')

        # --- X·ª≠ l√Ω Keyword Articles ---
        if not keyword_articles:
            print("   No new keyword articles found to analyze.")
        else:
            for i, article in enumerate(keyword_articles):
                article_title = article.get('Ti√™u ƒë·ªÅ', 'N/A')
                print(f"\n--- Processing KW Article {i+1}/{len(keyword_articles)}: {article_title[:70]}...")
                url = article.get('URL', 'N/A')
                content = article.get('N·ªôi dung', '')
                time_str = article.get('Th·ªùi gian', current_run_update_time_str)
                keywords_found_str = article.get("Keywords", "")
                detected_competitors_in_article = sorted(list(set(
                    comp_key for comp_key, kw_list in KEYWORDS.items()
                    if any(kw.strip() in keywords_found_str for kw in kw_list)
                )))

                if not detected_competitors_in_article:
                    print(f"   ‚ö†Ô∏è Skipping KW article: No valid competitors identified from keywords '{keywords_found_str}'.")
                    # Chuy·ªÉn b√†i n√†y sang non_keyword n·∫øu mu·ªën
                    article['N·ªôi dung'] = f"L·ªói KW: Kh√¥ng t√¨m th·∫•y ƒë·ªëi th·ªß t·ª´ '{keywords_found_str}'"
                    non_keyword_articles.append(article)
                    continue

                print(f"   Competitors mentioned (context for AI): {', '.join(detected_competitors_in_article)}")

                analysis_result = get_default_analysis_result("AI model disabled")
                if model:
                    print(f"   Calling AI analysis...")
                    analysis_result = analyze_with_gemini(model, article_title, content, detected_competitors_in_article)
                    if i < len(keyword_articles) - 1 and SLEEP_BETWEEN_AI_CALLS > 0:
                        print(f"   Sleeping for {SLEEP_BETWEEN_AI_CALLS:.1f}s...")
                        time.sleep(SLEEP_BETWEEN_AI_CALLS)
                else:
                    print("   Skipping AI analysis (AI model unavailable).")

                primary_focus = analysis_result.get('primary_competitor_focus', 'L·ªói AI')
                is_program_pr = analysis_result.get('is_program_pr', False)
                promoted_program = analysis_result.get('promoted_program_name', '')
                is_brand_pr_from_ai = analysis_result.get('is_brand_pr', False) # L·∫•y t·ª´ AI
                program_details = analysis_result.get('program_details', {})
                ai_summary = analysis_result.get('analysis_summary', 'L·ªói AI / AI b·ªã t·∫Øt')

                is_competitor_flag = "Kh√¥ng"
                is_ad_final = "Kh√¥ng"
                processing_status_final = "L·ªói AI / AI T·∫Øt"
                ad_classification_detail_final = "L·ªói AI / AI T·∫Øt"
                main_message_ai_final = "N/A"

                if "L·ªói AI" not in primary_focus and "AI T·∫Øt" not in primary_focus:
                    if primary_focus in KEYWORDS: is_competitor_flag = "C√≥"

                    if is_program_pr and promoted_program:
                        is_ad_final = "C√≥"
                        processing_status_final = f"ƒê√£ x·ª≠ l√Ω (CT: {primary_focus} - {promoted_program[:30]}...)"
                        ad_classification_detail_final = f"PR Ch∆∞∆°ng tr√¨nh ({primary_focus} - {promoted_program[:50]})"
                        main_message_ai_final = program_details.get('DacDiemChinh', ai_summary)
                        if model and gc and spreadsheet: # C·∫≠p nh·∫≠t sheet ch∆∞∆°ng tr√¨nh
                             add_or_update_program_sheet(
                                gc=gc, spreadsheet=spreadsheet, competitor_key=primary_focus,
                                program_name=promoted_program, program_details_dict=program_details,
                                article_url=url, crawl_time_str=current_run_update_time_str,
                                model=model, mbs_programs_list=mbs_program_data_for_comparison
                            )
                    elif is_brand_pr_from_ai and primary_focus in KEYWORDS:
                        is_ad_final = "C√≥"
                        processing_status_final = f"ƒê√£ x·ª≠ l√Ω (PR TH: {primary_focus})"
                        ad_classification_detail_final = f"PR Th∆∞∆°ng hi·ªáu ({primary_focus})"
                        main_message_ai_final = ai_summary
                    elif primary_focus in KEYWORDS: # C√≥ focus ƒë·ªëi th·ªß nh∆∞ng kh√¥ng r√µ l√† PR CT hay TH
                        is_ad_final = "C√≥ (Xem x√©t)" # Ho·∫∑c "Kh√¥ng" t√πy theo quy·∫øt ƒë·ªãnh c·ªßa b·∫°n
                        processing_status_final = f"ƒê√£ x·ª≠ l√Ω (Focus: {primary_focus}, c·∫ßn xem x√©t PR)"
                        ad_classification_detail_final = f"B√†i vi·∫øt v·ªÅ {primary_focus} (AI)"
                        main_message_ai_final = ai_summary
                    else: # Kh√¥ng focus ƒë·ªëi th·ªß c·ª• th·ªÉ ƒë∆∞·ª£c x√°c ƒë·ªãnh l√† KW
                        is_competitor_flag = "Kh√¥ng"
                        is_ad_final = "Kh√¥ng"
                        processing_status_final = "ƒê√£ x·ª≠ l√Ω (Kh√¥ng focus ƒêT)"
                        ad_classification_detail_final = "Kh√¥ng qu·∫£ng c√°o (AI)"
                        main_message_ai_final = ai_summary

                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p primary_focus kh√¥ng n·∫±m trong KEYWORDS nh∆∞ng AI v·∫´n c√≥ th·ªÉ t√¨m th·∫•y
                # V√≠ d·ª•: AI tr·∫£ v·ªÅ "C√¥ng ty ABC" nh∆∞ng "C√¥ng ty ABC" kh√¥ng c√≥ trong dict KEYWORDS
                if primary_focus != "N/A" and primary_focus not in KEYWORDS:
                    is_competitor_flag = "Kh√¥ng (Ngo√†i DS)" # Ho·∫∑c m·ªôt nh√£n kh√°c
                    # Gi·ªØ nguy√™n is_ad_final, ad_classification_detail_final d·ª±a tr√™n is_program_pr, is_brand_pr
                    # N·∫øu b·∫°n mu·ªën b√†i n√†y kh√¥ng ƒë∆∞·ª£c t√≠nh l√† PR c·ªßa ƒë·ªëi th·ªß (v√¨ kh√¥ng trong list), th√¨ set is_ad_final = "Kh√¥ng"
                    if not (is_program_pr or is_brand_pr_from_ai): # N·∫øu AI c≈©ng ko n√≥i l√† PR
                        is_ad_final = "Kh√¥ng"
                        ad_classification_detail_final = f"B√†i vi·∫øt v·ªÅ {primary_focus} (Ngo√†i DS)"


                sheet_v3_row = {
                    'Th·ªùi gian c·∫≠p nh·∫≠t': current_run_update_time_str,
                    'Ti√™u ƒë·ªÅ b√†i vi·∫øt': article_title,
                    'C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch': primary_focus if primary_focus != "L·ªói AI" else "N/A",
                    'L√† c√¥ng ty ƒë·ªëi th·ªß?': is_competitor_flag,
                    'L√† qu·∫£ng c√°o?': is_ad_final,
                    'T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn': promoted_program if is_program_pr else "",
                    'Th√¥ng ƒëi·ªáp ch√≠nh (AI)': main_message_ai_final,
                    'T√≥m t·∫Øt ph√¢n t√≠ch LLM (AI)': ai_summary,
                    'URL': url,
                    'Tr·∫°ng th√°i x·ª≠ l√Ω': processing_status_final,
                    'Tr·ªçng t√¢m b√†i vi·∫øt (AI)': primary_focus if primary_focus != "L·ªói AI" else "N/A",
                    'Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)': ad_classification_detail_final,
                    'Gi·ªçng ƒëi·ªáu (AI)': 'N/A', # B·∫°n c√≥ th·ªÉ y√™u c·∫ßu AI ph√¢n t√≠ch th√™m n·∫øu mu·ªën
                    'N·ªôi dung g·ªëc': content if len(content) < 32767 else content[:32764]+'...',
                }
                all_results_for_sheet.append(sheet_v3_row)

        # --- Chu·∫©n b·ªã Non-Keyword Articles ---
        if non_keyword_articles:
            print(f"\n--- Preparing {len(non_keyword_articles)} Non-Keyword articles for sheet ---")
        for item_non_kw in non_keyword_articles:
            row_dict_non_kw = {col: '' for col in SHEET_HEADER_V3}
            row_dict_non_kw['Th·ªùi gian c·∫≠p nh·∫≠t'] = item_non_kw.get('Th·ªùi gian', current_run_update_time_str)
            row_dict_non_kw['Ti√™u ƒë·ªÅ b√†i vi·∫øt'] = item_non_kw.get('Ti√™u ƒë·ªÅ', 'N/A')
            row_dict_non_kw['URL'] = item_non_kw.get('URL', 'N/A')
            original_content = item_non_kw.get('N·ªôi dung', 'N/A')
            row_dict_non_kw['N·ªôi dung g·ªëc'] = original_content if len(original_content) < 32767 else original_content[:32764]+'...'

            row_dict_non_kw['C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch'] = 'N/A'
            row_dict_non_kw['L√† c√¥ng ty ƒë·ªëi th·ªß?'] = 'Kh√¥ng'
            row_dict_non_kw['L√† qu·∫£ng c√°o?'] = 'Kh√¥ng'
            row_dict_non_kw['T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn'] = ''
            row_dict_non_kw['Th√¥ng ƒëi·ªáp ch√≠nh (AI)'] = 'N/A'
            row_dict_non_kw['T√≥m t·∫Øt ph√¢n t√≠ch LLM (AI)'] = 'N/A (Non-KW)'
            row_dict_non_kw['Tr·∫°ng th√°i x·ª≠ l√Ω'] = 'Non-KW (Kh√¥ng c√≥ t·ª´ kh√≥a ƒë·ªëi th·ªß)' if not str(original_content).startswith("L·ªñI") else f"L·ªói Crawl: {original_content[:100]}"
            row_dict_non_kw['Tr·ªçng t√¢m b√†i vi·∫øt (AI)'] = 'N/A'
            row_dict_non_kw['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = 'Non-KW'
            row_dict_non_kw['Gi·ªçng ƒëi·ªáu (AI)'] = 'N/A'
            all_results_for_sheet.append(row_dict_non_kw)

        analysis_duration = time.time() - analysis_start_step
        print(f"--- Step 7: Article Processing Finished (Duration: {analysis_duration:.2f}s) ---")

        # --- 8. Ghi T·∫§T C·∫¢ k·∫øt qu·∫£ v√†o Google Sheet (V3 Header) ---
        print("\n" + "="*20 + " Step 8: Writing ALL Results to Google Sheet " + "="*20)
        sheet_write_start_step = time.time()
        sheets_write_successful = False
        if gc and spreadsheet:
            if all_results_for_sheet: # Ghi n·∫øu c√≥ d·ªØ li·ªáu
                sheets_write_successful = write_to_google_sheet_v3(
                    gc=gc, spreadsheet=spreadsheet, worksheet_name=TARGET_WORKSHEET_NAME,
                    header=SHEET_HEADER_V3, data_to_write=all_results_for_sheet
                )
                if sheets_write_successful:
                    print(f"   ‚úÖ Successfully wrote {len(all_results_for_sheet)} rows to '{TARGET_WORKSHEET_NAME}'.")
                else:
                    print(f"   ‚ùå Failed to write all_results_for_sheet to '{TARGET_WORKSHEET_NAME}'.")

            else:
                print("   No new data (keyword or non-keyword) to write to the sheet.")
                sheets_write_successful = True # Coi nh∆∞ th√†nh c√¥ng v√¨ kh√¥ng c√≥ g√¨ ƒë·ªÉ ghi
        else:
            print("   Skipping Google Sheet write (Google Client or Spreadsheet object not available).")
        sheet_write_duration = time.time() - sheet_write_start_step
        print(f"--- Step 8: Sheet Writing Attempt {'Completed' if sheets_write_successful else 'Failed/Skipped'} (Duration: {sheet_write_duration:.2f}s) ---")
        print("="*60 + "\n")

        # --- 9. L∆∞u file Excel Backup (T√ôY CH·ªåN) ---
        # B·∫°n c√≥ th·ªÉ gi·ªØ l·∫°i b∆∞·ªõc n√†y ƒë·ªÉ backup, ho·∫∑c b·ªè qua n·∫øu kh√¥ng c·∫ßn
        print("="*20 + " Step 9: Saving Local Backup File (Optional) " + "="*20)
        save_start_step = time.time()
        # H√†m save_data_local_v3 gi·ªù s·∫Ω nh·∫≠n to√†n b·ªô all_results_for_sheet
        # ƒë·ªÉ tr√°nh tr√πng l·∫∑p. B·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh save_data_local_v3 m·ªôt ch√∫t
        # ho·∫∑c t·∫°o m·ªôt h√†m save m·ªõi ƒë∆°n gi·∫£n h∆°n ch·ªâ ƒë·ªÉ dump `all_results_for_sheet` ra Excel.
        # Gi·∫£ s·ª≠ save_data_local_v3 ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªÉ ch·ªâ nh·∫≠n 1 list data:
        # save_data_all_to_excel(all_results_for_sheet, sheets_write_successful)
        # Hi·ªán t·∫°i, ƒë·ªÉ ƒë∆°n gi·∫£n, ta v·∫´n c√≥ th·ªÉ g·ªçi save_data_local_v3 nh∆∞ c≈©,
        # nh∆∞ng n√≥ s·∫Ω ch·ªâ l∆∞u c√°c b√†i non_keyword ch∆∞a ƒë∆∞·ª£c g·ªôp v√†o all_results_for_sheet
        # n·∫øu b·∫°n kh√¥ng g·ªôp `non_keyword_articles` v√†o `all_results_for_sheet` ·ªü tr√™n
        # T·ªêT NH·∫§T L√Ä: S·ª≠a save_data_local_v3 ƒë·ªÉ n√≥ ch·ªâ nh·∫≠n 1 list l√† all_results_for_sheet
        # HO·∫∂C, n·∫øu b·∫°n mu·ªën gi·ªØ logic c≈© c·ªßa save_data_local_v3:
        # T√°ch final_keyword_results t·ª´ all_results_for_sheet (n·∫øu ƒë√£ g·ªôp)
        # final_keyword_results_for_excel = [row for row in all_results_for_sheet if row.get('Tr·∫°ng th√°i x·ª≠ l√Ω','').startswith('ƒê√£ x·ª≠ l√Ω') or row.get('Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)','').startswith('PR')]
        # non_keyword_for_excel = [row for row in all_results_for_sheet if 'Non-KW' in row.get('Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)','')]
        # save_data_local_v3(non_keyword_for_excel, final_keyword_results_for_excel, sheets_write_successful)
        print("   (Logic for saving to Excel needs review based on new flow - for now, this step might be redundant if Sheet is primary)")
        # V√≠ d·ª• ƒë∆°n gi·∫£n l√† kh√¥ng l√†m g√¨ ·ªü ƒë√¢y n·∫øu kh√¥ng mu·ªën Excel n·ªØa:
        if IS_COLAB and files: # V·∫´n c√≥ th·ªÉ mu·ªën download file t·ª´ Colab n·∫øu c√≥
            # T√¨m file Excel ƒë√£ t·∫°o (n·∫øu c√≥ t·ª´ l·∫ßn ch·∫°y tr∆∞·ªõc ho·∫∑c logic c≈©) ƒë·ªÉ download v√≠ d·ª•
            # Ho·∫∑c b·∫°n t·∫°o m·ªôt file excel m·ªõi t·ª´ `all_results_for_sheet` ·ªü ƒë√¢y
            pass

        save_backup_duration = time.time() - save_start_step
        print(f"--- Step 9: Backup Saving Finished (Duration: {save_backup_duration:.2f}s) ---")
        print("="*60 + "\n")

        # --- T·ªïng k·∫øt ---
        end_time_total = time.time()
        end_dt = datetime.now(target_tz)
        total_duration_script = end_time_total - start_time_total
        print(f"--- Main Process Finished at {end_dt.strftime('%Y-%m-%d %H:%M:%S %Z')} ---")
        print(f"Total Execution Duration: {total_duration_script:.2f} seconds ({total_duration_script/60:.2f} minutes)")
        print(f"   - Step 1 (Install Env): {install_duration:.2f}s")
        print(f"   - Step 2 (Connections): {conn_duration:.2f}s")
        print(f"   - Step 3 (Open Sheet):  {sheet_open_duration:.2f}s")
        print(f"   - Step 4 (Read MBS):    {read_mbs_duration:.2f}s")
        print(f"   - Step 5 (Read URLs):   {read_urls_duration:.2f}s")
        print(f"   - Step 6 (Crawl):       {crawl_duration:.2f}s")
        print(f"   - Step 7 (Processing):  {analysis_duration:.2f}s")
        print(f"   - Step 8 (Sheet Write): {sheet_write_duration:.2f}s")
        print(f"   - Step 9 (Save Backup): {save_backup_duration:.2f}s")
        print("="*70)

    except Exception as e_main:
        print(f"\n‚ùå‚ùå‚ùå CRITICAL ERROR in Main Execution ‚ùå‚ùå‚ùå")
        print(f"Error Type: {type(e_main).__name__}")
        print(f"Error Message: {e_main}")
        print("--- Traceback ---")
        traceback.print_exc()
        print("--- End Traceback ---")
    finally:
        print("\nScript execution finished.")

# --- Ch·∫°y ch∆∞∆°ng tr√¨nh ---
if __name__ == "__main__" or 'google.colab' in sys.modules:
    if 'config_valid' in globals() and config_valid:
        main()
    elif 'config_valid' not in globals():
         print("\n‚ùå Configuration validation status unknown. Please run Cell 3 first.")
    else:
        print("\n‚ùå Execution aborted due to configuration errors detected before running main(). Check Cell 3.")